{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamidb201214-svg/Lectures/blob/main/peft_prompt_tuning_patent_claim_style_FIXED_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2126f0",
      "metadata": {
        "id": "aa2126f0"
      },
      "source": [
        "# Prompt Tuning (PEFT) for Patent Claim Style  \n",
        "**Lecture notebook** — train a tiny prompt-tuning adapter that nudges a base LLM to write in *patent-claim style*.\n",
        "\n",
        "**Dataset:** `AI-Growth-Lab/patents_claims_1.5m_traim_test` (claims text + labels).  \n",
        "For this lecture we use **only the claim text** (`text`) to do *style adaptation*.\n",
        "\n",
        "> ⚠️ Not legal advice. Outputs must be reviewed by a qualified patent professional.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e40662f",
      "metadata": {
        "id": "8e40662f"
      },
      "source": [
        "## 0) Install dependencies\n",
        "\n",
        "> **Compatibility note:** PEFT versions below 0.18 are not compatible with Transformers v5, so we pin `transformers<5` for a stable lecture environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c6faf9",
      "metadata": {
        "id": "20c6faf9"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U \"transformers>=4.38,<5\" \"peft>=0.8.2,<0.18\" \"datasets>=2.14.5\" accelerate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1390da28",
      "metadata": {
        "id": "1390da28"
      },
      "source": [
        "## 1) Imports + configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7e4f626",
      "metadata": {
        "id": "e7e4f626"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "\n",
        "from peft import get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61fd7f2d",
      "metadata": {
        "id": "61fd7f2d"
      },
      "source": [
        "### Model choice\n",
        "\n",
        "For prompt tuning, a smaller base model is better for a live lecture.\n",
        "\n",
        "- Default: `bigscience/bloomz-560m` (instruct-tuned Bloom)\n",
        "- If you're on CPU and want faster runs, try `distilgpt2`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a650d5",
      "metadata": {
        "id": "f3a650d5"
      },
      "outputs": [],
      "source": [
        "model_name = \"bigscience/bloomz-560m\"\n",
        "# model_name = \"distilgpt2\"  # faster on CPU\n",
        "\n",
        "NUM_VIRTUAL_TOKENS = 8     # prompt tuning parameters (small)\n",
        "MAX_LENGTH = 256           # sequence length for training\n",
        "TRAIN_SAMPLES = 2000       # how many claims to stream in for training\n",
        "EVAL_SAMPLES = 200         # small held-out set for quick sanity check\n",
        "MAX_STEPS = 80             # for lecture speed; increase for better results\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2155ece",
      "metadata": {
        "id": "f2155ece"
      },
      "source": [
        "## 2) Load tokenizer + base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27774d60",
      "metadata": {
        "id": "27774d60"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Ensure pad token exists (important for batching)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load base model\n",
        "# (Keep it simple for lecture; device handling is done by Trainer)\n",
        "foundational_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Make sure the model knows what to use for padding (avoids warnings)\n",
        "foundational_model.config.pad_token_id = tokenizer.pad_token_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16979d1e",
      "metadata": {
        "id": "16979d1e"
      },
      "source": [
        "## 3) Load a small subset of the patents claims dataset (streaming)\n",
        "\n",
        "The full training CSV is several GB. For a lecture, we **stream** and only take a small sample.\n",
        "\n",
        "We use only the `text` field (claim text).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "870111d7",
      "metadata": {
        "id": "870111d7"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"AI-Growth-Lab/patents_claims_1.5m_traim_test\"\n",
        "\n",
        "# Stream to avoid downloading multi-GB files.\n",
        "train_stream = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
        "test_stream  = load_dataset(dataset_name, split=\"test\",  streaming=True)\n",
        "\n",
        "def take_text(stream, n):\n",
        "    out = []\n",
        "    for ex in itertools.islice(stream, n):\n",
        "        # Keep just claim text for language modeling\n",
        "        out.append({\"text\": ex[\"text\"]})\n",
        "    return Dataset.from_list(out)\n",
        "\n",
        "train_ds_raw = take_text(train_stream, TRAIN_SAMPLES)\n",
        "eval_ds_raw  = take_text(test_stream,  EVAL_SAMPLES)\n",
        "\n",
        "train_ds_raw, eval_ds_raw\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b72aa24d",
      "metadata": {
        "id": "b72aa24d"
      },
      "source": [
        "### Quick look at a couple claim examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863cadcc",
      "metadata": {
        "id": "863cadcc"
      },
      "outputs": [],
      "source": [
        "for i in range(2):\n",
        "    print(\"\\n--- Example\", i, \"---\")\n",
        "    print(train_ds_raw[i][\"text\"][:600])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d465e272",
      "metadata": {
        "id": "d465e272"
      },
      "source": [
        "## 4) Tokenize\n",
        "\n",
        "We train with standard causal LM objective: predict next token.  \n",
        "`DataCollatorForLanguageModeling(mlm=False)` will create labels from `input_ids`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e9c9d99",
      "metadata": {
        "id": "6e9c9d99"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "train_ds = train_ds_raw.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "eval_ds  = eval_ds_raw.map(tokenize_function,  batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "train_ds, eval_ds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acf13f44",
      "metadata": {
        "id": "acf13f44"
      },
      "source": [
        "## 5) Baseline generation (before prompt tuning)\n",
        "\n",
        "We test the base model on a patent-claim-like prompt and see what it does.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e733e1b1",
      "metadata": {
        "id": "e733e1b1"
      },
      "outputs": [],
      "source": [
        "def generate(model, prompt, max_new_tokens=80):\n",
        "    model.eval()\n",
        "    # Temporarily set padding_side to 'left' for generation with causal LMs\n",
        "    original_padding_side = tokenizer.padding_side\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Restore original padding_side\n",
        "    tokenizer.padding_side = original_padding_side\n",
        "\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_length=inputs['input_ids'].shape[1] + max_new_tokens, # Explicitly calculate total max_length\n",
        "            repetition_penalty=1.2,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            temperature=0.8,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            use_cache=False,\n",
        "        )\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b7d8b1d",
      "metadata": {
        "id": "5b7d8b1d"
      },
      "outputs": [],
      "source": [
        "# A simple patent-claim-like prompt we will reuse throughout this notebook\n",
        "baseline_prompt = \"\"\"1. A system comprising:\n",
        "    a processor; and\n",
        "    a memory storing instructions that, when executed by the processor, cause the processor to:\n",
        "        receive input data;\n",
        "        determine an output based on the input data; and\n",
        "        provide the output to a user interface.\n",
        "\"\"\"\n",
        "\n",
        "print(\"=== Baseline (Base Model; before prompt tuning) ===\")\n",
        "print(generate(foundational_model, baseline_prompt, max_new_tokens=140))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e78757f6",
      "metadata": {
        "id": "e78757f6"
      },
      "source": [
        "## 6) Create a Prompt Tuning adapter (PEFT)\n",
        "\n",
        "Only the **virtual prompt embeddings** are trainable; the base model weights stay frozen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2558f9a5",
      "metadata": {
        "id": "2558f9a5"
      },
      "outputs": [],
      "source": [
        "prompt_config = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    prompt_tuning_init=PromptTuningInit.RANDOM,\n",
        "    num_virtual_tokens=NUM_VIRTUAL_TOKENS,\n",
        "    tokenizer_name_or_path=model_name,\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(foundational_model, prompt_config)\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9a2c703",
      "metadata": {
        "id": "c9a2c703"
      },
      "source": [
        "## 7) Training\n",
        "\n",
        "For lecture speed we use `max_steps` (instead of full epochs).\n",
        "Increase steps (and/or sample size) for better style adaptation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ab5acf",
      "metadata": {
        "id": "28ab5acf"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./peft_patent_claim_style\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,  # effective batch size 16\n",
        "    learning_rate=3e-3,\n",
        "    max_steps=MAX_STEPS,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5677816a",
      "metadata": {
        "id": "5677816a"
      },
      "source": [
        "## 8) Generation after prompt tuning\n",
        "\n",
        "We run the same prompt again. With enough steps, you’ll usually see more consistent claim-like phrasing and structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZOy6tIPPAPNH",
      "metadata": {
        "id": "ZOy6tIPPAPNH"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "peft_model.to(device)\n",
        "\n",
        "print(\"=== After Prompt Tuning (PEFT adapter) ===\")\n",
        "print(generate(peft_model, baseline_prompt, max_new_tokens=140))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d6e02e7",
      "metadata": {
        "id": "0d6e02e7"
      },
      "source": [
        "## 9) Save + reload adapter\n",
        "\n",
        "You only need to save the **adapter**, not the full base model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de7e10d",
      "metadata": {
        "id": "0de7e10d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from peft import PeftModel\n",
        "\n",
        "adapter_dir = \"./peft_patent_claim_style_adapter\"\n",
        "os.makedirs(adapter_dir, exist_ok=True)\n",
        "\n",
        "peft_model.save_pretrained(adapter_dir)\n",
        "tokenizer.save_pretrained(adapter_dir)  # optional convenience\n",
        "\n",
        "print(\"Saved adapter (and tokenizer) to:\", adapter_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15aebebe",
      "metadata": {
        "id": "15aebebe"
      },
      "outputs": [],
      "source": [
        "# Reload adapter on top of a *fresh* base model instance\n",
        "# (More reliable than re-using `foundational_model`, which may already be wrapped/modified by PEFT.)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "base_model_for_reload = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "base_model_for_reload.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "reloaded = PeftModel.from_pretrained(base_model_for_reload, adapter_dir, is_trainable=False).to(device)\n",
        "reloaded.eval()\n",
        "\n",
        "print(\"=== Reloaded adapter output ===\")\n",
        "print(generate(reloaded, baseline_prompt, max_new_tokens=140))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dd42902",
      "metadata": {
        "id": "8dd42902"
      },
      "source": [
        "## 10) Suggested lecture exercises\n",
        "\n",
        "1. Change `baseline_prompt` to:\n",
        "   - `\"1. A method comprising:\"`\n",
        "   - `\"1. A computer-readable medium storing instructions that, when executed, cause:\"`\n",
        "2. Increase `MAX_STEPS` to 300–1000 (if time/compute allows).\n",
        "3. Try `NUM_VIRTUAL_TOKENS` in {4, 8, 16, 32} and compare.\n",
        "4. Switch base model to `distilgpt2` to demonstrate how *base model choice* affects output.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}