{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamidb201214-svg/Lectures/blob/main/M3_3_NLG_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How Smart Are They? Understanding the Scale of GPT-3 and GPT-4\n"
      ],
      "metadata": {
        "id": "J3ZgnpAvZ-_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Assumption                                  | Description                                                                                       |\n",
        "|---------------------------------------------|---------------------------------------------------------------------------------------------------|\n",
        "| **Average Tokens per Book**                 | Estimated at 135,000 tokens per book, based on an average book length of 80,000 to 100,000 words.  |\n",
        "| **Average Reading Lifetime of an Individual** | Estimated at 510 books per lifetime, assuming a moderate reading habit of 5-12 books per year over 60 years. |\n",
        "| **Tokens per Word**                         | Estimated at 1.5 tokens per word, accounting for spaces and punctuation.                          |\n",
        "\n"
      ],
      "metadata": {
        "id": "xE_UeBjWZlQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Detail                             | GPT-3                                   | GPT-4                                   |\n",
        "|------------------------------------|-----------------------------------------|-----------------------------------------|\n",
        "| **Developed By**                   | OpenAI                                  | OpenAI                                  |\n",
        "| **Approximate Training Data Size** | 45 terabytes of text data               | Larger than GPT-3 (exact size unknown)  |\n",
        "| **Estimated Token Count**          | 300-400 billion tokens                  | Likely over 500 billion tokens          |\n",
        "| **Equivalent Number of Books**     | 2,222,222 - 2,962,963 books             | >3,703,704 books                        |\n",
        "| **Equivalent Knowledge of People** | 4,356 - 5,810 people                    | >7,263 people                           |\n"
      ],
      "metadata": {
        "id": "vxvz6Gw_ZjY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/transformermodel_architecture.png)"
      ],
      "metadata": {
        "id": "_Ra10qce4Nx2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j431XmcTOzCY"
      },
      "source": [
        "# Why adapt the language model?\n",
        "\n",
        "- LMs are trained in a task-agnostic way.\n",
        "- Downstream tasks can be very different from language modeling on the Pile.\n",
        "For example, consider the natural language inference (NLI) task (is the hypothesis entailed by the premise?):\n",
        "\n",
        "      Premise: I have never seen an apple that is not red.\n",
        "      Hypothesis: I have never seen an apple.\n",
        "      Correct output: Not entailment (the reverse direction would be entailment)\n",
        "\n",
        "- The format of such a task may not be very natural for the model.\n",
        "\n",
        "# Ways downstream tasks can be different\n",
        "\n",
        "- **Formatting**: for example, NLI takes in two sentences and compares them to produce a single binary output. This is different from generating the next token or filling in MASKs. Another example is the presence of MASK tokens in BERT training vs. no MASKs in downstream tasks.\n",
        "- **Topic shift**: the downstream task is focused on a new or very specific topic (e.g., medical records)\n",
        "- **Temporal shift**: the downstream task requires new knowledge that is unavailable during pre-training because 1) the knowledge is new (e.g., GPT3 was trained before Biden became President), 2) the knowledge for the downstream task is not publicly available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEGxCwH7-L2q"
      },
      "source": [
        "\n",
        "# Optimizing Large Language Models\n",
        "\n",
        "There are several options to optimize Large Language Models:\n",
        "\n",
        "    Prompt engineering by providing samples (In-Context Learning)\n",
        "    Prompt Tuning\n",
        "    Fine-Tuning\n",
        "       - Supervised fine-tuning (SFT): Classic fine-tuning by changing all weights\n",
        "       - Transfer Learning - PEFT fine-tuning by changing only a few weights\n",
        "       - Reinforcement Learning Human Feedback (RLHF)\n",
        "\n",
        "An important question is which of these options is the most effective one and which one can overwrite previous optimizations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Prompt Engineering, Prompt Tuning, and PEFT\n",
        "These techniques are essential for efficiently adapting large, pre-trained models like GPT or BERT to specialized tasks or domains, optimizing resource usage and reducing training time.\n",
        "\n",
        "\n",
        "1. **Prompt Engineering (In-Context Learning)**:\n",
        "   - **Definition**: Crafting input prompts to guide a Large Language Model (LLM) for desired outputs.\n",
        "   - **Application**: Uses natural language prompts to \"program\" the LLM, leveraging its contextual understanding.\n",
        "   - **Model Change**: No alteration to the model's parameters; relies on the model's existing knowledge and interpretive abilities.\n",
        "\n",
        "2. **Prompt Tuning**:\n",
        "   - **Difference from Prompt Engineering**: Involves appending a trainable tensor (prompt tokens) to the LLM's input embeddings.\n",
        "   - **Process**: Fine-tunes this tensor for a specific task and dataset, keeping other model parameters unchanged.\n",
        "   - **Example**: Adapting a general LLM for specific tasks like sentiment classification by adjusting prompt tokens.\n",
        "\n",
        "3. **Parameter-Efficient Fine-Tuning (PEFT)**:\n",
        "   - **Overview**: A set of techniques to enhance model performance on specific tasks or datasets by tuning a small subset of parameters.\n",
        "   - **Objective**: Targeted improvements without the need for full model retraining.\n",
        "   - **Relation to Prompt Tuning**: Prompt tuning is a subset of PEFT, focusing on fine-tuning specific parts of the model for task/domain adaptation.\n",
        "\n"
      ],
      "metadata": {
        "id": "5bJ78Ja2Urh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/PEFT_LLMs.png)"
      ],
      "metadata": {
        "id": "F2alv_qaRfHx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npsUW-pPKYp8"
      },
      "source": [
        "### Challenges\n",
        "\n",
        "Fine-tuning models can certainly help to get models to do what you want them to do. However, there are some potential issues:\n",
        "\n",
        "> - **Catastrophic forgetting**: This phenomenon describes a behavior when fine-tuning or prompts can overwrite the pre-trained model characteristics.\n",
        "> - **Overfitting**: If only a certain AI task has been fine-tuned, other tasks can suffer in terms of performance.\n",
        "\n",
        "In general, fine-tuning should be used wisely and best practices should be applied, for example, the quality of the data is more important than the quantity and multiple AI tasks should be fine-tuned at the same time vs after each other."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications\n",
        "\n",
        "There are many platforms that can be used for LLMs' applications:\n"
      ],
      "metadata": {
        "id": "FYi_A6S-NHXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Tool                                                                                                    | Category                             | Best For                                                                         | Type        |\n",
        "| :------------------------------------------------------------------------------------------------------ | :----------------------------------- | :------------------------------------------------------------------------------- | :---------- |\n",
        "| **[LangChain](https://docs.langchain.com)**                                                             | Orchestration                        | Agents, tools, RAG, observability                                                | Open-source |\n",
        "| **[Flowise](https://docs.flowiseai.com)**                                                               | App Builder / Orchestration (Visual) | Low-code drag-and-drop LLM apps (chatbots, RAG flows), rapid prototyping         | Open-source |\n",
        "| **[CrewAI](https://docs.crewai.com)**                                                                   | Agent Orchestration (Multi-agent)    | Role-based multi-agent workflows, task delegation, coordinated tool-using agents | Open-source |\n",
        "| **[Hugging Face](https://huggingface.co/docs)**                                                         | Model Hub                            | Open models, fine-tuning, hosting                                                | Platform    |\n",
        "| **[vLLM](https://docs.vllm.ai)** / **[SGLang](https://github.com/sgl-project/sglang)**                  | Serving                              | High-throughput / Structured generation                                          | Open-source |\n",
        "| **[Ollama](https://github.com/ollama/ollama)** / **[llama.cpp](https://github.com/ggml-org/llama.cpp)** | Local Run                            | Local inference & model management                                               | Open-source |\n",
        "| **[bitsandbytes](https://huggingface.co/docs/transformers/en/quantization/bitsandbytes)**               | Quantization (4/8-bit)               | Fit models into less VRAM; decent speed/quality tradeoffs                        | Open-source |\n",
        "| **[Pydantic](https://docs.pydantic.dev/)**                                                              | Validation / Schemas                 | Type-safe data validation; enforce structured outputs and tool I/O               | Open-source |\n",
        "| **[LlamaIndex](https://docs.llamaindex.ai)**                                                            | Data / RAG                           | Ingestion, indexing, retrieval                                                   | Open-source |\n",
        "| **[Haystack](https://haystack.deepset.ai)**                                                             | RAG Pipelines                        | Production pipelines, Doc QA                                                     | Open-source |\n",
        "| **[Semantic Kernel](https://github.com/microsoft/semantic-kernel)**                                     | Orchestration                        | Enterprise workflows (C#/Python)                                                 | Open-source |\n"
      ],
      "metadata": {
        "id": "lk2E5h829RJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "VaGwQIZsgI_V",
        "outputId": "417c9da4-2513-405e-a6c2-c0da3e23fd88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Collecting transformers\n",
            "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting huggingface-hub<2.0,>=1.3.0 (from transformers)\n",
            "  Downloading huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Downloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 0.36.2\n",
            "    Uninstalling huggingface_hub-0.36.2:\n",
            "      Successfully uninstalled huggingface_hub-0.36.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.6\n",
            "    Uninstalling transformers-4.57.6:\n",
            "      Successfully uninstalled transformers-4.57.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "vllm 0.15.1 requires protobuf>=6.33.5, but you have protobuf 5.29.6 which is incompatible.\n",
            "vllm 0.15.1 requires transformers<5,>=4.56.0, but you have transformers 5.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-1.4.1 transformers-5.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "02c364ffa9f6454eaf783bef0dfc16b3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Delete the model and any other large tensors\n",
        "del model\n",
        "del tokenizer\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# Clear the PyTorch CUDA cache\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "1wQdXJUDh_Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
        "\n",
        "# load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# prepare the model input\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content\n",
        "try:\n",
        "    # rindex finding 151668 (</think>)\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "print(\"thinking content:\", thinking_content) # no opening <think> tag\n",
        "print(\"content:\", content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590,
          "referenced_widgets": [
            "fbdbbe26472a4478b888b1119e5ebb6f",
            "64ebc412643645178c8f3ae232175dd6",
            "bb6b866443364bbda24e8bb65eba9992",
            "42cc0db0a17740ada909b8c1103ae078",
            "eecdea0b6b0844d2b575c439967f73ab",
            "83e5f8303ac44711a13b7fc9410ef0a7",
            "6796784d7dda4dd4b7d2340decc06c8f",
            "b32406297dfc4e17b2726d5d88066f89",
            "1f9e07e96b5b479c84b5ff447b1306ad",
            "8b3f20bd95354b299e3f66846e7b01b6",
            "d8fa193ce7274c02b9e12a09475cb00d"
          ]
        },
        "id": "8LmUkdoVf9ZN",
        "outputId": "def514f1-b984-465a-df94-3da8068231e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbdbbe26472a4478b888b1119e5ebb6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thinking content: \n",
            "content: Okay, the user asked for a short introduction to large language models. Let me start by recalling what I know. LLMs are a big topic, so I need to keep it concise but informative.\n",
            "\n",
            "First, I should define what an LLM is. They're AI models trained on massive amounts of text data. The key points are scale—huge datasets, billions of parameters. Maybe mention that they learn patterns and language structures.\n",
            "\n",
            "Wait, the user might not know what parameters mean. Should I explain? But they want it short. Maybe just say \"billions of parameters\" without diving into technical details. Focus on the outcomes: generating human-like text, understanding context, answering questions.\n",
            "\n",
            "I should highlight common applications to make it relatable. Things like chatbots, writing, coding, translation. That shows real-world use. Also, note that they're not perfect—can hallucinate, need careful use.\n",
            "\n",
            "The user might be a beginner, so avoid jargon. Terms like \"transformer architecture\" might be too much. Skip that unless necessary. Emphasize the training process briefly: learning from vast text to predict next words.\n",
            "\n",
            "Check the length. The user said \"short,\" so aim for 3-4 sentences. Start with a simple definition, mention scale, what they do, and a note on limitations. \n",
            "\n",
            "Also, consider why they're asking. Maybe they heard the term and want a quick overview. Could be for a school project, personal interest, or work. Keeping it neutral and factual is safe.\n",
            "\n",
            "Avoid mentioning specific models like GPT or BERT unless asked. Focus on the general concept. Don't want to overload with examples. \n",
            "\n",
            "Make sure to clarify that LLMs are a type of AI, not the only type. But since the question is specific, stick to LLMs. \n",
            "\n",
            "Double-check for accuracy: LLMs do generate text, but they're also used for tasks beyond generation, like summarization. Should I include that? Maybe in the applications part. \n",
            "\n",
            "Final structure: Definition, scale, capabilities, common uses, and a brief caveat about limitations. Keep it under 5 sentences. \n",
            "\n",
            "Wait, the user said \"short,\" so maybe 3-4 sentences max. Let me draft: \"Large language models (LLMs) are AI systems trained on vast amounts of text data to understand and generate human-like language. They learn patterns from billions of words, enabling tasks like answering questions, writing stories, coding, and translating text. While powerful, they can sometimes produce\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1M7YDGxgr74",
        "outputId": "13acb813-6674-4ac8-b1d2-e5c2fe5cc4ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb  8 09:25:19 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0             27W /   70W |    8042MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes>=0.46.1"
      ],
      "metadata": {
        "id": "NKaVLy9NIYQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,   # <-- match fp16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,              # <-- match fp16\n",
        "    quantization_config=bnb_config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "7eba8c4c3a194d0386eda55e6fadb48c",
            "b956c2c66e994d0eb6047bedb2abe0eb",
            "5614c749da944b948c4143a6aa2b4dce",
            "f145925a848342d5899d0ac95eb2cb7e",
            "0233cb6619e14cd88e4d7092ae27b2d9",
            "fb94ba4694584c6e8114ab2d2a9ec193",
            "8649cf499a07490d97b8209db9282371",
            "67675b0a90da4204b5fd362cd9bba301",
            "71c1986235d44e63b2b57ad1e46d85f6",
            "697efb4fe87640569401ba939376367a",
            "7cc6f2281cd24295bec1f84038048b1b",
            "c07072cc50304c8289c1398f950a802c",
            "4456869fbaf44fd2a00bc64271a6e4ca",
            "1c78ab1b3c474f959b3917705c440297",
            "ed072ba716ef4a5f9d28e0fa61f7b3f4",
            "7f87c3e2af3c42c08e567950c8503d39",
            "c5d401985ab047d1a8a3c44138c11de6",
            "e1566f9df53e47dc9e2b397c0474c75a",
            "436650f88ba343f2b3a8821c7470aa37",
            "91ca98dbe30c45f7af07039a438a662a",
            "cb121df5a59f4782b43690a64859a72d",
            "997b01949e5a48d9a56ae753e4aa7b1b",
            "2047ea2a8b3e49e0b45cef60577231d9",
            "608907aada4e4999816b5e9291dc79cf",
            "80eea99b3385495c8209e66012632e95",
            "2f643706e6664a37b357b3fbc419024c",
            "6aa028885a2149d38cf961f99a83497f",
            "73598dc8c64245bf8fa2088ca418b11e",
            "b0533cb323424454afdc4ef0bfc39f98",
            "45c4026ff9d9401fb81eb047b06f57a9",
            "3e023f4a137048f48052d2282238f225",
            "0fb4572528dc45b38340a3ad902fe4b7",
            "dd7ba8e929b84fcc85c294dce35ddf5f",
            "ebd00cb981654cdca7d42738bda0722c",
            "b331987fbef9465c957ea0e04cdd1a9d",
            "e3d453d316db4621afa454ccefaf4e70",
            "c3890ff973f841f58ceb7534fba36a0a",
            "7fb321cd894e4a1987923553ad8964e8",
            "5cca4ae1659b4a1b9f65ac7453706445",
            "8cbf173fd539498ba30e13d53403a791",
            "74ca5d74bc734319bd7e11baa0afda8b",
            "40709d4bdf3345a4bac49a20c80f4b13",
            "c8c7cd48cc874af0a74438178f9d9fd3",
            "64688589eb8f4f50a977edc37d710884",
            "dae08d7081884f53ba511e1836555183",
            "b24205bc0f12470fb3ffe0844eb5c5aa",
            "6227af8efa854749befddbb1fef2e0df",
            "48bd5d36f9974035b9c53566e07ed2a5",
            "66b79777b390495dacf02723c0817e30",
            "0ebbfd64290c4b0d868df673a2af0338",
            "00b87d30a13f4845bae222d88141d57a",
            "f815cb4faf934a72891e893be9077911",
            "52a450a945184072b2a230966c7e44f6",
            "fac4c93fb3184a3eaa105c0021d57fd3",
            "a691fc6ce4e34d0eb0634aef23fbc221"
          ]
        },
        "id": "tFWt5k9lITEF",
        "outputId": "0c95614d-413c-46e0-e7db-211b311bfc98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7eba8c4c3a194d0386eda55e6fadb48c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c07072cc50304c8289c1398f950a802c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2047ea2a8b3e49e0b45cef60577231d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebd00cb981654cdca7d42738bda0722c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dae08d7081884f53ba511e1836555183"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# prepare the model input\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content\n",
        "try:\n",
        "    # rindex finding 151668 (</think>)\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "print(\"thinking content:\", thinking_content) # no opening <think> tag\n",
        "print(\"content:\", content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycnfbKDAGCWY",
        "outputId": "6e0df06f-5ad9-4393-cebe-814cc9b3821b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thinking content: Okay, the user asked for a short introduction to large language models. Hmm, they probably want something concise but informative—no jargon overload. \n",
            "\n",
            "First, I should clarify what LLMs *are* without drowning them in technicalities. The \"large\" part is key—they're not tiny models. Gotta emphasize scale (parameters) and training data (billions of words). \n",
            "\n",
            "User might be a beginner, so I'll avoid terms like \"transformers\" or \"self-attention\" unless I can explain them simply. Wait, they said \"short,\" so I'll skip deep dives. \n",
            "\n",
            "Also, should I mention applications? Yeah, briefly—like writing, coding—to show real-world relevance. User might care about *why* this matters. \n",
            "\n",
            "*Double-checking*: Must not confuse LLMs with chatbots. Important distinction! LLMs are the *underlying tech*, not the chat interfaces themselves. \n",
            "\n",
            "...Did they mean \"large\" as in size or \"language\" as in multilingual? Probably size, since \"large language model\" is a standard term. \n",
            "\n",
            "*Structure thoughts*: \n",
            "- Start with \"what it is\" (AI system) \n",
            "- Highlight scale (parameters, data) \n",
            "- Core purpose (understand/predict language) \n",
            "- Real-world use cases (writing, coding, etc.) \n",
            "- Quick note on limitations (not perfect, needs training) \n",
            "\n",
            "*Avoid*: \n",
            "- Math details \n",
            "- Specific model names (GPT, etc.) unless relevant \n",
            "- Overpromising (\"can do anything\") \n",
            "\n",
            "*User vibe check*: Feels like someone curious but not technical—maybe a student, writer, or tech enthusiast. They want clarity, not a lecture. \n",
            "\n",
            "*Final polish*: Keep it under 100 words? User said \"short.\" I'll aim for 3-4 sentences max. \n",
            "\n",
            "...Wait, should I add \"trained on vast text\" or \"learned from internet text\"? \"Vast\" sounds more natural than \"billions of words.\" \n",
            "\n",
            "*Decision*: Go with \"trained on vast amounts of text from the internet\" for clarity. \n",
            "\n",
            "*Double-check*: Yes, this covers what it is, why it's \"large,\" and why it matters—without fluff.\n",
            "</think>\n",
            "content: Here's a concise introduction to **Large Language Models (LLMs)**:\n",
            "\n",
            "> **Large Language Models (LLMs)** are advanced AI systems trained on vast amounts of text from the internet, books, and other sources. They learn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "yTuKrWMMGSuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d55230-97df-4a1f-9dac-cd5fef3f3d80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb  8 16:46:42 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0             35W /   70W |    2938MiB /  15360MiB |     39%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Literal, Optional\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Pydantic models (schemas)\n",
        "# -----------------------------\n",
        "class ChatMessage(BaseModel):\n",
        "    role: Literal[\"system\", \"user\", \"assistant\"]\n",
        "    content: str = Field(min_length=1)\n",
        "\n",
        "class GenerationRequest(BaseModel):\n",
        "    prompt: str = Field(min_length=1)\n",
        "    max_new_tokens: int = Field(default=512, ge=1, le=4096)\n",
        "\n",
        "class GenerationResult(BaseModel):\n",
        "    thinking: str = \"\"\n",
        "    answer: str = Field(min_length=1)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Your code, with validation\n",
        "# -----------------------------\n",
        "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "req = GenerationRequest(prompt=\"Give me a short introduction to large language model.\", max_new_tokens=512)\n",
        "\n",
        "# validate messages\n",
        "messages: List[ChatMessage] = [ChatMessage(role=\"user\", content=req.prompt)]\n",
        "messages_dicts = [m.model_dump() for m in messages]  # convert to plain dicts for HF\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages_dicts,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=req.max_new_tokens)\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content (your logic)\n",
        "try:\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token id\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "# validate / package output\n",
        "try:\n",
        "    result = GenerationResult(thinking=thinking_content, answer=content)\n",
        "except ValidationError as e:\n",
        "    # e.g. answer empty -> you get a clean error instead of silent bad data\n",
        "    raise\n",
        "\n",
        "print(\"thinking content:\", result.thinking)\n",
        "print(\"content:\", result.answer)\n"
      ],
      "metadata": {
        "id": "NrK-z6q7QpPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain\n",
        "## Deep Agents overview\n",
        "\n"
      ],
      "metadata": {
        "id": "V_fWlscoU42h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Install dependencies"
      ],
      "metadata": {
        "id": "AzdWG_F0PULe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepagents tavily-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XztJAgs7OZoV",
        "outputId": "5994e182-8b64-4abd-8b25-682bf6f19220"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepagents\n",
            "  Downloading deepagents-0.3.12-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting tavily-python\n",
            "  Downloading tavily_python-0.7.21-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.7 in /usr/local/lib/python3.12/dist-packages (from deepagents) (1.2.8)\n",
            "Requirement already satisfied: langchain<2.0.0,>=1.2.7 in /usr/local/lib/python3.12/dist-packages (from deepagents) (1.2.8)\n",
            "Collecting langchain-anthropic<2.0.0,>=1.3.1 (from deepagents)\n",
            "  Downloading langchain_anthropic-1.3.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting langchain-google-genai<5.0.0,>=4.2.0 (from deepagents)\n",
            "  Downloading langchain_google_genai-4.2.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting wcmatch (from deepagents)\n",
            "  Downloading wcmatch-10.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from tavily-python) (2.32.4)\n",
            "Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.12.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.28.1)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=1.2.7->deepagents) (1.0.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=1.2.7->deepagents) (2.12.3)\n",
            "Collecting anthropic<1.0.0,>=0.78.0 (from langchain-anthropic<2.0.0,>=1.3.1->deepagents)\n",
            "  Downloading anthropic-0.79.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.2.7 (from deepagents)\n",
            "  Downloading langchain_core-1.2.9-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->deepagents) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->deepagents) (0.6.8)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->deepagents) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->deepagents) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->deepagents) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->deepagents) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->deepagents) (0.14.0)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai<5.0.0,>=4.2.0->deepagents)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai<5.0.0,>=4.2.0->deepagents) (1.61.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.5.1->tavily-python) (2025.11.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (2026.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.16.0)\n",
            "Collecting bracex>=2.1.1 (from wcmatch->deepagents)\n",
            "  Downloading bracex-2.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (0.17.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (1.3.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (2.47.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (15.0.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.7->deepagents) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain<2.0.0,>=1.2.7->deepagents) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain<2.0.0,>=1.2.7->deepagents) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain<2.0.0,>=1.2.7->deepagents) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain<2.0.0,>=1.2.7->deepagents) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->deepagents) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->deepagents) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->deepagents) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=1.2.7->deepagents) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=1.2.7->deepagents) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=1.2.7->deepagents) (0.4.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (4.9.1)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain<2.0.0,>=1.2.7->deepagents) (1.12.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (0.6.2)\n",
            "Downloading deepagents-0.3.12-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.6/88.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tavily_python-0.7.21-py3-none-any.whl (17 kB)\n",
            "Downloading langchain_anthropic-1.3.2-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.2.9-py3-none-any.whl (496 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.3/496.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-4.2.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wcmatch-10.1-py3-none-any.whl (39 kB)\n",
            "Downloading anthropic-0.79.0-py3-none-any.whl (405 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.9/405.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bracex-2.6-py3-none-any.whl (11 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, bracex, wcmatch, tavily-python, anthropic, langchain-core, langchain-google-genai, langchain-anthropic, deepagents\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.2.8\n",
            "    Uninstalling langchain-core-1.2.8:\n",
            "      Successfully uninstalled langchain-core-1.2.8\n",
            "Successfully installed anthropic-0.79.0 bracex-2.6 deepagents-0.3.12 filetype-1.2.0 langchain-anthropic-1.3.2 langchain-core-1.2.9 langchain-google-genai-4.2.0 tavily-python-0.7.21 wcmatch-10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Set up your API keys"
      ],
      "metadata": {
        "id": "GQDqA7z8PXXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = getpass(\"Enter GEMINI_API_KEY: \").strip()\n",
        "os.environ[\"TAVILY_API_KEY\"] = getpass(\"Enter TAVILY_API_KEY: \").strip()\n",
        "\n",
        "print('export GEMINI_API_KEY=\"***\"')\n",
        "print('export TAVILY_API_KEY=\"***\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9AqK5JZOvXz",
        "outputId": "444152b6-0cd9-4f48-eb1e-362318e3a57d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter GEMINI_API_KEY: ··········\n",
            "Enter TAVILY_API_KEY: ··········\n",
            "export GEMINI_API_KEY=\"***\"\n",
            "export TAVILY_API_KEY=\"***\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Create a search tool"
      ],
      "metadata": {
        "id": "7NJ5pWDBPce_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Literal\n",
        "from tavily import TavilyClient\n",
        "from deepagents import create_deep_agent\n",
        "\n",
        "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "def internet_search(\n",
        "    query: str,\n",
        "    max_results: int = 5,\n",
        "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
        "    include_raw_content: bool = False,\n",
        "):\n",
        "    \"\"\"Run a web search\"\"\"\n",
        "    return tavily_client.search(\n",
        "        query,\n",
        "        max_results=max_results,\n",
        "        include_raw_content=include_raw_content,\n",
        "        topic=topic,\n",
        "    )"
      ],
      "metadata": {
        "id": "5Ve75I95PL8s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Create a deep agent"
      ],
      "metadata": {
        "id": "JSrJ3xK3PgCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# System prompt to steer the agent to be an expert researcher\n",
        "research_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n",
        "\n",
        "You have access to an internet search tool as your primary means of gathering information.\n",
        "\n",
        "## `internet_search`\n",
        "\n",
        "Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the Gemini model using the GEMINI_API_KEY set earlier\n",
        "# The model name 'gemini-1.5-flash' is a common and capable choice for general tasks.\n",
        "model = init_chat_model(model=\"google_genai:gemini-2.5-flash-lite\")\n",
        "\n",
        "agent = create_deep_agent(\n",
        "    model=model, # Explicitly pass the initialized model\n",
        "    tools=[internet_search],\n",
        "    system_prompt=research_instructions\n",
        ")"
      ],
      "metadata": {
        "id": "xj0cSriiPi9B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Run the agent"
      ],
      "metadata": {
        "id": "pKqtAdNVPnvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]})\n",
        "\n",
        "# Print the agent's response\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AWOHMC7Po_L",
        "outputId": "1bac9978-e13f-497f-c7a3-e1a1557c4fd5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph is an open-source framework developed by LangChain. It's designed to help developers build, manage, and deploy stateful AI agents and complex workflows. Think of it as a low-level orchestration framework that uses graph-based architectures to model the relationships between different components of an AI agent's workflow.\n",
            "\n",
            "Key features and benefits include:\n",
            "\n",
            "*   **Stateful Agents:** LangGraph is particularly useful for creating agents that can maintain state over long-running processes.\n",
            "*   **Orchestration:** It provides infrastructure for orchestrating complex AI tasks.\n",
            "*   **Integration with LangChain:** LangGraph can be used standalone or seamlessly integrated with other LangChain tools and products.\n",
            "*   **Developer Trust:** Companies like Klarna, Replit, and Elastic use LangGraph for building reliable AI applications.\n",
            "*   **Prototyping:** It supports rapid iteration and visual prototyping through tools like LangGraph Studio.\n",
            "\n",
            "In essence, LangGraph aims to simplify the development of sophisticated AI applications, from chatbots to complex task automation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNewzQ6zUkSD",
        "outputId": "c50ab731-1fd8-4b96-f4df-5476f7a93b24"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is langgraph?', additional_kwargs={}, response_metadata={}, id='c08a5f85-0c4e-473c-9585-53f69fbb5b29'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'internet_search', 'arguments': '{\"query\": \"langgraph\"}'}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c3e30-ab5e-7060-8e29-5c98e8bc94b7-0', tool_calls=[{'name': 'internet_search', 'args': {'query': 'langgraph'}, 'id': 'a629cc42-d48a-440b-866c-de53a824825f', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 5195, 'output_tokens': 16, 'total_tokens': 5211, 'input_token_details': {'cache_read': 0}}),\n",
              "  ToolMessage(content='{\"query\": \"langgraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://github.com/langchain-ai/langgraph\", \"title\": \"langchain-ai/langgraph: Build resilient language agents as ...\", \"content\": \"You signed in with another tab or window. You signed out in another tab or window. docs.langchain.com/oss/python/langgraph/. 24.4k stars   4.2k forks   Branches   Tags   Activity. # langchain-ai/langgraph. ## Repository files navigation. Trusted by companies shaping the future of agents – including Klarna, Replit, Elastic, and more – LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents. Get started with the LangGraph Quickstart. To quickly build agents with LangChain\\'s `create_agent` (built on LangGraph), see the LangChain Agents documentation. LangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in LangGraph Studio. docs.langchain.com/oss/python/langgraph/. python   open-source   enterprise   framework   ai   gemini   openai   multiagent   agents   ai-agents   rag   pydantic   llm   generative-ai   chatgpt   langchain   langgraph   deepagents. langgraph==1.0.8  Latest.\", \"score\": 0.9999678, \"raw_content\": null}, {\"url\": \"https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/\", \"title\": \"What is LangGraph?\", \"content\": \"LangGraph is an open-source framework built by LangChain that streamlines the creation and management of AI agent workflows.\", \"score\": 0.999966, \"raw_content\": null}, {\"url\": \"https://www.langchain.com/langgraph\", \"title\": \"LangGraph: Agent Orchestration Framework for Reliable ...\", \"content\": \"Gain control with LangGraph to design agents. Design agent-driven user experiences with LangGraph Platform\\'s APIs. Quickly deploy and scale your application with infrastructure built for agents. ### Developers trust LangGraph to build reliable agents. LangGraph sets the foundation for how we can build and scale AI workloads — from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.”. LangGraph sets the foundation for how we can build and scale AI workloads — from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.”.\", \"score\": 0.9999132, \"raw_content\": null}, {\"url\": \"https://towardsdatascience.com/from-basics-to-advanced-exploring-langgraph-e8c1cf4db787/\", \"title\": \"From Basics to Advanced: Exploring LangGraph\", \"content\": \"In this article, I will explore LangGraph\\'s key features and capabilities, including multi-agent applications. We\\'ll build a system that can answer different\", \"score\": 0.99981624, \"raw_content\": null}, {\"url\": \"https://www.ibm.com/think/topics/langgraph\", \"title\": \"What is LangGraph?\", \"content\": \"*   [Overview](https://www.ibm.com/think/topics/ai-agents#7281535). *   [What is agentic AI?](https://www.ibm.com/think/topics/agentic-ai#2095054954). *   [What is AI agent development?](https://www.ibm.com/think/topics/ai-agent-development#1083937704). *   [How to build an AI agent](https://www.ibm.com/think/topics/how-to-build-an-ai-agent#1083937707). *   [Overview](https://www.ibm.com/think/topics/ai-agent-types#72820454). *   [Overview](https://www.ibm.com/think/topics/components-of-ai-agents#498277090). *   [Communication](https://www.ibm.com/think/topics/ai-agent-communication#498277088). *   [Learning](https://www.ibm.com/think/topics/ai-agent-learning#498277087). *   [Memory](https://www.ibm.com/think/topics/ai-agent-memory#498277086). *   [Perception](https://www.ibm.com/think/topics/ai-agent-perception#498277085). *   [Planning](https://www.ibm.com/think/topics/ai-agent-planning#498277084). *   [What is agent orchestration?](https://www.ibm.com/think/topics/ai-agent-orchestration#228874317). *   [Overview](https://www.ibm.com/think/topics/ai-agent-protocols#1509394340). *   [What is agentic orchestration?](https://www.ibm.com/think/topics/ai-agent-orchestration#2142864945). *   [Tutorial: LangGraph SQL agent](https://www.ibm.com/think/tutorials/build-sql-agent-langgraph-mistral-medium-3-watsonx-ai#229412983). *   [Overview](https://www.ibm.com/think/topics/ai-agent-use-cases#257779831). *   [Finance](https://www.ibm.com/think/topics/ai-agents-in-finance#257779834). *   [Human resources](https://www.ibm.com/think/topics/ai-agents-in-human-resources#257779835). *   [Marketing](https://www.ibm.com/think/topics/ai-agents-in-marketing#257779836). *   [Procurement](https://www.ibm.com/think/topics/ai-agents-in-procurement#257779837). *   [RevOps](https://www.ibm.com/think/topics/ai-agents-revops#257779838). *   [Sales](https://www.ibm.com/think/topics/ai-agents-in-sales#257779839). LangGraph, created by [LangChain](https://www.ibm.com/think/topics/langchain), is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an [AI agent workflow](https://www.ibm.com/think/topics/ai-agents). LangGraph is also built on several key technologies, including [LangChain,](https://www.ibm.com/think/topics/langchain) a Python framework for building AI applications. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including [chatbots](https://www.ibm.com/think/topics/chatbots), state graphs and [other agent-based systems](https://www.ibm.com/think/topics/multiagent-system).\", \"score\": 0.9994967, \"raw_content\": null}], \"response_time\": 0.83, \"request_id\": \"e7b43489-da3c-4691-91ae-9fe88cfd0ad3\"}', name='internet_search', id='05558a4e-e3be-437a-bada-7b653d08015a', tool_call_id='a629cc42-d48a-440b-866c-de53a824825f'),\n",
              "  AIMessage(content=\"LangGraph is an open-source framework developed by LangChain. It's designed to help developers build, manage, and deploy stateful AI agents and complex workflows. Think of it as a low-level orchestration framework that uses graph-based architectures to model the relationships between different components of an AI agent's workflow.\\n\\nKey features and benefits include:\\n\\n*   **Stateful Agents:** LangGraph is particularly useful for creating agents that can maintain state over long-running processes.\\n*   **Orchestration:** It provides infrastructure for orchestrating complex AI tasks.\\n*   **Integration with LangChain:** LangGraph can be used standalone or seamlessly integrated with other LangChain tools and products.\\n*   **Developer Trust:** Companies like Klarna, Replit, and Elastic use LangGraph for building reliable AI applications.\\n*   **Prototyping:** It supports rapid iteration and visual prototyping through tools like LangGraph Studio.\\n\\nIn essence, LangGraph aims to simplify the development of sophisticated AI applications, from chatbots to complex task automation.\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c3e30-b2b4-77f0-94f8-8d6d2ffaae7f-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 7177, 'output_tokens': 214, 'total_tokens': 7391, 'input_token_details': {'cache_read': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip -q install -U google-genai\n",
        "\n",
        "import os\n",
        "from google import genai\n",
        "\n",
        "api_key = os.environ.get(\"GOOGLE_API_KEY\") or os.environ.get(\"GEMINI_API_KEY\")\n",
        "client = genai.Client(api_key=api_key)\n",
        "\n",
        "available = []\n",
        "for m in client.models.list():\n",
        "    # Docs example uses m.supported_actions and checks for \"generateContent\"\n",
        "    if \"generateContent\" in getattr(m, \"supported_actions\", []):\n",
        "        available.append(m.name.replace(\"models/\", \"\"))\n",
        "\n",
        "print(\"Models that support generateContent:\")\n",
        "for name in available[:50]:\n",
        "    print(\" -\", name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1Dm-6vsR6_C",
        "outputId": "0b70228f-daad-4d5a-be05-055d8a2fb07f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai._api_client:Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models that support generateContent:\n",
            " - gemini-2.5-flash\n",
            " - gemini-2.5-pro\n",
            " - gemini-2.0-flash\n",
            " - gemini-2.0-flash-001\n",
            " - gemini-2.0-flash-exp-image-generation\n",
            " - gemini-2.0-flash-lite-001\n",
            " - gemini-2.0-flash-lite\n",
            " - gemini-exp-1206\n",
            " - gemini-2.5-flash-preview-tts\n",
            " - gemini-2.5-pro-preview-tts\n",
            " - gemma-3-1b-it\n",
            " - gemma-3-4b-it\n",
            " - gemma-3-12b-it\n",
            " - gemma-3-27b-it\n",
            " - gemma-3n-e4b-it\n",
            " - gemma-3n-e2b-it\n",
            " - gemini-flash-latest\n",
            " - gemini-flash-lite-latest\n",
            " - gemini-pro-latest\n",
            " - gemini-2.5-flash-lite\n",
            " - gemini-2.5-flash-image\n",
            " - gemini-2.5-flash-preview-09-2025\n",
            " - gemini-2.5-flash-lite-preview-09-2025\n",
            " - gemini-3-pro-preview\n",
            " - gemini-3-flash-preview\n",
            " - gemini-3-pro-image-preview\n",
            " - nano-banana-pro-preview\n",
            " - gemini-robotics-er-1.5-preview\n",
            " - gemini-2.5-computer-use-preview-10-2025\n",
            " - deep-research-pro-preview-12-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwzPi19qtyaF",
        "outputId": "7ab1bfa2-0ae9-45f5-fe27-70cad9922f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting huggingface-hub<1.0.0,>=0.33.4 (from langchain_huggingface)\n",
            "  Using cached huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (1.2.9)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.22.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (1.2.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.67.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.15.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.6.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (2.12.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2026.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.16.0)\n",
            "Downloading langchain_huggingface-1.2.0-py3-none-any.whl (30 kB)\n",
            "Using cached huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
            "Installing collected packages: huggingface-hub, langchain_huggingface\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 1.4.1\n",
            "    Uninstalling huggingface_hub-1.4.1:\n",
            "      Successfully uninstalled huggingface_hub-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 5.1.0 requires huggingface-hub<2.0,>=1.3.0, but you have huggingface-hub 0.36.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.36.2 langchain_huggingface-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install -U \"protobuf>=5.26.1,<6\" \"grpcio-status>=1.71.2,<2\" jedi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfNvZN12aG1Y",
        "outputId": "b86ff346-9240-4325-c45c-3201a8f7d291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/320.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.5/320.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "vllm 0.15.1 requires protobuf>=6.33.5, but you have protobuf 5.29.6 which is incompatible.\n",
            "grpcio-reflection 1.78.0 requires protobuf<7.0.0,>=6.31.1, but you have protobuf 5.29.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f vllm || true\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ABkLltGaPDw",
        "outputId": "97093a7d-1081-49ff-9652-b1727b30f605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "Sun Feb  8 08:40:37 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langgraph deepagents \"langchain[openai]\" \"langchain[google-genai]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wzb2xnZhHfuX",
        "outputId": "e1272f24-53cf-4e3d-9044-b42a3b550e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.7)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-1.0.8-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: deepagents in /usr/local/lib/python3.12/dist-packages (0.3.12)\n",
            "Requirement already satisfied: langchain[openai] in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Collecting langchain[openai]\n",
            "  Downloading langchain-1.2.9-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.2.9)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.3)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.12.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: langchain-anthropic<2.0.0,>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from deepagents) (1.3.2)\n",
            "Requirement already satisfied: langchain-google-genai<5.0.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from deepagents) (4.2.0)\n",
            "Requirement already satisfied: wcmatch in /usr/local/lib/python3.12/dist-packages (from deepagents) (10.1)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (from langchain[openai]) (1.1.7)\n",
            "Requirement already satisfied: anthropic<1.0.0,>=0.78.0 in /usr/local/lib/python3.12/dist-packages (from langchain-anthropic<2.0.0,>=1.3.1->deepagents) (0.79.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.6.8)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.14.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai<5.0.0,>=4.2.0->deepagents) (1.2.0)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai<5.0.0,>=4.2.0->deepagents) (1.61.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai->langchain[openai]) (2.16.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai->langchain[openai]) (0.12.0)\n",
            "Requirement already satisfied: bracex>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from wcmatch->deepagents) (2.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (0.17.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (1.3.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (2.47.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (2.32.4)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (15.0.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai->langchain[openai]) (4.67.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai->langchain[openai]) (2025.11.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (4.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (0.6.2)\n",
            "Downloading langgraph-1.0.8-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-1.2.9-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.2/111.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langgraph, langchain\n",
            "  Attempting uninstall: langgraph\n",
            "    Found existing installation: langgraph 1.0.7\n",
            "    Uninstalling langgraph-1.0.7:\n",
            "      Successfully uninstalled langgraph-1.0.7\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 1.2.8\n",
            "    Uninstalling langchain-1.2.8:\n",
            "      Successfully uninstalled langchain-1.2.8\n",
            "Successfully installed langchain-1.2.9 langgraph-1.0.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langchain",
                  "langgraph"
                ]
              },
              "id": "eabe1b7572334e15827c5fbfc0bbf9a1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Human-in-the-loop"
      ],
      "metadata": {
        "id": "ka5gzDAhdmSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learn how to configure human approval for sensitive tool operations\n",
        "\n",
        "Some tool operations may be sensitive and require human approval before execution. Deep agents support human-in-the-loop workflows through LangGraph’s interrupt capabilities. You can configure which tools require approval using the interrupt_on parameter."
      ],
      "metadata": {
        "id": "ygvDnWf7dpeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import uuid\n",
        "import getpass\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from langchain.tools import tool\n",
        "from langchain.chat_models import init_chat_model\n",
        "from deepagents import create_deep_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.types import Command\n",
        "\n",
        "# -----------------------------\n",
        "# 0) Provider selection + API key prompting\n",
        "# -----------------------------\n",
        "OPENAI_DEFAULT_MODEL = \"openai:gpt-4o-mini\"\n",
        "GEMINI_DEFAULT_MODEL = \"google_genai:gemini-2.5-flash-lite\"\n",
        "\n",
        "def choose_provider(cli_value: Optional[str]) -> str:\n",
        "    if cli_value in {\"openai\", \"gemini\"}:\n",
        "        return cli_value\n",
        "\n",
        "    # Interactive prompt if not provided\n",
        "    while True:\n",
        "        choice = input(\"Choose provider [openai/gemini] (default: openai): \").strip().lower()\n",
        "        if choice == \"\":\n",
        "            return \"openai\"\n",
        "        if choice in {\"openai\", \"gemini\"}:\n",
        "            return choice\n",
        "        print(\"Please type 'openai' or 'gemini'.\")\n",
        "\n",
        "def ensure_api_key(provider: str) -> None:\n",
        "    \"\"\"\n",
        "    Prompt for the provider's API key if missing, and store in env.\n",
        "    - OpenAI: OPENAI_API_KEY\n",
        "    - Gemini: GOOGLE_API_KEY (LangChain checks this first; GEMINI_API_KEY is also supported as fallback)\n",
        "    \"\"\"\n",
        "    if provider == \"openai\":\n",
        "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "            key = getpass.getpass(\"Enter OPENAI_API_KEY (input hidden): \").strip()\n",
        "            if not key:\n",
        "                raise RuntimeError(\"OPENAI_API_KEY was not provided.\")\n",
        "            os.environ[\"OPENAI_API_KEY\"] = key\n",
        "\n",
        "    elif provider == \"gemini\":\n",
        "        # Prefer GOOGLE_API_KEY because that's what LangChain docs show; GEMINI_API_KEY is also accepted.\n",
        "        if not (os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")):\n",
        "            key = getpass.getpass(\"Enter GOOGLE_API_KEY (Gemini) (input hidden): \").strip()\n",
        "            if not key:\n",
        "                raise RuntimeError(\"GOOGLE_API_KEY was not provided.\")\n",
        "            os.environ[\"GOOGLE_API_KEY\"] = key\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unknown provider. Use 'openai' or 'gemini'.\")\n",
        "\n",
        "def pick_model_id(provider: str, override: Optional[str]) -> str:\n",
        "    if override:\n",
        "        return override\n",
        "    return OPENAI_DEFAULT_MODEL if provider == \"openai\" else GEMINI_DEFAULT_MODEL\n",
        "\n",
        "# -----------------------------\n",
        "# Storage layout: ./submissions/<student_id>/*\n",
        "# -----------------------------\n",
        "ROOT = Path(\"./submissions\").resolve()\n",
        "ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "def _student_dir(student_id: str) -> Path:\n",
        "    p = (ROOT / student_id).resolve()\n",
        "    if ROOT not in p.parents:\n",
        "        raise ValueError(\"Invalid student_id (path traversal blocked).\")\n",
        "    p.mkdir(exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def _list_files(student_id: str) -> List[str]:\n",
        "    d = _student_dir(student_id)\n",
        "    return sorted([p.name for p in d.iterdir() if p.is_file()])\n",
        "\n",
        "def _read_file(student_id: str, filename: str) -> str:\n",
        "    d = _student_dir(student_id)\n",
        "    p = (d / filename).resolve()\n",
        "    if d not in p.parents:\n",
        "        raise ValueError(\"Invalid filename (path traversal blocked).\")\n",
        "    if not p.exists():\n",
        "        return \"\"\n",
        "    return p.read_text(encoding=\"utf-8\")\n",
        "\n",
        "def _append_outbox(text: str) -> None:\n",
        "    outbox = ROOT / \"OUTBOX.txt\"\n",
        "    existing = outbox.read_text(encoding=\"utf-8\") if outbox.exists() else \"\"\n",
        "    outbox.write_text(existing + text, encoding=\"utf-8\")\n",
        "\n",
        "# -----------------------------\n",
        "# Tools (LangChain)\n",
        "# -----------------------------\n",
        "@tool\n",
        "def list_submission_files(student_id: str) -> List[str]:\n",
        "    \"\"\"List files in a student's submission folder.\"\"\"\n",
        "    return _list_files(student_id)\n",
        "\n",
        "@tool\n",
        "def read_submission_file(student_id: str, filename: str) -> str:\n",
        "    \"\"\"Read a file from a student's submission folder.\"\"\"\n",
        "    text = _read_file(student_id, filename)\n",
        "    if text == \"\":\n",
        "        return f\"(empty or missing) {filename}\"\n",
        "    return text\n",
        "\n",
        "@tool\n",
        "def auto_validate(student_id: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run simple validity checks and return a report + recommended verdict.\n",
        "    Verdict: 'valid' or 'resubmit'\n",
        "    \"\"\"\n",
        "    files = _list_files(student_id)\n",
        "    required = {\"report.md\", \"solution.py\"}\n",
        "    missing_files = sorted(list(required - set(files)))\n",
        "\n",
        "    report = _read_file(student_id, \"report.md\")\n",
        "    solution = _read_file(student_id, \"solution.py\")\n",
        "\n",
        "    required_headings = [\"# Problem\", \"# Method\", \"# Results\"]\n",
        "    missing_headings = [h for h in required_headings if h not in report]\n",
        "\n",
        "    has_required_function = \"def solve(\" in solution\n",
        "\n",
        "    issues = []\n",
        "    if missing_files:\n",
        "        issues.append(f\"Missing required files: {missing_files}\")\n",
        "    if \"report.md\" in files and missing_headings:\n",
        "        issues.append(f\"Missing required headings in report.md: {missing_headings}\")\n",
        "    if \"solution.py\" in files and not has_required_function:\n",
        "        issues.append(\"solution.py missing required function signature: def solve(...)\")\n",
        "\n",
        "    recommended_verdict = \"valid\" if not issues else \"resubmit\"\n",
        "\n",
        "    recommended_message = (\n",
        "        \"✅ Your submission looks valid. Nice work!\"\n",
        "        if recommended_verdict == \"valid\"\n",
        "        else \"⚠️ Please fix the issues listed and resubmit.\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"student_id\": student_id,\n",
        "        \"files\": files,\n",
        "        \"issues\": issues,\n",
        "        \"recommended_verdict\": recommended_verdict,\n",
        "        \"recommended_message\": recommended_message,\n",
        "    }\n",
        "\n",
        "@tool\n",
        "def record_verdict(student_id: str, verdict: str, notes: str) -> str:\n",
        "    \"\"\"\n",
        "    Record the official verdict (sensitive).\n",
        "    Writes to ./submissions/verdicts.json\n",
        "    \"\"\"\n",
        "    out = ROOT / \"verdicts.json\"\n",
        "    data = json.loads(out.read_text(encoding=\"utf-8\")) if out.exists() else {}\n",
        "    data[student_id] = {\"verdict\": verdict, \"notes\": notes}\n",
        "    out.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
        "    return f\"Recorded verdict for {student_id}: {verdict}\"\n",
        "\n",
        "@tool\n",
        "def message_student(student_id: str, message: str) -> str:\n",
        "    \"\"\"\n",
        "    Mock messaging (sensitive).\n",
        "    Appends to ./submissions/OUTBOX.txt instead of actually sending email.\n",
        "    \"\"\"\n",
        "    _append_outbox(f\"\\n=== TO {student_id} ===\\n{message}\\n\")\n",
        "    return f\"Queued message to {student_id} (see submissions/OUTBOX.txt)\"\n",
        "\n",
        "# -----------------------------\n",
        "# Console HITL \"review UI\"\n",
        "# -----------------------------\n",
        "def _prompt_decision(tool_name: str, args: Dict[str, Any], allowed: List[str]) -> Dict[str, Any]:\n",
        "    print(\"\\n--- HUMAN REVIEW REQUIRED ---\")\n",
        "    print(f\"Tool: {tool_name}\")\n",
        "    print(\"Proposed args:\")\n",
        "    print(json.dumps(args, indent=2))\n",
        "    print(f\"Allowed decisions: {allowed}\")\n",
        "\n",
        "    while True:\n",
        "        choice = input(\"Type approve / reject / edit: \").strip().lower()\n",
        "        if choice == \"approve\" and \"approve\" in allowed:\n",
        "            return {\"type\": \"approve\"}\n",
        "        if choice == \"reject\" and \"reject\" in allowed:\n",
        "            return {\"type\": \"reject\"}\n",
        "        if choice == \"edit\" and \"edit\" in allowed:\n",
        "            print(\n",
        "                \"Paste edited args as JSON \"\n",
        "                \"(e.g. {\\\"student_id\\\": \\\"student_001\\\", \\\"verdict\\\": \\\"valid\\\", \\\"notes\\\": \\\"...\\\"})\"\n",
        "            )\n",
        "            edited_args = json.loads(input(\"> \").strip())\n",
        "            return {\"type\": \"edit\", \"edited_action\": {\"name\": tool_name, \"args\": edited_args}}\n",
        "        print(\"Invalid choice for this tool. Try again.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Main runner\n",
        "# -----------------------------\n",
        "def run(student_id: str, provider: str, model_id: str) -> None:\n",
        "    # Ensure correct key exists before model init\n",
        "    ensure_api_key(provider)\n",
        "\n",
        "    checkpointer = MemorySaver()\n",
        "\n",
        "    # init_chat_model accepts provider:model identifiers like openai:... and google_genai:...\n",
        "    model = init_chat_model(model_id)\n",
        "\n",
        "    agent = create_deep_agent(\n",
        "        model=model,\n",
        "        tools=[\n",
        "            list_submission_files,\n",
        "            read_submission_file,\n",
        "            auto_validate,\n",
        "            record_verdict,\n",
        "            message_student,\n",
        "        ],\n",
        "        system_prompt=(\n",
        "            \"You are a TA agent.\\n\"\n",
        "            \"Workflow:\\n\"\n",
        "            \"1) Call auto_validate(student_id).\\n\"\n",
        "            \"2) Summarize the issues (if any).\\n\"\n",
        "            \"3) Propose record_verdict(student_id, verdict, notes).\\n\"\n",
        "            \"4) If helpful, propose message_student(student_id, message).\\n\"\n",
        "            \"Keep notes short and factual.\"\n",
        "        ),\n",
        "        interrupt_on={\n",
        "            # Sensitive: human must approve/edit/reject official verdict\n",
        "            \"record_verdict\": True,  # default allows approve/edit/reject\n",
        "            # Sensitive: outbound message needs approval (no edit allowed here)\n",
        "            \"message_student\": {\"allowed_decisions\": [\"approve\", \"reject\"]},\n",
        "            # Safe: no interrupts\n",
        "            \"auto_validate\": False,\n",
        "            \"read_submission_file\": False,\n",
        "            \"list_submission_files\": False,\n",
        "        },\n",
        "        checkpointer=checkpointer,\n",
        "    )\n",
        "\n",
        "    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
        "    user_prompt = (\n",
        "        f\"Validate {student_id}. \"\n",
        "        \"Run auto checks, then record an official verdict, and message the student with next steps.\"\n",
        "    )\n",
        "\n",
        "    result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_prompt}]}, config=config)\n",
        "\n",
        "    while result.get(\"__interrupt__\"):\n",
        "        payload = result[\"__interrupt__\"][0].value\n",
        "        action_requests = payload[\"action_requests\"]\n",
        "        review_configs = {cfg[\"action_name\"]: cfg for cfg in payload[\"review_configs\"]}\n",
        "\n",
        "        decisions = []\n",
        "        for action in action_requests:\n",
        "            name = action[\"name\"]\n",
        "            args = action[\"args\"]\n",
        "            allowed = review_configs[name][\"allowed_decisions\"]\n",
        "            decisions.append(_prompt_decision(name, args, allowed))\n",
        "\n",
        "        result = agent.invoke(Command(resume={\"decisions\": decisions}), config=config)\n",
        "\n",
        "    print(\"\\n=== FINAL ASSISTANT MESSAGE ===\")\n",
        "    print(result[\"messages\"][-1].content)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--provider\", choices=[\"openai\", \"gemini\"], help=\"Model provider\")\n",
        "    parser.add_argument(\"--model\", help=\"Override model id (e.g., openai:gpt-4o-mini or google_genai:gemini-2.5-flash-lite)\")\n",
        "    parser.add_argument(\"--student\", default=\"student_001\", help=\"Student submission folder name\")\n",
        "    parser.add_argument(\"--seed\", action=\"store_true\", help=\"Create a demo submission if missing\")\n",
        "    args, unknown = parser.parse_known_args() # Modified line\n",
        "\n",
        "    provider = choose_provider(args.provider)\n",
        "    model_id = pick_model_id(provider, args.model)\n",
        "\n",
        "    # Optional demo seed\n",
        "    if args.seed:\n",
        "        sid = args.student\n",
        "        sdir = _student_dir(sid)\n",
        "        if not (sdir / \"report.md\").exists():\n",
        "            (sdir / \"report.md\").write_text(\"# Problem\\n...\\n# Method\\n...\\n# Results\\n...\\n\", encoding=\"utf-8\")\n",
        "        if not (sdir / \"solution.py\").exists():\n",
        "            (sdir / \"solution.py\").write_text(\"def solve(x):\\n    return x\\n\", encoding=\"utf-8\")\n",
        "        print(f\"Seeded demo submission in: {sdir}\")\n",
        "\n",
        "    run(args.student, provider, model_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQwZdmRlFCn1",
        "outputId": "169ca81b-5971-4872-f0c9-8f11532841fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose provider [openai/gemini] (default: openai): gemini\n",
            "Enter GOOGLE_API_KEY (Gemini) (input hidden): ··········\n",
            "\n",
            "--- HUMAN REVIEW REQUIRED ---\n",
            "Tool: record_verdict\n",
            "Proposed args:\n",
            "{\n",
            "  \"verdict\": \"valid\",\n",
            "  \"student_id\": \"student_001\",\n",
            "  \"notes\": \"Auto validation passed with no issues.\"\n",
            "}\n",
            "Allowed decisions: ['approve', 'edit', 'reject']\n",
            "Type approve / reject / edit: a\n",
            "Invalid choice for this tool. Try again.\n",
            "Type approve / reject / edit: approve\n",
            "\n",
            "--- HUMAN REVIEW REQUIRED ---\n",
            "Tool: message_student\n",
            "Proposed args:\n",
            "{\n",
            "  \"student_id\": \"student_001\",\n",
            "  \"message\": \"\\u2705 Your submission looks valid. Nice work!\"\n",
            "}\n",
            "Allowed decisions: ['approve', 'reject']\n",
            "Type approve / reject / edit: approve\n",
            "\n",
            "=== FINAL ASSISTANT MESSAGE ===\n",
            "The student's submission has been validated and recorded as 'valid'. They have also been messaged with the next steps.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ji6U-ewWlQxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fbdbbe26472a4478b888b1119e5ebb6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64ebc412643645178c8f3ae232175dd6",
              "IPY_MODEL_bb6b866443364bbda24e8bb65eba9992",
              "IPY_MODEL_42cc0db0a17740ada909b8c1103ae078"
            ],
            "layout": "IPY_MODEL_eecdea0b6b0844d2b575c439967f73ab"
          }
        },
        "64ebc412643645178c8f3ae232175dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83e5f8303ac44711a13b7fc9410ef0a7",
            "placeholder": "​",
            "style": "IPY_MODEL_6796784d7dda4dd4b7d2340decc06c8f",
            "value": "Loading weights: 100%"
          }
        },
        "bb6b866443364bbda24e8bb65eba9992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b32406297dfc4e17b2726d5d88066f89",
            "max": 398,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f9e07e96b5b479c84b5ff447b1306ad",
            "value": 398
          }
        },
        "42cc0db0a17740ada909b8c1103ae078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b3f20bd95354b299e3f66846e7b01b6",
            "placeholder": "​",
            "style": "IPY_MODEL_d8fa193ce7274c02b9e12a09475cb00d",
            "value": " 398/398 [00:23&lt;00:00, 14.99it/s, Materializing param=model.norm.weight]"
          }
        },
        "eecdea0b6b0844d2b575c439967f73ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83e5f8303ac44711a13b7fc9410ef0a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6796784d7dda4dd4b7d2340decc06c8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b32406297dfc4e17b2726d5d88066f89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f9e07e96b5b479c84b5ff447b1306ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b3f20bd95354b299e3f66846e7b01b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8fa193ce7274c02b9e12a09475cb00d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7eba8c4c3a194d0386eda55e6fadb48c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b956c2c66e994d0eb6047bedb2abe0eb",
              "IPY_MODEL_5614c749da944b948c4143a6aa2b4dce",
              "IPY_MODEL_f145925a848342d5899d0ac95eb2cb7e"
            ],
            "layout": "IPY_MODEL_0233cb6619e14cd88e4d7092ae27b2d9"
          }
        },
        "b956c2c66e994d0eb6047bedb2abe0eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb94ba4694584c6e8114ab2d2a9ec193",
            "placeholder": "​",
            "style": "IPY_MODEL_8649cf499a07490d97b8209db9282371",
            "value": "model.safetensors.index.json: "
          }
        },
        "5614c749da944b948c4143a6aa2b4dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67675b0a90da4204b5fd362cd9bba301",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71c1986235d44e63b2b57ad1e46d85f6",
            "value": 1
          }
        },
        "f145925a848342d5899d0ac95eb2cb7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_697efb4fe87640569401ba939376367a",
            "placeholder": "​",
            "style": "IPY_MODEL_7cc6f2281cd24295bec1f84038048b1b",
            "value": " 32.8k/? [00:00&lt;00:00, 826kB/s]"
          }
        },
        "0233cb6619e14cd88e4d7092ae27b2d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb94ba4694584c6e8114ab2d2a9ec193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8649cf499a07490d97b8209db9282371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67675b0a90da4204b5fd362cd9bba301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "71c1986235d44e63b2b57ad1e46d85f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "697efb4fe87640569401ba939376367a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cc6f2281cd24295bec1f84038048b1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c07072cc50304c8289c1398f950a802c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4456869fbaf44fd2a00bc64271a6e4ca",
              "IPY_MODEL_1c78ab1b3c474f959b3917705c440297",
              "IPY_MODEL_ed072ba716ef4a5f9d28e0fa61f7b3f4"
            ],
            "layout": "IPY_MODEL_7f87c3e2af3c42c08e567950c8503d39"
          }
        },
        "4456869fbaf44fd2a00bc64271a6e4ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5d401985ab047d1a8a3c44138c11de6",
            "placeholder": "​",
            "style": "IPY_MODEL_e1566f9df53e47dc9e2b397c0474c75a",
            "value": "Download complete: 100%"
          }
        },
        "1c78ab1b3c474f959b3917705c440297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_436650f88ba343f2b3a8821c7470aa37",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91ca98dbe30c45f7af07039a438a662a",
            "value": 1
          }
        },
        "ed072ba716ef4a5f9d28e0fa61f7b3f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb121df5a59f4782b43690a64859a72d",
            "placeholder": "​",
            "style": "IPY_MODEL_997b01949e5a48d9a56ae753e4aa7b1b",
            "value": " 8.04G/8.04G [01:25&lt;00:00, 186MB/s]"
          }
        },
        "7f87c3e2af3c42c08e567950c8503d39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5d401985ab047d1a8a3c44138c11de6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1566f9df53e47dc9e2b397c0474c75a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "436650f88ba343f2b3a8821c7470aa37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "91ca98dbe30c45f7af07039a438a662a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb121df5a59f4782b43690a64859a72d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "997b01949e5a48d9a56ae753e4aa7b1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2047ea2a8b3e49e0b45cef60577231d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_608907aada4e4999816b5e9291dc79cf",
              "IPY_MODEL_80eea99b3385495c8209e66012632e95",
              "IPY_MODEL_2f643706e6664a37b357b3fbc419024c"
            ],
            "layout": "IPY_MODEL_6aa028885a2149d38cf961f99a83497f"
          }
        },
        "608907aada4e4999816b5e9291dc79cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73598dc8c64245bf8fa2088ca418b11e",
            "placeholder": "​",
            "style": "IPY_MODEL_b0533cb323424454afdc4ef0bfc39f98",
            "value": "Fetching 3 files: 100%"
          }
        },
        "80eea99b3385495c8209e66012632e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45c4026ff9d9401fb81eb047b06f57a9",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e023f4a137048f48052d2282238f225",
            "value": 3
          }
        },
        "2f643706e6664a37b357b3fbc419024c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fb4572528dc45b38340a3ad902fe4b7",
            "placeholder": "​",
            "style": "IPY_MODEL_dd7ba8e929b84fcc85c294dce35ddf5f",
            "value": " 3/3 [01:25&lt;00:00, 85.37s/it]"
          }
        },
        "6aa028885a2149d38cf961f99a83497f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73598dc8c64245bf8fa2088ca418b11e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0533cb323424454afdc4ef0bfc39f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45c4026ff9d9401fb81eb047b06f57a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e023f4a137048f48052d2282238f225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fb4572528dc45b38340a3ad902fe4b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd7ba8e929b84fcc85c294dce35ddf5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebd00cb981654cdca7d42738bda0722c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b331987fbef9465c957ea0e04cdd1a9d",
              "IPY_MODEL_e3d453d316db4621afa454ccefaf4e70",
              "IPY_MODEL_c3890ff973f841f58ceb7534fba36a0a"
            ],
            "layout": "IPY_MODEL_7fb321cd894e4a1987923553ad8964e8"
          }
        },
        "b331987fbef9465c957ea0e04cdd1a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cca4ae1659b4a1b9f65ac7453706445",
            "placeholder": "​",
            "style": "IPY_MODEL_8cbf173fd539498ba30e13d53403a791",
            "value": "Loading weights: 100%"
          }
        },
        "e3d453d316db4621afa454ccefaf4e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74ca5d74bc734319bd7e11baa0afda8b",
            "max": 398,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40709d4bdf3345a4bac49a20c80f4b13",
            "value": 398
          }
        },
        "c3890ff973f841f58ceb7534fba36a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8c7cd48cc874af0a74438178f9d9fd3",
            "placeholder": "​",
            "style": "IPY_MODEL_64688589eb8f4f50a977edc37d710884",
            "value": " 398/398 [00:34&lt;00:00, 12.00it/s, Materializing param=model.norm.weight]"
          }
        },
        "7fb321cd894e4a1987923553ad8964e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cca4ae1659b4a1b9f65ac7453706445": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cbf173fd539498ba30e13d53403a791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74ca5d74bc734319bd7e11baa0afda8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40709d4bdf3345a4bac49a20c80f4b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8c7cd48cc874af0a74438178f9d9fd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64688589eb8f4f50a977edc37d710884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dae08d7081884f53ba511e1836555183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b24205bc0f12470fb3ffe0844eb5c5aa",
              "IPY_MODEL_6227af8efa854749befddbb1fef2e0df",
              "IPY_MODEL_48bd5d36f9974035b9c53566e07ed2a5"
            ],
            "layout": "IPY_MODEL_66b79777b390495dacf02723c0817e30"
          }
        },
        "b24205bc0f12470fb3ffe0844eb5c5aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ebbfd64290c4b0d868df673a2af0338",
            "placeholder": "​",
            "style": "IPY_MODEL_00b87d30a13f4845bae222d88141d57a",
            "value": "generation_config.json: 100%"
          }
        },
        "6227af8efa854749befddbb1fef2e0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f815cb4faf934a72891e893be9077911",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52a450a945184072b2a230966c7e44f6",
            "value": 239
          }
        },
        "48bd5d36f9974035b9c53566e07ed2a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fac4c93fb3184a3eaa105c0021d57fd3",
            "placeholder": "​",
            "style": "IPY_MODEL_a691fc6ce4e34d0eb0634aef23fbc221",
            "value": " 239/239 [00:00&lt;00:00, 24.4kB/s]"
          }
        },
        "66b79777b390495dacf02723c0817e30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ebbfd64290c4b0d868df673a2af0338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00b87d30a13f4845bae222d88141d57a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f815cb4faf934a72891e893be9077911": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52a450a945184072b2a230966c7e44f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fac4c93fb3184a3eaa105c0021d57fd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a691fc6ce4e34d0eb0634aef23fbc221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}