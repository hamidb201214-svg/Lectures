{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamidb201214-svg/Lectures/blob/main/PatentDesign_Unsloth_VLM_Tutorial_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93f3bda3",
      "metadata": {
        "id": "93f3bda3"
      },
      "source": [
        "# Fine-tune a Vision-Language Model on Design Patent Figures\n",
        "\n",
        "**The Primary Goal:** *The primary goal of this code is to teach a Vision-Language Model (VLM) how to behave like a patent analyst. By fine-tuning on the IMPACT dataset, the model learns the specific \"language of patents\"‚Äîmoving away from generic image descriptions (e.g., \"a drawing of a shoe\") toward technical, concrete visual descriptions (e.g., \"A side view of a footwear design featuring a textured midsole and ornamental stitching on the heel\") that are required for design patent documentation.*\n",
        "\n",
        "**To run this, press `Runtime` ‚Üí `Run all` on a free Tesla T4 Google Colab instance.**\n",
        "\n",
        "This teaching notebook shows:\n",
        "1. **Data prep** with `AI4Patents/IMPACT` (sample-first, full dataset optional)\n",
        "2. **Finetuning** a Vision-Language Model (VLM) with **Unsloth + LoRA**\n",
        "3. **Inference** before/after training\n",
        "4. **Quick evaluation**\n",
        "5. **Saving & reloading** LoRA adapters (and optional merged weights)\n",
        "\n",
        "> Dataset: IMPACT (Integrated Multimodal Patent Analysis and CreaTion Dataset) ‚Äî design patent figures + captions (USPTO, 2007‚Äì2022).  \n",
        "> License: **CC BY-SA 4.0** (dataset). Respect the license when sharing derivatives.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb428a7",
      "metadata": {
        "id": "9fb428a7"
      },
      "source": [
        "## 0) Setup (GPU)  \n",
        "In Colab: `Runtime` ‚Üí `Change runtime type` ‚Üí **GPU** (Tesla T4 is enough for this demo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b0b9db",
      "metadata": {
        "id": "b9b0b9db"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "\n",
        "# Unsloth's Colab-friendly install pattern (keeps installs reasonably fast on free T4)\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip -q install unsloth\n",
        "else:\n",
        "    import torch\n",
        "    v = re.match(r'[\\d]{1,}\\.[\\d]{1,}', str(torch.__version__)).group(0)\n",
        "    xformers = 'xformers==' + {'2.10':'0.0.34','2.9':'0.0.33.post1','2.8':'0.0.32.post2'}.get(v, \"0.0.34\")\n",
        "    !pip -q install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip -q install --no-deps unsloth_zoo bitsandbytes accelerate {xformers} peft trl triton unsloth\n",
        "\n",
        "# Pin known-good versions (adjust if you already have newer working installs)\n",
        "!pip -q install \"transformers==4.56.2\"\n",
        "!pip -q install --no-deps \"trl==0.22.2\"\n",
        "\n",
        "print(\"‚úÖ Installed! Restart runtime if Colab asks.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2741f816",
      "metadata": {
        "id": "2741f816"
      },
      "source": [
        "## 1) Load a VLM (Qwen2-VL 2B) + add LoRA adapters\n",
        "\n",
        "We use a 4-bit quantized VLM so it fits comfortably on a free T4.  \n",
        "Then we add **LoRA adapters** so we train only a small fraction of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c14fe1df",
      "metadata": {
        "id": "c14fe1df"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastVisionModel\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    load_in_4bit = True,                  # 4bit = lower VRAM\n",
        "    use_gradient_checkpointing = \"unsloth\" # helps with longer context\n",
        ")\n",
        "\n",
        "# Add LoRA adapters (parameter-efficient finetuning)\n",
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True,\n",
        "    finetune_language_layers   = True,\n",
        "    finetune_attention_modules = True,\n",
        "    finetune_mlp_modules       = True,\n",
        "\n",
        "    r = 16,\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model + LoRA ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e1402cf",
      "metadata": {
        "id": "7e1402cf"
      },
      "source": [
        "## 2) Get IMPACT data (sample-first)\n",
        "\n",
        "**Why sample-first?** The full IMPACT dataset is very large (yearly ZIPs are multiple GB).  \n",
        "For teaching and quick iteration, we start with the repo‚Äôs **Sample data** (small), then show an **optional** section for pulling a small subset from Hugging Face.\n",
        "\n",
        "If the repo layout changes, this notebook tries to discover files automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_32nJmz3HPt"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = \"https://github.com/AI4Patents/IMPACT.git\"\n",
        "repo = Path(\"IMPACT\")\n",
        "sample_dir = repo / \"Sample data\"\n",
        "\n",
        "# Clone once (if needed)\n",
        "if not repo.exists():\n",
        "    !git clone -q {REPO_URL}\n",
        "\n",
        "print(\"Repo exists:\", repo.exists())\n",
        "print(\"Sample dir:\", sample_dir, \"exists:\", sample_dir.exists())\n",
        "\n",
        "# Quick tree (first ~200 paths)\n",
        "if sample_dir.exists():\n",
        "    for p in list(sample_dir.rglob(\"*\"))[:200]:\n",
        "        print(p)\n",
        "else:\n",
        "    print(\"Sample data folder not found. Use the Hugging Face download option later.\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "d_32nJmz3HPt"
    },
    {
      "cell_type": "markdown",
      "id": "dba2c6ce",
      "metadata": {
        "id": "dba2c6ce"
      },
      "source": [
        "### 2.1 Load sample CSV + set the dataset schema\n",
        "\n",
        "We‚Äôll:\n",
        "- Find a CSV inside `IMPACT/Sample data/`\n",
        "- Read it with pandas\n",
        "- **Set the key column names once** (image filenames + captions)\n",
        "\n",
        "> The IMPACT sample CSV stores **lists** of figure filenames and captions per patent.\n",
        "  To keep this notebook easy to follow, we *don‚Äôt* do any heuristic column guessing.\n",
        "  Instead, we print `df.columns` and you edit the `COL_*` variables in the next cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "828c7b5f",
      "metadata": {
        "id": "828c7b5f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "sample_dir = Path(\"IMPACT\") / \"Sample data\"\n",
        "csvs = sorted(sample_dir.rglob(\"*.csv\")) if sample_dir.exists() else []\n",
        "if not csvs:\n",
        "    raise FileNotFoundError(\n",
        "        \"No sample CSV found. If the repo layout changed, skip to the HF download section below.\"\n",
        "    )\n",
        "\n",
        "csv_path = csvs[0]\n",
        "print(\"Using CSV:\", csv_path)\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "print(\"Rows:\", len(df))\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# =====================\n",
        "# IMPORTANT: EDIT THESE\n",
        "# =====================\n",
        "# Column with a *list* (or list-like string) of figure filenames / relative paths.\n",
        "COL_FILE_NAMES = \"file_names\"\n",
        "\n",
        "# Column with a *list* (or list-like string) of captions aligned with COL_FILE_NAMES.\n",
        "COL_CAPTIONS   = \"caption\"\n",
        "\n",
        "# Optional: extra per-figure descriptions aligned with COL_FILE_NAMES (set to None if not present).\n",
        "COL_FIG_DESC   = \"fig_desc\"\n",
        "\n",
        "# Root folder where images live for the sample data.\n",
        "IMAGE_ROOT = sample_dir\n",
        "\n",
        "# Validate required columns\n",
        "missing = [c for c in [COL_FILE_NAMES, COL_CAPTIONS] if c not in df.columns]\n",
        "if missing:\n",
        "    raise KeyError(\n",
        "        f\"Missing columns: {missing}. Update COL_FILE_NAMES / COL_CAPTIONS above based on df.columns.\"\n",
        "    )\n",
        "\n",
        "if COL_FIG_DESC is not None and COL_FIG_DESC not in df.columns:\n",
        "    print(f\"Note: {COL_FIG_DESC!r} not found; continuing without fig_desc.\")\n",
        "    COL_FIG_DESC = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de1098ea",
      "metadata": {
        "id": "de1098ea"
      },
      "source": [
        "### 2.2 Preview a few examples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast, re\n",
        "import pandas as pd\n",
        "\n",
        "def parse_maybe_list(x):\n",
        "    \"\"\"Turn list-like strings into Python lists.\n",
        "\n",
        "    The IMPACT CSV often stores lists as strings like \"['a','b']\".\n",
        "    \"\"\"\n",
        "    if isinstance(x, list):\n",
        "        return x\n",
        "    if not isinstance(x, str):\n",
        "        return []\n",
        "    s = x.strip()\n",
        "    # Looks like \"['a', 'b', ...]\" ‚Üí parse safely\n",
        "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
        "        try:\n",
        "            v = ast.literal_eval(s)\n",
        "            if isinstance(v, list):\n",
        "                return [str(i).strip() for i in v]\n",
        "        except Exception:\n",
        "            pass\n",
        "    # Fallback: split on commas/semicolons\n",
        "    parts = [p.strip().strip(\"'\\\"\") for p in re.split(r\"[;,]\\s*\", s) if p.strip()]\n",
        "    return parts if parts else [s.strip(\"'\\\"\")]\n",
        "\n",
        "# Explode patents ‚Üí figures (one row per image)\n",
        "rows = []\n",
        "for _, r in df.iterrows():\n",
        "    files = parse_maybe_list(r.get(COL_FILE_NAMES))\n",
        "    caps  = parse_maybe_list(r.get(COL_CAPTIONS))\n",
        "    descs = parse_maybe_list(r.get(COL_FIG_DESC)) if COL_FIG_DESC else []\n",
        "\n",
        "    for i, f in enumerate(files):\n",
        "        cap  = caps[i]  if i < len(caps)  else (caps[0]  if caps  else \"\")\n",
        "        desc = descs[i] if i < len(descs) else (descs[0] if descs else \"\")\n",
        "        rows.append({\n",
        "            \"title\": r.get(\"title\", \"\"),\n",
        "            \"id\": r.get(\"id\", \"\"),\n",
        "            \"date\": r.get(\"date\", \"\"),\n",
        "            \"class\": r.get(\"class\", \"\"),\n",
        "            \"file_name\": str(f).strip(),\n",
        "            \"caption_i\": str(cap).strip(),\n",
        "            \"fig_desc_i\": str(desc).strip(),\n",
        "        })\n",
        "\n",
        "df_fig = pd.DataFrame(rows)\n",
        "print(\"Exploded rows (figures):\", len(df_fig))\n",
        "print(df_fig[[\"file_name\",\"caption_i\"]].head(3))\n"
      ],
      "metadata": {
        "id": "cNGsRF7SzEPt"
      },
      "id": "cNGsRF7SzEPt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def resolve_image_path_from_filename(filename, root=IMAGE_ROOT):\n",
        "    if not isinstance(filename, str) or not filename.strip():\n",
        "        return None\n",
        "\n",
        "    name = filename.strip().strip(\"'\\\"\")\n",
        "    p = Path(name)\n",
        "\n",
        "    # If it already includes a subpath, try that relative to root\n",
        "    p2 = root / p\n",
        "    if p2.is_file():\n",
        "        return p2\n",
        "\n",
        "    # Try just the basename\n",
        "    p3 = root / p.name\n",
        "    if p3.is_file():\n",
        "        return p3\n",
        "\n",
        "    # Last resort (small sample ok): search recursively by basename\n",
        "    hits = list(root.rglob(p.name))\n",
        "    return hits[0] if hits else None\n"
      ],
      "metadata": {
        "id": "CDhbTfcwvpIr"
      },
      "id": "CDhbTfcwvpIr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8eb5dd",
      "metadata": {
        "id": "7b8eb5dd"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    fn = df_fig.iloc[i][\"file_name\"]\n",
        "    path = resolve_image_path_from_filename(fn)\n",
        "    print(fn, \"->\", path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0af5a4b7",
      "metadata": {
        "id": "0af5a4b7"
      },
      "source": [
        "## 3) Convert to Unsloth VLM chat format\n",
        "\n",
        "Vision finetuning expects each example as:\n",
        "\n",
        "```python\n",
        "[\n",
        "  {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":...}, {\"type\":\"image\",\"image\":...}]},\n",
        "  {\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":...}]},\n",
        "]\n",
        "```\n",
        "\n",
        "We‚Äôll build an instruction that teaches the model to describe **design patent figures**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c11892a",
      "metadata": {
        "id": "8c11892a"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "CAPTION_COL = \"caption_i\"\n",
        "IMAGE_COL   = \"file_name\"\n",
        "\n",
        "INSTRUCTION = (\n",
        "    \"You are a design patent analyst. \"\n",
        "    \"Describe the product and key visual features shown in this patent figure. \"\n",
        "    \"Be concrete (shape, parts, proportions) in 1‚Äì3 sentences.\"\n",
        ")\n",
        "\n",
        "def row_to_example(row):\n",
        "    img_path = resolve_image_path_from_filename(row[IMAGE_COL])\n",
        "    if img_path is None:\n",
        "        return None\n",
        "    caption = str(row.get(CAPTION_COL, \"\")).strip()\n",
        "    if not caption:\n",
        "        return None\n",
        "\n",
        "    im = Image.open(img_path)\n",
        "    # TIFFs can be multi-frame; use first frame\n",
        "    try:\n",
        "        im.seek(0)\n",
        "    except Exception:\n",
        "        pass\n",
        "    im = im.convert(\"RGB\")\n",
        "\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"text\", \"text\": INSTRUCTION},\n",
        "                {\"type\": \"image\", \"image\": im},\n",
        "            ]},\n",
        "            {\"role\": \"assistant\", \"content\": [\n",
        "                {\"type\": \"text\", \"text\": caption},\n",
        "            ]},\n",
        "        ]\n",
        "    }\n",
        "\n",
        "examples = []\n",
        "for _, row in df_fig.iloc[:50, :].iterrows():\n",
        "    ex = row_to_example(row)\n",
        "    if ex is not None:\n",
        "        examples.append(ex)\n",
        "\n",
        "print(\"Prepared examples:\", len(examples))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf6eeaca",
      "metadata": {
        "id": "bf6eeaca"
      },
      "source": [
        "## 4) Baseline inference (before training)\n",
        "\n",
        "Let‚Äôs see what the base VLM says about one figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5a37244",
      "metadata": {
        "id": "e5a37244"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "from unsloth import FastVisionModel\n",
        "\n",
        "FastVisionModel.for_inference(model)\n",
        "\n",
        "# pick an example\n",
        "sample = examples[0][\"messages\"][0][\"content\"][1][\"image\"]  # the PIL image\n",
        "messages = [{\"role\":\"user\",\"content\":[{\"type\":\"image\"}, {\"type\":\"text\",\"text\":INSTRUCTION}]}]\n",
        "\n",
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = tokenizer(sample, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    temperature=1.2,\n",
        "    min_p=0.1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63cf96cd",
      "metadata": {
        "id": "63cf96cd"
      },
      "source": [
        "## 5) Train (SFT)\n",
        "\n",
        "We‚Äôll do a short run (**~30‚Äì80 steps**) so it finishes quickly on a free T4.  \n",
        "For better results, increase `max_steps` or use `num_train_epochs=1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8ce3b47",
      "metadata": {
        "id": "c8ce3b47"
      },
      "outputs": [],
      "source": [
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from unsloth import FastVisionModel\n",
        "\n",
        "FastVisionModel.for_training(model)\n",
        "\n",
        "# quick train/val split\n",
        "from datasets import Dataset\n",
        "ds = Dataset.from_list(examples).train_test_split(test_size=0.1, seed=3407)\n",
        "train_ds, eval_ds = ds[\"train\"], ds[\"test\"]\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=UnslothVisionDataCollator(model, tokenizer),\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds.select(range(min(30, len(eval_ds)))),  # small eval on T4\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=50,                 # üëà change to 200+ for better results\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.001,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",\n",
        "\n",
        "        # required for vision finetuning\n",
        "        remove_unused_columns=False,\n",
        "        dataset_text_field=\"\",\n",
        "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "        max_length=2048,\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "trainer_stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1908899c",
      "metadata": {
        "id": "1908899c"
      },
      "source": [
        "## 6) Inference (after training)\n",
        "\n",
        "Same prompt, same image ‚Äî now with LoRA adapters active."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "from unsloth import FastVisionModel\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Robust image handling (PIL / dict / path) ---\n",
        "def ensure_pil(x):\n",
        "    if isinstance(x, Image.Image):\n",
        "        return x.convert(\"RGB\")\n",
        "    if isinstance(x, str):\n",
        "        return Image.open(x).convert(\"RGB\")\n",
        "    if isinstance(x, dict):\n",
        "        if x.get(\"bytes\", None) is not None:\n",
        "            return Image.open(BytesIO(x[\"bytes\"])).convert(\"RGB\")\n",
        "        if x.get(\"path\", None):\n",
        "            return Image.open(x[\"path\"]).convert(\"RGB\")\n",
        "        for k in (\"image\", \"data\"):\n",
        "            if k in x:\n",
        "                return ensure_pil(x[k])\n",
        "    raise TypeError(f\"Unsupported image type: {type(x)}\")\n",
        "\n",
        "# Ensure inference mode (and adapters are ON)\n",
        "FastVisionModel.for_inference(model)\n",
        "model.eval()\n",
        "\n",
        "# Pull a sample image from our prepared examples\n",
        "sample_img = ensure_pil(examples[0][\"messages\"][0][\"content\"][1][\"image\"])\n",
        "print(\"Sample image size:\", sample_img.size)\n",
        "\n",
        "# (Optional) visualize the image\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(sample_img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Qwen2-VL expects an image placeholder in the chat template,\n",
        "# and you pass the actual PIL image to the tokenizer separately.\n",
        "messages = [{\"role\":\"user\",\"content\":[{\"type\":\"image\"}, {\"type\":\"text\",\"text\":INSTRUCTION}]}]\n",
        "\n",
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = tokenizer(\n",
        "    sample_img,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Stream output nicely (no prompt, no special tokens)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    _ = model.generate(\n",
        "        **inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=160,\n",
        "        use_cache=True,\n",
        "        do_sample=True,\n",
        "        temperature=1.5,\n",
        "        min_p=0.1,\n",
        "        repetition_penalty=1.05,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "Wcj1XHIxRmci"
      },
      "id": "Wcj1XHIxRmci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "78ad8772",
      "metadata": {
        "id": "78ad8772"
      },
      "source": [
        "## 7) Quick evaluation (ROUGE on a tiny eval subset)\n",
        "\n",
        "This is *not* a perfect metric for captions, but it‚Äôs a quick sanity check.\n",
        "We compare:\n",
        "- **Base model** (freshly loaded, no LoRA)\n",
        "- **Finetuned model** (LoRA adapters)\n",
        "\n",
        "We keep evaluation small to stay fast on a free T4."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from unsloth import FastVisionModel\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def generate_caption(vlm, tok, image, instruction, max_new_tokens=96):\n",
        "    # `ensure_pil` was defined in the inference cell above.\n",
        "    image = ensure_pil(image)\n",
        "    msgs = [{\"role\":\"user\",\"content\":[{\"type\":\"image\"}, {\"type\":\"text\",\"text\":instruction}]}]\n",
        "    input_text = tok.apply_chat_template(msgs, add_generation_prompt=True)\n",
        "    inputs = tok(image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        out = vlm.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, use_cache=True)\n",
        "    gen_ids = out[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    return tok.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "# Reload the base model for a fair comparison (no LoRA)\n",
        "base_model, base_tok = FastVisionModel.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    load_in_4bit=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")\n",
        "FastVisionModel.for_inference(base_model)\n",
        "FastVisionModel.for_inference(model)\n",
        "\n",
        "eval_small = eval_ds.select(range(min(25, len(eval_ds))))\n",
        "\n",
        "refs, base_preds, ft_preds = [], [], []\n",
        "for ex in tqdm(eval_small, desc=\"Evaluating\"):\n",
        "    img = ex[\"messages\"][0][\"content\"][1][\"image\"]\n",
        "    ref = ex[\"messages\"][1][\"content\"][0][\"text\"]\n",
        "    refs.append(ref)\n",
        "\n",
        "    base_preds.append(generate_caption(base_model, base_tok, img, INSTRUCTION))\n",
        "    ft_preds.append(generate_caption(model, tokenizer, img, INSTRUCTION))\n",
        "\n",
        "base_scores = rouge.compute(predictions=base_preds, references=refs)\n",
        "ft_scores   = rouge.compute(predictions=ft_preds,   references=refs)\n",
        "\n",
        "print(\"Base ROUGE:\", base_scores)\n",
        "print(\"FT   ROUGE:\", ft_scores)\n",
        "\n",
        "# Show a few qualitative examples\n",
        "for i in range(min(3, len(refs))):\n",
        "    print(\"=\"*100)\n",
        "    print(\"REF :\", refs[i][:300])\n",
        "    print(\"BASE:\", base_preds[i][:300])\n",
        "    print(\"FT  :\", ft_preds[i][:300])\n"
      ],
      "metadata": {
        "id": "oqNRmhjcRn1W"
      },
      "id": "oqNRmhjcRn1W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6286321a",
      "metadata": {
        "id": "6286321a"
      },
      "source": [
        "## 8) Save / load LoRA adapters\n",
        "\n",
        "This saves only the LoRA adapters (small), not the full base model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d252d486",
      "metadata": {
        "id": "d252d486"
      },
      "outputs": [],
      "source": [
        "SAVE_DIR = \"impact_qwen2vl_lora\"\n",
        "\n",
        "model.save_pretrained(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "\n",
        "print(\"‚úÖ Saved LoRA + tokenizer to:\", SAVE_DIR)\n",
        "\n",
        "# --- Optional: reload later for inference ---\n",
        "if False:\n",
        "    from unsloth import FastVisionModel\n",
        "    reloaded, reloaded_tok = FastVisionModel.from_pretrained(\n",
        "        model_name=SAVE_DIR,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    FastVisionModel.for_inference(reloaded)\n",
        "    print(\"Reloaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c21a7141",
      "metadata": {
        "id": "c21a7141"
      },
      "source": [
        "## 9) (Optional) Use the full Hugging Face dataset (advanced)\n",
        "\n",
        "The HF repo stores **yearly CSVs** and **yearly ZIPs**. ZIPs are multiple GB.  \n",
        "This section shows how to:\n",
        "- download a year CSV\n",
        "- optionally open the year ZIP and extract **only a handful** of images for a tiny experiment\n",
        "\n",
        "‚ö†Ô∏è This is optional and may still be slow depending on bandwidth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "479974f7",
      "metadata": {
        "id": "479974f7"
      },
      "outputs": [],
      "source": [
        "# --- OPTIONAL ---\n",
        "# Download one year CSV from Hugging Face (about ~35‚Äì60MB)\n",
        "from huggingface_hub import hf_hub_download, hf_hub_url\n",
        "import pandas as pd\n",
        "\n",
        "YEAR = \"2012\"  # pick 2007..2022\n",
        "csv_file = f\"{YEAR}.csv\"\n",
        "\n",
        "csv_path = hf_hub_download(repo_id=\"AI4Patents/IMPACT\", filename=csv_file, repo_type=\"dataset\")\n",
        "print(\"Downloaded:\", csv_path)\n",
        "\n",
        "full_df = pd.read_csv(csv_path)\n",
        "print(\"Rows:\", len(full_df))\n",
        "print(\"Columns:\", list(full_df.columns)[:30])\n",
        "\n",
        "# If you want images too, you can download the corresponding ZIP (multi-GB) OR try the \"remote zip\" trick.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b2c1d6d",
      "metadata": {
        "id": "0b2c1d6d"
      },
      "outputs": [],
      "source": [
        "# --- OPTIONAL (advanced): read a few images from the remote ZIP without downloading the whole file ---\n",
        "# This uses HTTP range requests + zipfile central directory reading.\n",
        "# Works only if the server supports range requests for the resolved ZIP URL.\n",
        "\n",
        "import zipfile, fsspec, io\n",
        "from huggingface_hub import hf_hub_url\n",
        "\n",
        "YEAR = \"2012\"\n",
        "zip_name = f\"{YEAR}.zip\"\n",
        "zip_url = hf_hub_url(repo_id=\"AI4Patents/IMPACT\", filename=zip_name, repo_type=\"dataset\")\n",
        "print(\"ZIP URL:\", zip_url)\n",
        "\n",
        "# open remote ZIP as a file-like object\n",
        "http_fs = fsspec.filesystem(\"https\")\n",
        "f = http_fs.open(zip_url, \"rb\")\n",
        "\n",
        "zf = zipfile.ZipFile(f)\n",
        "all_names = zf.namelist()\n",
        "print(\"Files inside zip:\", len(all_names))\n",
        "print(\"First 10 names:\", all_names[:10])\n",
        "\n",
        "# You now need to map rows in the CSV to filenames inside the ZIP.\n",
        "# A common pattern is that the CSV has a column containing the figure filename/path.\n",
        "# Inspect full_df columns to find it, then use zf.read(filename) to get bytes.\n",
        "\n",
        "# Example (pseudo):\n",
        "# img_bytes = zf.read(filename_from_csv)\n",
        "# image = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "128de908",
      "metadata": {
        "id": "128de908"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}