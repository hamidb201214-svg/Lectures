{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamidb201214-svg/Lectures/blob/main/M3_3_NLG_3_RAG_Mistral_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0f6f7b8",
      "metadata": {
        "id": "d0f6f7b8"
      },
      "source": [
        "\n",
        "## Simple Retrieval Augmented Generation (RAG)\n",
        "#### RAG with LangChain + Chroma + Hugging Face (Sentence Embeddings + Local ~2B LLM)\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*s_pbYF-jOTqSYrMG.png)\n",
        "\n",
        "To work with external files, LangChain provides data loaders that can be used to load documents from various sources. Combining LLMs with external data is generally referred to as Retrieval Augmented Generation (RAG).\n",
        "\n",
        "## Using the **“Attention Is All You Need”** paper as the knowledge source (PDF)\n",
        "\n",
        "This notebook builds a **Retrieval-Augmented Generation (RAG)** demo using:\n",
        "\n",
        "- **Sentence embeddings** from Hugging Face (**sentence-transformers**)\n",
        "- **ChromaDB** as the vector store\n",
        "- A **local** small LLM from Hugging Face (example: **Gemma 2B Instruct**)\n",
        "- The corpus is loaded from a **PDF** (default: *Attention Is All You Need*, Vaswani et al. 2017)\n",
        "\n",
        "> If you prefer to use a different PDF, just set `PDF_PATH` to your file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa2dd517",
      "metadata": {
        "id": "fa2dd517"
      },
      "outputs": [],
      "source": [
        "# Install (restart kernel after install if needed)\n",
        "%pip -q install -U \\\n",
        "  langchain \\\n",
        "  langchain-classic \\\n",
        "  langchain-chroma \\\n",
        "  langchain-huggingface \\\n",
        "  langchain-community \\\n",
        "  langchain-text-splitters \\\n",
        "  sentence-transformers \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  bitsandbytes \\\n",
        "  chromadb \\\n",
        "  pypdf \\\n",
        "  requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2701e68f",
      "metadata": {
        "id": "2701e68f"
      },
      "source": [
        "## 1) Hugging Face token (needed for gated models)\n",
        "\n",
        "Set `HF_TOKEN` or `HUGGINGFACEHUB_API_TOKEN`.\n",
        "- If you're using Gemma, you must accept the model license on Hugging Face first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5914469",
      "metadata": {
        "id": "d5914469"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "\n",
        "# Pick one env var name and stick to it:\n",
        "if not (os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")):\n",
        "    token = getpass.getpass(\"Hugging Face token (for gated models): \")\n",
        "    os.environ[\"HF_TOKEN\"] = token\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99234973",
      "metadata": {
        "id": "99234973"
      },
      "source": [
        "## 2) Download + load the Attention paper (PDF)\n",
        "\n",
        "Default: **arXiv PDF** for *Attention Is All You Need*.\n",
        "\n",
        "If your environment has **no internet**, download the PDF manually and set `PDF_PATH` to that file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6cab6b5",
      "metadata": {
        "id": "d6cab6b5"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "# You can replace this with your own PDF.\n",
        "ARXIV_PDF_URL = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
        "\n",
        "DATA_DIR = Path(\"./data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "PDF_PATH = DATA_DIR / \"attention_is_all_you_need.pdf\"\n",
        "\n",
        "# Download if missing (requires internet)\n",
        "if not PDF_PATH.exists():\n",
        "    print(\"Downloading:\", ARXIV_PDF_URL)\n",
        "    r = requests.get(ARXIV_PDF_URL, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    PDF_PATH.write_bytes(r.content)\n",
        "\n",
        "print(\"PDF path:\", PDF_PATH.resolve())\n",
        "print(\"PDF size (MB):\", round(PDF_PATH.stat().st_size / 1e6, 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca2cd0a",
      "metadata": {
        "id": "eca2cd0a"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(str(PDF_PATH))\n",
        "docs = loader.load()\n",
        "\n",
        "print(\"Loaded pages:\", len(docs))\n",
        "print(\"Example metadata:\", docs[0].metadata)\n",
        "print(\"\\nFirst page snippet:\\n\", docs[0].page_content[:500], \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a20e111e",
      "metadata": {
        "id": "a20e111e"
      },
      "source": [
        "## 3) Split documents into chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a590c262",
      "metadata": {
        "id": "a590c262"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=150)\n",
        "splits = splitter.split_documents(docs)\n",
        "\n",
        "print(\"Chunks:\", len(splits))\n",
        "print(\"Example chunk:\\n\", splits[0].page_content[:300], \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a265c0ba",
      "metadata": {
        "id": "a265c0ba"
      },
      "source": [
        "## 4) Sentence embeddings from Hugging Face (sentence-transformers)\n",
        "\n",
        "A popular fast baseline: `sentence-transformers/all-MiniLM-L6-v2`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18511894",
      "metadata": {
        "id": "18511894"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    # Good default for cosine similarity:\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "# Quick sanity check:\n",
        "vec = embeddings.embed_query(\"hello world\")\n",
        "print(\"Embedding dim:\", len(vec))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db18d0c",
      "metadata": {
        "id": "2db18d0c"
      },
      "source": [
        "## 5) Store embeddings in Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91dafca5",
      "metadata": {
        "id": "91dafca5"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "PERSIST_DIR = \"./chroma_attention_paper_db\"\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"attention_paper_rag\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=PERSIST_DIR,\n",
        ")\n",
        "\n",
        "vector_store.add_documents(splits)\n",
        "\n",
        "print(\"Persist dir:\", PERSIST_DIR)\n",
        "print(\"Count:\", vector_store._collection.count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b467eec",
      "metadata": {
        "id": "8b467eec"
      },
      "source": [
        "## 6) Load a ~2B model locally (example: Gemma 2B Instruct)\n",
        "\n",
        "Model options you can try:\n",
        "- `google/gemma-2b-it` (2B, gated on HF)\n",
        "- `BSC-LT/salamandraTA-2b-instruct` (2B, Apache-2.0)\n",
        "- If you can accept slightly smaller: `HuggingFaceTB/SmolLM2-1.7B-Instruct`\n",
        "\n",
        "The cell below tries **4-bit** (bitsandbytes) if available; otherwise it falls back to standard loading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "386329d2",
      "metadata": {
        "id": "386329d2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "\n",
        "# Pick your model here:\n",
        "MODEL_ID = \"google/gemma-2b-it\"   # gated\n",
        "# MODEL_ID = \"BSC-LT/salamandraTA-2b-instruct\"  # open\n",
        "# MODEL_ID = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"  # smaller but easy to run\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        token=HF_TOKEN,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=\"auto\",\n",
        "    )\n",
        "    print(\"Loaded in 4-bit.\")\n",
        "except Exception as e:\n",
        "    print(\"4-bit load failed, falling back to standard load. Error was:\\n\", str(e)[:500], \"...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        token=HF_TOKEN,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=\"auto\",\n",
        "    )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
        "\n",
        "gen_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    do_sample=False,\n",
        "    temperature=0.0,\n",
        "    return_full_text=False,\n",
        ")\n",
        "\n",
        "# Small smoke test:\n",
        "out = gen_pipe(\"Say hello in one short sentence.\")[0][\"generated_text\"]\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a74135dd",
      "metadata": {
        "id": "a74135dd"
      },
      "source": [
        "## 7) Build a RAG chain (retriever + prompt + LLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d23c052e",
      "metadata": {
        "id": "d23c052e"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Wrap pipeline for LangChain\n",
        "llm = HuggingFacePipeline(pipeline=gen_pipe)\n",
        "\n",
        "# Treat it like a chat model (works well with instruct models)\n",
        "chat_model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "# In LangChain v1+, helper chains live in langchain-classic:\n",
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a helpful assistant. Answer the question using ONLY the context from the Attention paper.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "If the context does not contain the answer, say you don't know.\"\"\"\n",
        ")\n",
        "\n",
        "doc_chain = create_stuff_documents_chain(chat_model, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, doc_chain)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88a2834d",
      "metadata": {
        "id": "88a2834d"
      },
      "source": [
        "## 8) Ask questions + inspect retrieved sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cda1215",
      "metadata": {
        "id": "5cda1215"
      },
      "outputs": [],
      "source": [
        "question = \"What is multi-head attention and why is it useful in the Transformer?\"\n",
        "\n",
        "result = rag_chain.invoke({\"input\": question})\n",
        "\n",
        "print(\"ANSWER:\\n\", result[\"answer\"])\n",
        "print(\"\\nSOURCES (top retrieved chunks):\")\n",
        "for i, d in enumerate(result[\"context\"], 1):\n",
        "    print(f\"\\n--- Chunk {i} ---\")\n",
        "    print(\"metadata:\", d.metadata)\n",
        "    print(d.page_content[:400], \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35d51484",
      "metadata": {
        "id": "35d51484"
      },
      "source": [
        "## 9) (Optional) Reload the persisted Chroma DB later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47cb6359",
      "metadata": {
        "id": "47cb6359"
      },
      "outputs": [],
      "source": [
        "# vector_store_reloaded = Chroma(\n",
        "#     collection_name=\"attention_paper_rag\",\n",
        "#     embedding_function=embeddings,\n",
        "#     persist_directory=PERSIST_DIR,\n",
        "# )\n",
        "# print(vector_store_reloaded._collection.count())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}