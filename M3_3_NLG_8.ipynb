{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamidb201214-svg/Lectures/blob/main/M3_3_NLG_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How Smart Are They? Understanding the Scale of GPT-3 and GPT-4\n"
      ],
      "metadata": {
        "id": "J3ZgnpAvZ-_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Assumption                                  | Description                                                                                       |\n",
        "|---------------------------------------------|---------------------------------------------------------------------------------------------------|\n",
        "| **Average Tokens per Book**                 | Estimated at 135,000 tokens per book, based on an average book length of 80,000 to 100,000 words.  |\n",
        "| **Average Reading Lifetime of an Individual** | Estimated at 510 books per lifetime, assuming a moderate reading habit of 5-12 books per year over 60 years. |\n",
        "| **Tokens per Word**                         | Estimated at 1.5 tokens per word, accounting for spaces and punctuation.                          |\n",
        "\n"
      ],
      "metadata": {
        "id": "xE_UeBjWZlQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Detail                             | GPT-3                                   | GPT-4                                   |\n",
        "|------------------------------------|-----------------------------------------|-----------------------------------------|\n",
        "| **Developed By**                   | OpenAI                                  | OpenAI                                  |\n",
        "| **Approximate Training Data Size** | 45 terabytes of text data               | Larger than GPT-3 (exact size unknown)  |\n",
        "| **Estimated Token Count**          | 300-400 billion tokens                  | Likely over 500 billion tokens          |\n",
        "| **Equivalent Number of Books**     | 2,222,222 - 2,962,963 books             | >3,703,704 books                        |\n",
        "| **Equivalent Knowledge of People** | 4,356 - 5,810 people                    | >7,263 people                           |\n"
      ],
      "metadata": {
        "id": "vxvz6Gw_ZjY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/transformermodel_architecture.png)"
      ],
      "metadata": {
        "id": "_Ra10qce4Nx2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j431XmcTOzCY"
      },
      "source": [
        "# Why adapt the language model?\n",
        "\n",
        "- LMs are trained in a task-agnostic way.\n",
        "- Downstream tasks can be very different from language modeling on the Pile.\n",
        "For example, consider the natural language inference (NLI) task (is the hypothesis entailed by the premise?):\n",
        "\n",
        "      Premise: I have never seen an apple that is not red.\n",
        "      Hypothesis: I have never seen an apple.\n",
        "      Correct output: Not entailment (the reverse direction would be entailment)\n",
        "\n",
        "- The format of such a task may not be very natural for the model.\n",
        "\n",
        "# Ways downstream tasks can be different\n",
        "\n",
        "- **Formatting**: for example, NLI takes in two sentences and compares them to produce a single binary output. This is different from generating the next token or filling in MASKs. Another example is the presence of MASK tokens in BERT training vs. no MASKs in downstream tasks.\n",
        "- **Topic shift**: the downstream task is focused on a new or very specific topic (e.g., medical records)\n",
        "- **Temporal shift**: the downstream task requires new knowledge that is unavailable during pre-training because 1) the knowledge is new (e.g., GPT3 was trained before Biden became President), 2) the knowledge for the downstream task is not publicly available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEGxCwH7-L2q"
      },
      "source": [
        "\n",
        "# Optimizing Large Language Models\n",
        "\n",
        "There are several options to optimize Large Language Models:\n",
        "\n",
        "    Prompt engineering by providing samples (In-Context Learning)\n",
        "    Prompt Tuning\n",
        "    Fine-Tuning\n",
        "       - Supervised fine-tuning (SFT): Classic fine-tuning by changing all weights\n",
        "       - Transfer Learning - PEFT fine-tuning by changing only a few weights\n",
        "       - Reinforcement Learning Human Feedback (RLHF)\n",
        "\n",
        "An important question is which of these options is the most effective one and which one can overwrite previous optimizations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Prompt Engineering, Prompt Tuning, and PEFT\n",
        "These techniques are essential for efficiently adapting large, pre-trained models like GPT or BERT to specialized tasks or domains, optimizing resource usage and reducing training time.\n",
        "\n",
        "\n",
        "1. **Prompt Engineering (In-Context Learning)**:\n",
        "   - **Definition**: Crafting input prompts to guide a Large Language Model (LLM) for desired outputs.\n",
        "   - **Application**: Uses natural language prompts to \"program\" the LLM, leveraging its contextual understanding.\n",
        "   - **Model Change**: No alteration to the model's parameters; relies on the model's existing knowledge and interpretive abilities.\n",
        "\n",
        "2. **Prompt Tuning**:\n",
        "   - **Difference from Prompt Engineering**: Involves appending a trainable tensor (prompt tokens) to the LLM's input embeddings.\n",
        "   - **Process**: Fine-tunes this tensor for a specific task and dataset, keeping other model parameters unchanged.\n",
        "   - **Example**: Adapting a general LLM for specific tasks like sentiment classification by adjusting prompt tokens.\n",
        "\n",
        "3. **Parameter-Efficient Fine-Tuning (PEFT)**:\n",
        "   - **Overview**: A set of techniques to enhance model performance on specific tasks or datasets by tuning a small subset of parameters.\n",
        "   - **Objective**: Targeted improvements without the need for full model retraining.\n",
        "   - **Relation to Prompt Tuning**: Prompt tuning is a subset of PEFT, focusing on fine-tuning specific parts of the model for task/domain adaptation.\n",
        "\n"
      ],
      "metadata": {
        "id": "5bJ78Ja2Urh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/PEFT_LLMs.png)"
      ],
      "metadata": {
        "id": "F2alv_qaRfHx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npsUW-pPKYp8"
      },
      "source": [
        "### Challenges\n",
        "\n",
        "Fine-tuning models can certainly help to get models to do what you want them to do. However, there are some potential issues:\n",
        "\n",
        "> - **Catastrophic forgetting**: This phenomenon describes a behavior when fine-tuning or prompts can overwrite the pre-trained model characteristics.\n",
        "> - **Overfitting**: If only a certain AI task has been fine-tuned, other tasks can suffer in terms of performance.\n",
        "\n",
        "In general, fine-tuning should be used wisely and best practices should be applied, for example, the quality of the data is more important than the quantity and multiple AI tasks should be fine-tuned at the same time vs after each other."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications\n",
        "\n",
        "There are many platforms that can be used for LLMs' applications:\n"
      ],
      "metadata": {
        "id": "FYi_A6S-NHXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Tool                                                                                                    | Category                             | Best For                                                                         | Type        |\n",
        "| :------------------------------------------------------------------------------------------------------ | :----------------------------------- | :------------------------------------------------------------------------------- | :---------- |\n",
        "| **[LangChain](https://docs.langchain.com)**                                                             | Orchestration                        | Agents, tools, RAG, observability                                                | Open-source |\n",
        "| **[Flowise](https://docs.flowiseai.com)**                                                               | App Builder / Orchestration (Visual) | Low-code drag-and-drop LLM apps (chatbots, RAG flows), rapid prototyping         | Open-source |\n",
        "| **[CrewAI](https://docs.crewai.com)**                                                                   | Agent Orchestration (Multi-agent)    | Role-based multi-agent workflows, task delegation, coordinated tool-using agents | Open-source |\n",
        "| **[Hugging Face](https://huggingface.co/docs)**                                                         | Model Hub                            | Open models, fine-tuning, hosting                                                | Platform    |\n",
        "| **[vLLM](https://docs.vllm.ai)** / **[SGLang](https://github.com/sgl-project/sglang)**                  | Serving                              | High-throughput / Structured generation                                          | Open-source |\n",
        "| **[Ollama](https://github.com/ollama/ollama)** / **[llama.cpp](https://github.com/ggml-org/llama.cpp)** | Local Run                            | Local inference & model management                                               | Open-source |\n",
        "| **[bitsandbytes](https://huggingface.co/docs/transformers/en/quantization/bitsandbytes)**               | Quantization (4/8-bit)               | Fit models into less VRAM; decent speed/quality tradeoffs                        | Open-source |\n",
        "| **[Pydantic](https://docs.pydantic.dev/)**                                                              | Validation / Schemas                 | Type-safe data validation; enforce structured outputs and tool I/O               | Open-source |\n",
        "| **[LlamaIndex](https://docs.llamaindex.ai)**                                                            | Data / RAG                           | Ingestion, indexing, retrieval                                                   | Open-source |\n",
        "| **[Haystack](https://haystack.deepset.ai)**                                                             | RAG Pipelines                        | Production pipelines, Doc QA                                                     | Open-source |\n",
        "| **[Semantic Kernel](https://github.com/microsoft/semantic-kernel)**                                     | Orchestration                        | Enterprise workflows (C#/Python)                                                 | Open-source |\n"
      ],
      "metadata": {
        "id": "lk2E5h829RJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "VaGwQIZsgI_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Delete the model and any other large tensors\n",
        "del model\n",
        "del tokenizer\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# Clear the PyTorch CUDA cache\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "1wQdXJUDh_Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
        "\n",
        "# load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# prepare the model input\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content\n",
        "try:\n",
        "    # rindex finding 151668 (</think>)\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "print(\"thinking content:\", thinking_content) # no opening <think> tag\n",
        "print(\"content:\", content)\n"
      ],
      "metadata": {
        "id": "8LmUkdoVf9ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "P1M7YDGxgr74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes>=0.46.1"
      ],
      "metadata": {
        "id": "NKaVLy9NIYQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,   # <-- match fp16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,              # <-- match fp16\n",
        "    quantization_config=bnb_config,\n",
        ")"
      ],
      "metadata": {
        "id": "tFWt5k9lITEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# prepare the model input\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content\n",
        "try:\n",
        "    # rindex finding 151668 (</think>)\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "print(\"thinking content:\", thinking_content) # no opening <think> tag\n",
        "print(\"content:\", content)\n"
      ],
      "metadata": {
        "id": "ycnfbKDAGCWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "yTuKrWMMGSuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Tool                                                                                                                     | Category                     | Best For                                                                         | Type        |\n",
        "| :----------------------------------------------------------------------------------------------------------------------- | :--------------------------- | :------------------------------------------------------------------------------- | :---------- |\n",
        "| **[Transformers](https://huggingface.co/docs/transformers)**                                                             | Python Inference (HF-native) | `pipeline()` / `generate()`, chat templates, quick prototyping                   | Open-source |\n",
        "| **[Accelerate](https://huggingface.co/docs/accelerate)**                                                                 | Loading / Sharding           | `device_map=\"auto\"`, CPU/GPU split, offload big models that don’t fit            | Open-source |\n",
        "| **[huggingface_hub](https://huggingface.co/docs/huggingface_hub)**                                                       | Download / Caching           | `snapshot_download()` / caching / pinned revisions for reproducible loads        | Open-source |\n",
        "| **[PEFT](https://huggingface.co/docs/peft)**                                                                             | Adapters                     | Load LoRA/adapters on top of a base Mistral (cheap “fine-tune” deployments)      | Open-source |\n",
        "| **[bitsandbytes](https://huggingface.co/docs/transformers/en/quantization/bitsandbytes)**                                | Quantization (4/8-bit)       | Fit models into less VRAM; decent speed/quality tradeoffs                        | Open-source |\n",
        "| **[Transformers Quantization (AWQ/GPTQ)](https://huggingface.co/docs/transformers/en/main_classes/quantization)**        | Quantization (algos)         | Using AWQ/GPTQ paths supported by Transformers for inference workflows           | Open-source |\n",
        "| **[AutoAWQ](https://github.com/casper-hansen/AutoAWQ)**                                                                  | Quantization + Kernels       | INT4 AWQ quantization and fast inference for AWQ checkpoints                     | Open-source |\n",
        "| **[vLLM](https://docs.vllm.ai)**                                                                                         | Serving                      | High-throughput serving + OpenAI-compatible API server                           | Open-source |\n",
        "| **[SGLang](https://github.com/sgl-project/sglang)**                                                                      | Serving / Structured Gen     | Low-latency serving + structured generation runtime patterns                     | Open-source |\n",
        "| **[Text Generation Inference (TGI)](https://huggingface.co/docs/text-generation-inference/en/index)**                    | Serving                      | Classic HF inference server (see note on maintenance mode)                       | Open-source |\n",
        "| **[Ollama](https://github.com/ollama/ollama)**                                                                           | Local Run / Model Mgmt       | “Just run it locally” experience + simple local API                              | Open-source |\n",
        "| **[llama.cpp](https://github.com/ggml-org/llama.cpp)**                                                                   | Local Run (GGUF)             | CPU-friendly / wide-hardware inference via GGUF models                           | Open-source |\n",
        "| **[HF Inference Endpoints](https://huggingface.co/docs/inference-endpoints/en/index)**                                   | Managed Serving              | Deploy HF Hub models on managed infra (autoscaling, logs/metrics)                | Managed     |\n",
        "| **[HF Inference Client / Providers](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client)** | Hosted Inference API         | Call hosted endpoints/providers (and also talk to local servers) with one client | Platform    |\n"
      ],
      "metadata": {
        "id": "x1TxySqqDVR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Literal, Optional\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Pydantic models (schemas)\n",
        "# -----------------------------\n",
        "class ChatMessage(BaseModel):\n",
        "    role: Literal[\"system\", \"user\", \"assistant\"]\n",
        "    content: str = Field(min_length=1)\n",
        "\n",
        "class GenerationRequest(BaseModel):\n",
        "    prompt: str = Field(min_length=1)\n",
        "    max_new_tokens: int = Field(default=512, ge=1, le=4096)\n",
        "\n",
        "class GenerationResult(BaseModel):\n",
        "    thinking: str = \"\"\n",
        "    answer: str = Field(min_length=1)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Your code, with validation\n",
        "# -----------------------------\n",
        "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "req = GenerationRequest(prompt=\"Give me a short introduction to large language model.\", max_new_tokens=512)\n",
        "\n",
        "# validate messages\n",
        "messages: List[ChatMessage] = [ChatMessage(role=\"user\", content=req.prompt)]\n",
        "messages_dicts = [m.model_dump() for m in messages]  # convert to plain dicts for HF\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages_dicts,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=req.max_new_tokens)\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content (your logic)\n",
        "try:\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token id\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "# validate / package output\n",
        "try:\n",
        "    result = GenerationResult(thinking=thinking_content, answer=content)\n",
        "except ValidationError as e:\n",
        "    # e.g. answer empty -> you get a clean error instead of silent bad data\n",
        "    raise\n",
        "\n",
        "print(\"thinking content:\", result.thinking)\n",
        "print(\"content:\", result.answer)\n"
      ],
      "metadata": {
        "id": "NrK-z6q7QpPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain\n",
        "## Deep Agents overview\n",
        "\n"
      ],
      "metadata": {
        "id": "V_fWlscoU42h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Install dependencies"
      ],
      "metadata": {
        "id": "AzdWG_F0PULe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepagents tavily-python"
      ],
      "metadata": {
        "id": "XztJAgs7OZoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Set up your API keys"
      ],
      "metadata": {
        "id": "GQDqA7z8PXXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = getpass(\"Enter GEMINI_API_KEY: \").strip()\n",
        "os.environ[\"TAVILY_API_KEY\"] = getpass(\"Enter TAVILY_API_KEY: \").strip()\n",
        "\n",
        "print('export GEMINI_API_KEY=\"***\"')\n",
        "print('export TAVILY_API_KEY=\"***\"')"
      ],
      "metadata": {
        "id": "T9AqK5JZOvXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Create a search tool"
      ],
      "metadata": {
        "id": "7NJ5pWDBPce_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Literal\n",
        "from tavily import TavilyClient\n",
        "from deepagents import create_deep_agent\n",
        "\n",
        "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "def internet_search(\n",
        "    query: str,\n",
        "    max_results: int = 5,\n",
        "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
        "    include_raw_content: bool = False,\n",
        "):\n",
        "    \"\"\"Run a web search\"\"\"\n",
        "    return tavily_client.search(\n",
        "        query,\n",
        "        max_results=max_results,\n",
        "        include_raw_content=include_raw_content,\n",
        "        topic=topic,\n",
        "    )"
      ],
      "metadata": {
        "id": "5Ve75I95PL8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Create a deep agent"
      ],
      "metadata": {
        "id": "JSrJ3xK3PgCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# System prompt to steer the agent to be an expert researcher\n",
        "research_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n",
        "\n",
        "You have access to an internet search tool as your primary means of gathering information.\n",
        "\n",
        "## `internet_search`\n",
        "\n",
        "Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the Gemini model using the GEMINI_API_KEY set earlier\n",
        "# The model name 'gemini-1.5-flash' is a common and capable choice for general tasks.\n",
        "model = init_chat_model(model=\"google_genai:gemini-2.5-flash-lite\")\n",
        "\n",
        "agent = create_deep_agent(\n",
        "    model=model, # Explicitly pass the initialized model\n",
        "    tools=[internet_search],\n",
        "    system_prompt=research_instructions\n",
        ")"
      ],
      "metadata": {
        "id": "xj0cSriiPi9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Run the agent"
      ],
      "metadata": {
        "id": "pKqtAdNVPnvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]})\n",
        "\n",
        "# Print the agent's response\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "6AWOHMC7Po_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "BNewzQ6zUkSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip -q install -U google-genai\n",
        "\n",
        "import os\n",
        "from google import genai\n",
        "\n",
        "api_key = os.environ.get(\"GOOGLE_API_KEY\") or os.environ.get(\"GEMINI_API_KEY\")\n",
        "client = genai.Client(api_key=api_key)\n",
        "\n",
        "available = []\n",
        "for m in client.models.list():\n",
        "    # Docs example uses m.supported_actions and checks for \"generateContent\"\n",
        "    if \"generateContent\" in getattr(m, \"supported_actions\", []):\n",
        "        available.append(m.name.replace(\"models/\", \"\"))\n",
        "\n",
        "print(\"Models that support generateContent:\")\n",
        "for name in available[:50]:\n",
        "    print(\" -\", name)\n"
      ],
      "metadata": {
        "id": "l1Dm-6vsR6_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface"
      ],
      "metadata": {
        "id": "nwzPi19qtyaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install -U \"protobuf>=5.26.1,<6\" \"grpcio-status>=1.71.2,<2\" jedi\n"
      ],
      "metadata": {
        "id": "OfNvZN12aG1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f vllm || true\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "9ABkLltGaPDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langgraph deepagents \"langchain[openai]\" \"langchain[google-genai]\""
      ],
      "metadata": {
        "id": "wzb2xnZhHfuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Human-in-the-loop"
      ],
      "metadata": {
        "id": "ka5gzDAhdmSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learn how to configure human approval for sensitive tool operations\n",
        "\n",
        "Some tool operations may be sensitive and require human approval before execution. Deep agents support human-in-the-loop workflows through LangGraph’s interrupt capabilities. You can configure which tools require approval using the interrupt_on parameter."
      ],
      "metadata": {
        "id": "ygvDnWf7dpeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import uuid\n",
        "import getpass\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from langchain.tools import tool\n",
        "from langchain.chat_models import init_chat_model\n",
        "from deepagents import create_deep_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.types import Command\n",
        "\n",
        "# -----------------------------\n",
        "# 0) Provider selection + API key prompting\n",
        "# -----------------------------\n",
        "OPENAI_DEFAULT_MODEL = \"openai:gpt-4o-mini\"\n",
        "GEMINI_DEFAULT_MODEL = \"google_genai:gemini-2.5-flash-lite\"\n",
        "\n",
        "def choose_provider(cli_value: Optional[str]) -> str:\n",
        "    if cli_value in {\"openai\", \"gemini\"}:\n",
        "        return cli_value\n",
        "\n",
        "    # Interactive prompt if not provided\n",
        "    while True:\n",
        "        choice = input(\"Choose provider [openai/gemini] (default: openai): \").strip().lower()\n",
        "        if choice == \"\":\n",
        "            return \"openai\"\n",
        "        if choice in {\"openai\", \"gemini\"}:\n",
        "            return choice\n",
        "        print(\"Please type 'openai' or 'gemini'.\")\n",
        "\n",
        "def ensure_api_key(provider: str) -> None:\n",
        "    \"\"\"\n",
        "    Prompt for the provider's API key if missing, and store in env.\n",
        "    - OpenAI: OPENAI_API_KEY\n",
        "    - Gemini: GOOGLE_API_KEY (LangChain checks this first; GEMINI_API_KEY is also supported as fallback)\n",
        "    \"\"\"\n",
        "    if provider == \"openai\":\n",
        "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "            key = getpass.getpass(\"Enter OPENAI_API_KEY (input hidden): \").strip()\n",
        "            if not key:\n",
        "                raise RuntimeError(\"OPENAI_API_KEY was not provided.\")\n",
        "            os.environ[\"OPENAI_API_KEY\"] = key\n",
        "\n",
        "    elif provider == \"gemini\":\n",
        "        # Prefer GOOGLE_API_KEY because that's what LangChain docs show; GEMINI_API_KEY is also accepted.\n",
        "        if not (os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")):\n",
        "            key = getpass.getpass(\"Enter GOOGLE_API_KEY (Gemini) (input hidden): \").strip()\n",
        "            if not key:\n",
        "                raise RuntimeError(\"GOOGLE_API_KEY was not provided.\")\n",
        "            os.environ[\"GOOGLE_API_KEY\"] = key\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unknown provider. Use 'openai' or 'gemini'.\")\n",
        "\n",
        "def pick_model_id(provider: str, override: Optional[str]) -> str:\n",
        "    if override:\n",
        "        return override\n",
        "    return OPENAI_DEFAULT_MODEL if provider == \"openai\" else GEMINI_DEFAULT_MODEL\n",
        "\n",
        "# -----------------------------\n",
        "# Storage layout: ./submissions/<student_id>/*\n",
        "# -----------------------------\n",
        "ROOT = Path(\"./submissions\").resolve()\n",
        "ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "def _student_dir(student_id: str) -> Path:\n",
        "    p = (ROOT / student_id).resolve()\n",
        "    if ROOT not in p.parents:\n",
        "        raise ValueError(\"Invalid student_id (path traversal blocked).\")\n",
        "    p.mkdir(exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def _list_files(student_id: str) -> List[str]:\n",
        "    d = _student_dir(student_id)\n",
        "    return sorted([p.name for p in d.iterdir() if p.is_file()])\n",
        "\n",
        "def _read_file(student_id: str, filename: str) -> str:\n",
        "    d = _student_dir(student_id)\n",
        "    p = (d / filename).resolve()\n",
        "    if d not in p.parents:\n",
        "        raise ValueError(\"Invalid filename (path traversal blocked).\")\n",
        "    if not p.exists():\n",
        "        return \"\"\n",
        "    return p.read_text(encoding=\"utf-8\")\n",
        "\n",
        "def _append_outbox(text: str) -> None:\n",
        "    outbox = ROOT / \"OUTBOX.txt\"\n",
        "    existing = outbox.read_text(encoding=\"utf-8\") if outbox.exists() else \"\"\n",
        "    outbox.write_text(existing + text, encoding=\"utf-8\")\n",
        "\n",
        "# -----------------------------\n",
        "# Tools (LangChain)\n",
        "# -----------------------------\n",
        "@tool\n",
        "def list_submission_files(student_id: str) -> List[str]:\n",
        "    \"\"\"List files in a student's submission folder.\"\"\"\n",
        "    return _list_files(student_id)\n",
        "\n",
        "@tool\n",
        "def read_submission_file(student_id: str, filename: str) -> str:\n",
        "    \"\"\"Read a file from a student's submission folder.\"\"\"\n",
        "    text = _read_file(student_id, filename)\n",
        "    if text == \"\":\n",
        "        return f\"(empty or missing) {filename}\"\n",
        "    return text\n",
        "\n",
        "@tool\n",
        "def auto_validate(student_id: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run simple validity checks and return a report + recommended verdict.\n",
        "    Verdict: 'valid' or 'resubmit'\n",
        "    \"\"\"\n",
        "    files = _list_files(student_id)\n",
        "    required = {\"report.md\", \"solution.py\"}\n",
        "    missing_files = sorted(list(required - set(files)))\n",
        "\n",
        "    report = _read_file(student_id, \"report.md\")\n",
        "    solution = _read_file(student_id, \"solution.py\")\n",
        "\n",
        "    required_headings = [\"# Problem\", \"# Method\", \"# Results\"]\n",
        "    missing_headings = [h for h in required_headings if h not in report]\n",
        "\n",
        "    has_required_function = \"def solve(\" in solution\n",
        "\n",
        "    issues = []\n",
        "    if missing_files:\n",
        "        issues.append(f\"Missing required files: {missing_files}\")\n",
        "    if \"report.md\" in files and missing_headings:\n",
        "        issues.append(f\"Missing required headings in report.md: {missing_headings}\")\n",
        "    if \"solution.py\" in files and not has_required_function:\n",
        "        issues.append(\"solution.py missing required function signature: def solve(...)\")\n",
        "\n",
        "    recommended_verdict = \"valid\" if not issues else \"resubmit\"\n",
        "\n",
        "    recommended_message = (\n",
        "        \"✅ Your submission looks valid. Nice work!\"\n",
        "        if recommended_verdict == \"valid\"\n",
        "        else \"⚠️ Please fix the issues listed and resubmit.\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"student_id\": student_id,\n",
        "        \"files\": files,\n",
        "        \"issues\": issues,\n",
        "        \"recommended_verdict\": recommended_verdict,\n",
        "        \"recommended_message\": recommended_message,\n",
        "    }\n",
        "\n",
        "@tool\n",
        "def record_verdict(student_id: str, verdict: str, notes: str) -> str:\n",
        "    \"\"\"\n",
        "    Record the official verdict (sensitive).\n",
        "    Writes to ./submissions/verdicts.json\n",
        "    \"\"\"\n",
        "    out = ROOT / \"verdicts.json\"\n",
        "    data = json.loads(out.read_text(encoding=\"utf-8\")) if out.exists() else {}\n",
        "    data[student_id] = {\"verdict\": verdict, \"notes\": notes}\n",
        "    out.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
        "    return f\"Recorded verdict for {student_id}: {verdict}\"\n",
        "\n",
        "@tool\n",
        "def message_student(student_id: str, message: str) -> str:\n",
        "    \"\"\"\n",
        "    Mock messaging (sensitive).\n",
        "    Appends to ./submissions/OUTBOX.txt instead of actually sending email.\n",
        "    \"\"\"\n",
        "    _append_outbox(f\"\\n=== TO {student_id} ===\\n{message}\\n\")\n",
        "    return f\"Queued message to {student_id} (see submissions/OUTBOX.txt)\"\n",
        "\n",
        "# -----------------------------\n",
        "# Console HITL \"review UI\"\n",
        "# -----------------------------\n",
        "def _prompt_decision(tool_name: str, args: Dict[str, Any], allowed: List[str]) -> Dict[str, Any]:\n",
        "    print(\"\\n--- HUMAN REVIEW REQUIRED ---\")\n",
        "    print(f\"Tool: {tool_name}\")\n",
        "    print(\"Proposed args:\")\n",
        "    print(json.dumps(args, indent=2))\n",
        "    print(f\"Allowed decisions: {allowed}\")\n",
        "\n",
        "    while True:\n",
        "        choice = input(\"Type approve / reject / edit: \").strip().lower()\n",
        "        if choice == \"approve\" and \"approve\" in allowed:\n",
        "            return {\"type\": \"approve\"}\n",
        "        if choice == \"reject\" and \"reject\" in allowed:\n",
        "            return {\"type\": \"reject\"}\n",
        "        if choice == \"edit\" and \"edit\" in allowed:\n",
        "            print(\n",
        "                \"Paste edited args as JSON \"\n",
        "                \"(e.g. {\\\"student_id\\\": \\\"student_001\\\", \\\"verdict\\\": \\\"valid\\\", \\\"notes\\\": \\\"...\\\"})\"\n",
        "            )\n",
        "            edited_args = json.loads(input(\"> \").strip())\n",
        "            return {\"type\": \"edit\", \"edited_action\": {\"name\": tool_name, \"args\": edited_args}}\n",
        "        print(\"Invalid choice for this tool. Try again.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Main runner\n",
        "# -----------------------------\n",
        "def run(student_id: str, provider: str, model_id: str) -> None:\n",
        "    # Ensure correct key exists before model init\n",
        "    ensure_api_key(provider)\n",
        "\n",
        "    checkpointer = MemorySaver()\n",
        "\n",
        "    # init_chat_model accepts provider:model identifiers like openai:... and google_genai:...\n",
        "    model = init_chat_model(model_id)\n",
        "\n",
        "    agent = create_deep_agent(\n",
        "        model=model,\n",
        "        tools=[\n",
        "            list_submission_files,\n",
        "            read_submission_file,\n",
        "            auto_validate,\n",
        "            record_verdict,\n",
        "            message_student,\n",
        "        ],\n",
        "        system_prompt=(\n",
        "            \"You are a TA agent.\\n\"\n",
        "            \"Workflow:\\n\"\n",
        "            \"1) Call auto_validate(student_id).\\n\"\n",
        "            \"2) Summarize the issues (if any).\\n\"\n",
        "            \"3) Propose record_verdict(student_id, verdict, notes).\\n\"\n",
        "            \"4) If helpful, propose message_student(student_id, message).\\n\"\n",
        "            \"Keep notes short and factual.\"\n",
        "        ),\n",
        "        interrupt_on={\n",
        "            # Sensitive: human must approve/edit/reject official verdict\n",
        "            \"record_verdict\": True,  # default allows approve/edit/reject\n",
        "            # Sensitive: outbound message needs approval (no edit allowed here)\n",
        "            \"message_student\": {\"allowed_decisions\": [\"approve\", \"reject\"]},\n",
        "            # Safe: no interrupts\n",
        "            \"auto_validate\": False,\n",
        "            \"read_submission_file\": False,\n",
        "            \"list_submission_files\": False,\n",
        "        },\n",
        "        checkpointer=checkpointer,\n",
        "    )\n",
        "\n",
        "    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
        "    user_prompt = (\n",
        "        f\"Validate {student_id}. \"\n",
        "        \"Run auto checks, then record an official verdict, and message the student with next steps.\"\n",
        "    )\n",
        "\n",
        "    result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_prompt}]}, config=config)\n",
        "\n",
        "    while result.get(\"__interrupt__\"):\n",
        "        payload = result[\"__interrupt__\"][0].value\n",
        "        action_requests = payload[\"action_requests\"]\n",
        "        review_configs = {cfg[\"action_name\"]: cfg for cfg in payload[\"review_configs\"]}\n",
        "\n",
        "        decisions = []\n",
        "        for action in action_requests:\n",
        "            name = action[\"name\"]\n",
        "            args = action[\"args\"]\n",
        "            allowed = review_configs[name][\"allowed_decisions\"]\n",
        "            decisions.append(_prompt_decision(name, args, allowed))\n",
        "\n",
        "        result = agent.invoke(Command(resume={\"decisions\": decisions}), config=config)\n",
        "\n",
        "    print(\"\\n=== FINAL ASSISTANT MESSAGE ===\")\n",
        "    print(result[\"messages\"][-1].content)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--provider\", choices=[\"openai\", \"gemini\"], help=\"Model provider\")\n",
        "    parser.add_argument(\"--model\", help=\"Override model id (e.g., openai:gpt-4o-mini or google_genai:gemini-2.5-flash-lite)\")\n",
        "    parser.add_argument(\"--student\", default=\"student_001\", help=\"Student submission folder name\")\n",
        "    parser.add_argument(\"--seed\", action=\"store_true\", help=\"Create a demo submission if missing\")\n",
        "    args, unknown = parser.parse_known_args() # Modified line\n",
        "\n",
        "    provider = choose_provider(args.provider)\n",
        "    model_id = pick_model_id(provider, args.model)\n",
        "\n",
        "    # Optional demo seed\n",
        "    if args.seed:\n",
        "        sid = args.student\n",
        "        sdir = _student_dir(sid)\n",
        "        if not (sdir / \"report.md\").exists():\n",
        "            (sdir / \"report.md\").write_text(\"# Problem\\n...\\n# Method\\n...\\n# Results\\n...\\n\", encoding=\"utf-8\")\n",
        "        if not (sdir / \"solution.py\").exists():\n",
        "            (sdir / \"solution.py\").write_text(\"def solve(x):\\n    return x\\n\", encoding=\"utf-8\")\n",
        "        print(f\"Seeded demo submission in: {sdir}\")\n",
        "\n",
        "    run(args.student, provider, model_id)\n"
      ],
      "metadata": {
        "id": "BQwZdmRlFCn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepagents langchain-openai"
      ],
      "metadata": {
        "id": "MZQn5NbWE63v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uTdkb3xeFKP"
      },
      "source": [
        "# LangChain\n",
        "\n",
        "    Build simple application with LangChain\n",
        "    Trace your application with LangSmith\n",
        "    Serve your application with LangServe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN5n9sr2eSJX"
      },
      "source": [
        "The simplest and most common chain contains three things:\n",
        "\n",
        "- **Model/Chat (LLM) Wrappers**: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\n",
        "\n",
        "- **Prompt Template**: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.\n",
        "\n",
        "- **Memory**: Provides a construct for storing and retrieving messages during a conversation which can be either short term or long term.\n",
        "\n",
        "- **Indexes**: Help LLMs interact with documents by providing a way to structure them. LangChain provides Document Loaders to load documents, Text Splitters to split documents into smaller chunks, Vector Stores to store documents as embeddings, and Retrievers to fetch relevant documents.\n",
        "\n",
        "- **Chain**: Probably the most important component of LangChain is the Chain class. It's a wrapper around the LLM that allows you to create a chain of actions.\n",
        "\n",
        "- **Agents**:: Agents are the most powerful feature of LangChain. They allow you to combine LLMs with external data and tools.\n",
        "\n",
        "- **Callbacks**: Callbacks mechanism allows you to go back to different stages of your LLM application using ‘callbacks’ argument of the API. It is used for logging, monitoring, streaming etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDXn_5CQeZSU"
      },
      "source": [
        "In this guide we'll cover those three components individually, and then go over how to combine them. Understanding these concepts will set you up well for being able to use and customize LangChain applications. Most LangChain applications allow you to configure the model and/or the prompt, so knowing how to take advantage of this will be a big enabler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMQ38X_A0jbg"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Installing LangChain is easy. You can install it with pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYUgdL8xKXF6"
      },
      "outputs": [],
      "source": [
        "%time\n",
        "!pip install langchain langchain_community -qqq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyK7F2gr0m3v"
      },
      "source": [
        "Note that we're also installing a few other libraries that we'll be using in this tutorial.\n",
        "\n",
        "## Model (LLM) Wrappers\n",
        "\n",
        "Using Llama 2 is as easy as using any other HuggingFace model. We'll be using the HuggingFacePipeline wrapper (from LangChain) to make it even easier to use. To load the 13B version of the model, we'll use a GPTQ (Generative Pre-trained Transformer Quantization) version of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEC0V1DcpG1U"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "# Load the model and tokenizer from Hugging Face\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create a Hugging Face text-generation pipeline with desired parameters\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.01,        # More deterministic output\n",
        "    top_p=0.95,              # Focus on the top 95% of the probability distribution\n",
        "    do_sample=True,          # Enable sampling for randomness\n",
        "    repetition_penalty=1.15  # Discourage repetitive outputs\n",
        ")\n",
        "\n",
        "# Wrap the pipeline in LangChain's HuggingFacePipeline LLM wrapper\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPTQ has been shown to be able to quantize GPTs down to 4-bit weights with minimal loss of accuracy. This means that GPTQs can be run on much smaller and cheaper hardware, such as smartphones and laptops.\n",
        "\n",
        "GPTQ is a promising new technology that could make LLMs more accessible to a wider range of users.\n",
        "\n",
        "Here are some of the benefits of using GPTQ:\n",
        "\n",
        "> - Smaller model size: GPTQ can reduce the model size by up to 90%, without sacrificing too much accuracy. This makes it possible to deploy GPTs on smaller and cheaper hardware.\n",
        "- Faster inference: GPTQ can also speed up inference by up to 4x. This makes it possible to use GPTs in more real-time applications.\n",
        "- Lower power consumption: GPTQ can also reduce power consumption by up to 80%. This makes it possible to use GPTs on battery-powered devices."
      ],
      "metadata": {
        "id": "wX33SZLwWYQs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EdDMkKO0sD4"
      },
      "source": [
        "Good thing is that the transformers library supports loading models in GPTQ format using the AutoGPTQ library. Let's try out our LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2FPyRZsrFE9"
      },
      "outputs": [],
      "source": [
        "result = llm(\n",
        "    \"Explain the difference between ChatGPT and open source LLMs in a couple of lines.\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 1:\n",
        "\n",
        "Check the results of different settings for a prompt. You can change temperature, top_p, do_sample, and repetition_penalty in the model configuration and compare the results."
      ],
      "metadata": {
        "id": "9xYlQ4rtbQ20"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnFu5nIK3QV1"
      },
      "source": [
        "## Prompts and Prompt Templates\n",
        "\n",
        "One of the most useful features of LangChain is the ability to create prompt templates. A prompt template is a string that contains a placeholder for input variable(s). Let's see how we can use them:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define the template for generating prompts\n",
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Behave as a teacher and provide an explanation for the following query:\n",
        "<</SYS>>\n",
        "\n",
        "{text} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the PromptTemplate with the specified variables and template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],  # Specify the variables to be included in the prompt\n",
        "    template=template,  # Define the template structure\n",
        ")"
      ],
      "metadata": {
        "id": "d43v3tW6i-8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"How does attention mechanism work? Let's think step by step\""
      ],
      "metadata": {
        "id": "fb3RejM3jXdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format(text=text))"
      ],
      "metadata": {
        "id": "6s8keQGkjZML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm(prompt.format(text=text))\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ZnUHNqifjc2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Basic Prompt Formatting for Sum Calculation\n",
        "\n",
        "Define a PromptTemplate acting as a calculator that takes two input values and formats a prompt to calculate their sum."
      ],
      "metadata": {
        "id": "XOpXU964jfgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3:\n",
        "\n",
        "Modify the prompt or question to explore how we can improve the model's performance."
      ],
      "metadata": {
        "id": "R65w6GODPmSv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofLPTJDat4UM"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Act as a Machine Learning engineer who is teaching high school students.\n",
        "<</SYS>>\n",
        "\n",
        "{text} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hylEBPA23Tn6"
      },
      "source": [
        "The variable must be surrounded by {}. The input_variables argument is a list of variable names that will be used to format the template. Let's see how we can use it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8mxfp8TuDE8"
      },
      "outputs": [],
      "source": [
        "text = \"Explain what are Deep Neural Networks in 2-3 sentences\"\n",
        "print(prompt.format(text=text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EkGBnY73Wd_"
      },
      "source": [
        "You just have to use the format method of the PromptTemplate instance. The format method returns a string that can be used as input to the LLM. Let's see how we can use it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWync3332POR"
      },
      "outputs": [],
      "source": [
        "result = llm(prompt.format(text=text))\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfcew-I-uelc"
      },
      "source": [
        "## Chain\n",
        "\n",
        "Probably the most important component of LangChain is the Chain class. It's a wrapper around the LLM that allows you to create a chain of actions. Here's how you can use the simplest chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP018xXHuT92"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "result = chain.run(text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoVl1RPOuotZ"
      },
      "source": [
        "The arguments to the LLMChain class are the LLM instance and the prompt template."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Use the LLMChain for Direct Response Generation\n",
        "Task: Create a new PromptTemplate for a fitness coach explaining the benefits of regular exercise and use LLMChain to generate a response."
      ],
      "metadata": {
        "id": "5XmD8tmzRCKp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcoh94ATwaLZ"
      },
      "source": [
        "#### Chaining Chains\n",
        "\n",
        "The LLMChain is not that different from using the LLM directly. Let's see how we can chain multiple chains together. We'll create a chain that will first explain what are Deep Neural Networks and then give a few examples of practical applications. Let's start by creating the second chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ3DzjryukM5"
      },
      "outputs": [],
      "source": [
        "template = \"<s>[INST] Use the summary {summary} and give 3 examples of practical applications with 1 sentence explaining each [/INST]\"\n",
        "\n",
        "examples_prompt = PromptTemplate(\n",
        "    input_variables=[\"summary\"],\n",
        "    template=template,\n",
        ")\n",
        "examples_chain = LLMChain(llm=llm, prompt=examples_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0GVll2fwfFM"
      },
      "source": [
        "Now we can reuse our first chain along with the examples_chain and combine them into a single chain using the SimpleSequentialChain class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bxlvPFUwdLB"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "# Create an instance of 'SimpleSequentialChain'. This chain will execute two other chains\n",
        "# sequentially. The 'chains' parameter is a list of these chains - 'chain' and 'examples_chain'.\n",
        "multi_chain = SimpleSequentialChain(chains=[chain, examples_chain], verbose=True)\n",
        "\n",
        "# The 'run' method executes the chains in the order they are listed, passing the output\n",
        "# of one chain as the input to the next. The final output is then stored in the variable 'result'.\n",
        "result = multi_chain.run(text)\n",
        "\n",
        "print(result.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 5: Chaining Multiple Chains Together\n",
        "Task: Explain a scientific concept and then provide real-world applications."
      ],
      "metadata": {
        "id": "cKRDfx18SVZt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2TKHUIew3w6"
      },
      "source": [
        "## Chatbot\n",
        "\n",
        "LangChain makes it easy to create chatbots. Let's see how we can create a simple chatbot that will answer questions about Deep Neural Networks. We'll use the ChatPromptTemplate class to create a template for the chatbot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmhfr6-nwgdi"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "\n",
        "template = \"Act as an experienced high school teacher that teaches {subject}. Always give examples and analogies\"\n",
        "human_template = \"{text}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessagePromptTemplate.from_template(template),\n",
        "        HumanMessage(content=\"Hello teacher!\"),\n",
        "        AIMessage(content=\"Welcome everyone!\"),\n",
        "        HumanMessagePromptTemplate.from_template(human_template),\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_prompt.format_messages(\n",
        "    subject=\"Artificial Intelligence\", text=\"What is the most powerful AI model?\"\n",
        ")\n",
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGzTP5oBw-m4"
      },
      "source": [
        "We start by creating a system message that will be used to initialize the chatbot. Then we create a human message that will be used to start the conversation. Next, we create an AI message that will be used to respond to the human message. Finally, we create a human message that will be used to ask the question. We can use the format_messages method to format the messages.\n",
        "\n",
        "To use our LLM with the messages, we'll pass them to the predict_messages method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4dbCGSuw6oY"
      },
      "outputs": [],
      "source": [
        "result = llm.predict_messages(messages)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming necessary imports and initializations have been done...\n",
        "\n",
        "# Define the initial template for the AI acting as a high school teacher.\n",
        "teacher_template = \"Act as an experienced high school teacher specializing in {subject}. Respond to the student's questions with informative answers, examples, and analogies.\"\n",
        "\n",
        "# Set the subject that the teacher specializes in.\n",
        "subject = \"Artificial Intelligence\"\n",
        "\n",
        "# The loop for the interactive conversation.\n",
        "while True:\n",
        "    # Get user input.\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for a quit condition.\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    # Construct the complete prompt for the AI model.\n",
        "    # This includes the role description (teacher_template) and the user's question.\n",
        "    complete_prompt = teacher_template.format(subject=subject) + \"\\nStudent asks: \" + user_input + \"\\nTeacher:\"\n",
        "\n",
        "    # Use the language model to generate a response.\n",
        "    # Ensure that 'llm.predict' is the correct method for your setup.\n",
        "    # This method should take the prompt as input and return the AI's response.\n",
        "    ai_response = llm.predict(complete_prompt)\n",
        "\n",
        "    # Print the AI's response.\n",
        "    # Make sure that 'ai_response' is being correctly extracted from the model's output.\n",
        "    print(\"Teacher:\", ai_response)\n",
        "\n",
        "# End the conversation loop.\n",
        "print(\"Conversation ended.\")"
      ],
      "metadata": {
        "id": "hvDhNPSHsKug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhy1WQo6zLNz"
      },
      "source": [
        "## Agents\n",
        "\n",
        "Agents are the most powerful feature of LangChain. They allow you to combine LLMs with external data and tools. Let's see how we can create a simple agent that will use the Python REPL to calculate the square root of a number and divide it by 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdUQ4plazI5K"
      },
      "outputs": [],
      "source": [
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "\n",
        "agent = create_python_agent(llm=llm, tool=PythonREPLTool(), verbose=True)\n",
        "\n",
        "result = agent.run(\"Calculate the square root of a number and divide it by 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python REPL stands for \"Read-Eval-Print Loop.\" It's an interactive environment where you can write Python code and execute it immediately."
      ],
      "metadata": {
        "id": "kTtiWlo61oKU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XhPBoX6zSDg"
      },
      "source": [
        "Here's the final answer from our agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCtARjW3zOrV"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhN6ZKOOzWjo"
      },
      "source": [
        "Let's run the code from the agent in a Python REPL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15UIrCtUzT7s"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "\n",
        "x = 16\n",
        "y = sqrt(x)\n",
        "z = y / 2\n",
        "z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QsiYJh5znhj"
      },
      "source": [
        "So, our agent works but made a mistake in the calculations. This is important, you might hear great things about AI, but it's still not perfect. Maybe another, more powerful LLM, will get this right. Try it out and let me know.\n",
        "\n",
        "Here's the response to the same prompt but using ChatGPT:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl-T0AZyzyTB"
      },
      "source": [
        "     Enter a number: 16\n",
        "     The square root of 16.0 divided by 2 is: 2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "2AR4uYAZlSuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "\n",
        "class WikipediaAgent:\n",
        "    def search(self, query):\n",
        "        # Search Wikipedia and return the summary of the first result.\n",
        "        try:\n",
        "            # Get the page summary for the query\n",
        "            summary = wikipedia.summary(query)\n",
        "            return summary\n",
        "        except wikipedia.exceptions.DisambiguationError as e:\n",
        "            # If there's a disambiguation issue, return the options.\n",
        "            return \"Disambiguation Error: \" + '; '.join(e.options)\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            # If the page is not found, inform the user.\n",
        "            return \"Page not found for the query.\"\n",
        "\n",
        "# Create an instance of the WikipediaAgent\n",
        "wiki_agent = WikipediaAgent()\n",
        "\n",
        "# Example use of the agent to search for a term\n",
        "result = wiki_agent.search(\"Artificial Intelligence\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "KtDdQq-o5GNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ji6U-ewWlQxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}