{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamidb201214-svg/Lectures/blob/main/M3_3_NLG_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How Smart Are They? Understanding the Scale of GPT-3 and GPT-4\n"
      ],
      "metadata": {
        "id": "J3ZgnpAvZ-_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Assumption                                  | Description                                                                                       |\n",
        "|---------------------------------------------|---------------------------------------------------------------------------------------------------|\n",
        "| **Average Tokens per Book**                 | Estimated at 135,000 tokens per book, based on an average book length of 80,000 to 100,000 words.  |\n",
        "| **Average Reading Lifetime of an Individual** | Estimated at 510 books per lifetime, assuming a moderate reading habit of 5-12 books per year over 60 years. |\n",
        "| **Tokens per Word**                         | Estimated at 1.5 tokens per word, accounting for spaces and punctuation.                          |\n",
        "\n"
      ],
      "metadata": {
        "id": "xE_UeBjWZlQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Detail                             | GPT-3                                   | GPT-4                                   |\n",
        "|------------------------------------|-----------------------------------------|-----------------------------------------|\n",
        "| **Developed By**                   | OpenAI                                  | OpenAI                                  |\n",
        "| **Approximate Training Data Size** | 45 terabytes of text data               | Larger than GPT-3 (exact size unknown)  |\n",
        "| **Estimated Token Count**          | 300-400 billion tokens                  | Likely over 500 billion tokens          |\n",
        "| **Equivalent Number of Books**     | 2,222,222 - 2,962,963 books             | >3,703,704 books                        |\n",
        "| **Equivalent Knowledge of People** | 4,356 - 5,810 people                    | >7,263 people                           |\n"
      ],
      "metadata": {
        "id": "vxvz6Gw_ZjY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/transformermodel_architecture.png)"
      ],
      "metadata": {
        "id": "_Ra10qce4Nx2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j431XmcTOzCY"
      },
      "source": [
        "# Why adapt the language model?\n",
        "\n",
        "- LMs are trained in a task-agnostic way.\n",
        "- Downstream tasks can be very different from language modeling on the Pile.\n",
        "For example, consider the natural language inference (NLI) task (is the hypothesis entailed by the premise?):\n",
        "\n",
        "      Premise: I have never seen an apple that is not red.\n",
        "      Hypothesis: I have never seen an apple.\n",
        "      Correct output: Not entailment (the reverse direction would be entailment)\n",
        "\n",
        "- The format of such a task may not be very natural for the model.\n",
        "\n",
        "# Ways downstream tasks can be different\n",
        "\n",
        "- **Formatting**: for example, NLI takes in two sentences and compares them to produce a single binary output. This is different from generating the next token or filling in MASKs. Another example is the presence of MASK tokens in BERT training vs. no MASKs in downstream tasks.\n",
        "- **Topic shift**: the downstream task is focused on a new or very specific topic (e.g., medical records)\n",
        "- **Temporal shift**: the downstream task requires new knowledge that is unavailable during pre-training because 1) the knowledge is new (e.g., GPT3 was trained before Biden became President), 2) the knowledge for the downstream task is not publicly available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEGxCwH7-L2q"
      },
      "source": [
        "\n",
        "# Optimizing Large Language Models\n",
        "\n",
        "There are several options to optimize Large Language Models:\n",
        "\n",
        "    Prompt engineering by providing samples (In-Context Learning)\n",
        "    Prompt Tuning\n",
        "    Fine-Tuning\n",
        "       - Supervised fine-tuning (SFT): Classic fine-tuning by changing all weights\n",
        "       - Transfer Learning - PEFT fine-tuning by changing only a few weights\n",
        "       - Reinforcement Learning Human Feedback (RLHF)\n",
        "\n",
        "An important question is which of these options is the most effective one and which one can overwrite previous optimizations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Prompt Engineering, Prompt Tuning, and PEFT\n",
        "These techniques are essential for efficiently adapting large, pre-trained models like GPT or BERT to specialized tasks or domains, optimizing resource usage and reducing training time.\n",
        "\n",
        "\n",
        "1. **Prompt Engineering (In-Context Learning)**:\n",
        "   - **Definition**: Crafting input prompts to guide a Large Language Model (LLM) for desired outputs.\n",
        "   - **Application**: Uses natural language prompts to \"program\" the LLM, leveraging its contextual understanding.\n",
        "   - **Model Change**: No alteration to the model's parameters; relies on the model's existing knowledge and interpretive abilities.\n",
        "\n",
        "2. **Prompt Tuning**:\n",
        "   - **Difference from Prompt Engineering**: Involves appending a trainable tensor (prompt tokens) to the LLM's input embeddings.\n",
        "   - **Process**: Fine-tunes this tensor for a specific task and dataset, keeping other model parameters unchanged.\n",
        "   - **Example**: Adapting a general LLM for specific tasks like sentiment classification by adjusting prompt tokens.\n",
        "\n",
        "3. **Parameter-Efficient Fine-Tuning (PEFT)**:\n",
        "   - **Overview**: A set of techniques to enhance model performance on specific tasks or datasets by tuning a small subset of parameters.\n",
        "   - **Objective**: Targeted improvements without the need for full model retraining.\n",
        "   - **Relation to Prompt Tuning**: Prompt tuning is a subset of PEFT, focusing on fine-tuning specific parts of the model for task/domain adaptation.\n",
        "\n"
      ],
      "metadata": {
        "id": "5bJ78Ja2Urh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/PEFT_LLMs.png)"
      ],
      "metadata": {
        "id": "F2alv_qaRfHx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npsUW-pPKYp8"
      },
      "source": [
        "### Challenges\n",
        "\n",
        "Fine-tuning models can certainly help to get models to do what you want them to do. However, there are some potential issues:\n",
        "\n",
        "> - **Catastrophic forgetting**: This phenomenon describes a behavior when fine-tuning or prompts can overwrite the pre-trained model characteristics.\n",
        "> - **Overfitting**: If only a certain AI task has been fine-tuned, other tasks can suffer in terms of performance.\n",
        "\n",
        "In general, fine-tuning should be used wisely and best practices should be applied, for example, the quality of the data is more important than the quantity and multiple AI tasks should be fine-tuned at the same time vs after each other."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications\n",
        "\n",
        "There are many platforms that can be used for LLMs' applications:\n"
      ],
      "metadata": {
        "id": "FYi_A6S-NHXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Tool                                                                                                    | Category                             | Best For                                                                         | Type        |\n",
        "| :------------------------------------------------------------------------------------------------------ | :----------------------------------- | :------------------------------------------------------------------------------- | :---------- |\n",
        "| **[LangChain](https://docs.langchain.com)**                                                             | Orchestration                        | Agents, tools, RAG, observability                                                | Open-source |\n",
        "| **[Flowise](https://docs.flowiseai.com)**                                                               | App Builder / Orchestration (Visual) | Low-code drag-and-drop LLM apps (chatbots, RAG flows), rapid prototyping         | Open-source |\n",
        "| **[CrewAI](https://docs.crewai.com)**                                                                   | Agent Orchestration (Multi-agent)    | Role-based multi-agent workflows, task delegation, coordinated tool-using agents | Open-source |\n",
        "| **[Hugging Face](https://huggingface.co/docs)**                                                         | Model Hub                            | Open models, fine-tuning, hosting                                                | Platform    |\n",
        "| **[vLLM](https://docs.vllm.ai)** / **[SGLang](https://github.com/sgl-project/sglang)**                  | Serving                              | High-throughput / Structured generation                                          | Open-source |\n",
        "| **[Ollama](https://github.com/ollama/ollama)** / **[llama.cpp](https://github.com/ggml-org/llama.cpp)** | Local Run                            | Local inference & model management                                               | Open-source |\n",
        "| **[bitsandbytes](https://huggingface.co/docs/transformers/en/quantization/bitsandbytes)**               | Quantization (4/8-bit)               | Fit models into less VRAM; decent speed/quality tradeoffs                        | Open-source |\n",
        "| **[Pydantic](https://docs.pydantic.dev/)**                                                              | Validation / Schemas                 | Type-safe data validation; enforce structured outputs and tool I/O               | Open-source |\n",
        "| **[LlamaIndex](https://docs.llamaindex.ai)**                                                            | Data / RAG                           | Ingestion, indexing, retrieval                                                   | Open-source |\n",
        "| **[Haystack](https://haystack.deepset.ai)**                                                             | RAG Pipelines                        | Production pipelines, Doc QA                                                     | Open-source |\n",
        "| **[Semantic Kernel](https://github.com/microsoft/semantic-kernel)**                                     | Orchestration                        | Enterprise workflows (C#/Python)                                                 | Open-source |\n"
      ],
      "metadata": {
        "id": "lk2E5h829RJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "VaGwQIZsgI_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Delete the model and any other large tensors\n",
        "del model\n",
        "del tokenizer\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# Clear the PyTorch CUDA cache\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "1wQdXJUDh_Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
        "\n",
        "# load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# prepare the model input\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content\n",
        "try:\n",
        "    # rindex finding 151668 (</think>)\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "print(\"thinking content:\", thinking_content) # no opening <think> tag\n",
        "print(\"content:\", content)\n"
      ],
      "metadata": {
        "id": "8LmUkdoVf9ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "P1M7YDGxgr74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes>=0.46.1"
      ],
      "metadata": {
        "id": "NKaVLy9NIYQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,   # <-- match fp16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,              # <-- match fp16\n",
        "    quantization_config=bnb_config,\n",
        ")"
      ],
      "metadata": {
        "id": "tFWt5k9lITEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# prepare the model input\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content\n",
        "try:\n",
        "    # rindex finding 151668 (</think>)\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "print(\"thinking content:\", thinking_content) # no opening <think> tag\n",
        "print(\"content:\", content)\n"
      ],
      "metadata": {
        "id": "ycnfbKDAGCWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "yTuKrWMMGSuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Literal, Optional\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Pydantic models (schemas)\n",
        "# -----------------------------\n",
        "class ChatMessage(BaseModel):\n",
        "    role: Literal[\"system\", \"user\", \"assistant\"]\n",
        "    content: str = Field(min_length=1)\n",
        "\n",
        "class GenerationRequest(BaseModel):\n",
        "    prompt: str = Field(min_length=1)\n",
        "    max_new_tokens: int = Field(default=512, ge=1, le=4096)\n",
        "\n",
        "class GenerationResult(BaseModel):\n",
        "    thinking: str = \"\"\n",
        "    answer: str = Field(min_length=1)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Your code, with validation\n",
        "# -----------------------------\n",
        "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "req = GenerationRequest(prompt=\"Give me a short introduction to large language model.\", max_new_tokens=512)\n",
        "\n",
        "# validate messages\n",
        "messages: List[ChatMessage] = [ChatMessage(role=\"user\", content=req.prompt)]\n",
        "messages_dicts = [m.model_dump() for m in messages]  # convert to plain dicts for HF\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages_dicts,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=req.max_new_tokens)\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content (your logic)\n",
        "try:\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token id\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "# validate / package output\n",
        "try:\n",
        "    result = GenerationResult(thinking=thinking_content, answer=content)\n",
        "except ValidationError as e:\n",
        "    # e.g. answer empty -> you get a clean error instead of silent bad data\n",
        "    raise\n",
        "\n",
        "print(\"thinking content:\", result.thinking)\n",
        "print(\"content:\", result.answer)\n"
      ],
      "metadata": {
        "id": "NrK-z6q7QpPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain\n",
        "## Deep Agents overview\n",
        "\n"
      ],
      "metadata": {
        "id": "V_fWlscoU42h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Install dependencies"
      ],
      "metadata": {
        "id": "AzdWG_F0PULe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepagents tavily-python"
      ],
      "metadata": {
        "id": "XztJAgs7OZoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Set up your API keys"
      ],
      "metadata": {
        "id": "GQDqA7z8PXXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = getpass(\"Enter GEMINI_API_KEY: \").strip()\n",
        "os.environ[\"TAVILY_API_KEY\"] = getpass(\"Enter TAVILY_API_KEY: \").strip()\n",
        "\n",
        "print('export GEMINI_API_KEY=\"***\"')\n",
        "print('export TAVILY_API_KEY=\"***\"')"
      ],
      "metadata": {
        "id": "T9AqK5JZOvXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Create a search tool"
      ],
      "metadata": {
        "id": "7NJ5pWDBPce_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Literal\n",
        "from tavily import TavilyClient\n",
        "from deepagents import create_deep_agent\n",
        "\n",
        "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "def internet_search(\n",
        "    query: str,\n",
        "    max_results: int = 5,\n",
        "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
        "    include_raw_content: bool = False,\n",
        "):\n",
        "    \"\"\"Run a web search\"\"\"\n",
        "    return tavily_client.search(\n",
        "        query,\n",
        "        max_results=max_results,\n",
        "        include_raw_content=include_raw_content,\n",
        "        topic=topic,\n",
        "    )"
      ],
      "metadata": {
        "id": "5Ve75I95PL8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Create a deep agent"
      ],
      "metadata": {
        "id": "JSrJ3xK3PgCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# System prompt to steer the agent to be an expert researcher\n",
        "research_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n",
        "\n",
        "You have access to an internet search tool as your primary means of gathering information.\n",
        "\n",
        "## `internet_search`\n",
        "\n",
        "Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the Gemini model using the GEMINI_API_KEY set earlier\n",
        "# The model name 'gemini-1.5-flash' is a common and capable choice for general tasks.\n",
        "model = init_chat_model(model=\"google_genai:gemini-2.5-flash-lite\")\n",
        "\n",
        "agent = create_deep_agent(\n",
        "    model=model, # Explicitly pass the initialized model\n",
        "    tools=[internet_search],\n",
        "    system_prompt=research_instructions\n",
        ")"
      ],
      "metadata": {
        "id": "xj0cSriiPi9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Run the agent"
      ],
      "metadata": {
        "id": "pKqtAdNVPnvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]})\n",
        "\n",
        "# Print the agent's response\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "6AWOHMC7Po_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "BNewzQ6zUkSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip -q install -U google-genai\n",
        "\n",
        "import os\n",
        "from google import genai\n",
        "\n",
        "api_key = os.environ.get(\"GOOGLE_API_KEY\") or os.environ.get(\"GEMINI_API_KEY\")\n",
        "client = genai.Client(api_key=api_key)\n",
        "\n",
        "available = []\n",
        "for m in client.models.list():\n",
        "    # Docs example uses m.supported_actions and checks for \"generateContent\"\n",
        "    if \"generateContent\" in getattr(m, \"supported_actions\", []):\n",
        "        available.append(m.name.replace(\"models/\", \"\"))\n",
        "\n",
        "print(\"Models that support generateContent:\")\n",
        "for name in available[:50]:\n",
        "    print(\" -\", name)\n"
      ],
      "metadata": {
        "id": "l1Dm-6vsR6_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface"
      ],
      "metadata": {
        "id": "nwzPi19qtyaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install -U \"protobuf>=5.26.1,<6\" \"grpcio-status>=1.71.2,<2\" jedi\n"
      ],
      "metadata": {
        "id": "OfNvZN12aG1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f vllm || true\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "9ABkLltGaPDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langgraph deepagents \"langchain[openai]\" \"langchain[google-genai]\""
      ],
      "metadata": {
        "id": "wzb2xnZhHfuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Human-in-the-loop"
      ],
      "metadata": {
        "id": "ka5gzDAhdmSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learn how to configure human approval for sensitive tool operations\n",
        "\n",
        "Some tool operations may be sensitive and require human approval before execution. Deep agents support human-in-the-loop workflows through LangGraph’s interrupt capabilities. You can configure which tools require approval using the interrupt_on parameter."
      ],
      "metadata": {
        "id": "ygvDnWf7dpeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import uuid\n",
        "import getpass\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from langchain.tools import tool\n",
        "from langchain.chat_models import init_chat_model\n",
        "from deepagents import create_deep_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.types import Command\n",
        "\n",
        "# -----------------------------\n",
        "# 0) Provider selection + API key prompting\n",
        "# -----------------------------\n",
        "OPENAI_DEFAULT_MODEL = \"openai:gpt-4o-mini\"\n",
        "GEMINI_DEFAULT_MODEL = \"google_genai:gemini-2.5-flash-lite\"\n",
        "\n",
        "def choose_provider(cli_value: Optional[str]) -> str:\n",
        "    if cli_value in {\"openai\", \"gemini\"}:\n",
        "        return cli_value\n",
        "\n",
        "    # Interactive prompt if not provided\n",
        "    while True:\n",
        "        choice = input(\"Choose provider [openai/gemini] (default: openai): \").strip().lower()\n",
        "        if choice == \"\":\n",
        "            return \"openai\"\n",
        "        if choice in {\"openai\", \"gemini\"}:\n",
        "            return choice\n",
        "        print(\"Please type 'openai' or 'gemini'.\")\n",
        "\n",
        "def ensure_api_key(provider: str) -> None:\n",
        "    \"\"\"\n",
        "    Prompt for the provider's API key if missing, and store in env.\n",
        "    - OpenAI: OPENAI_API_KEY\n",
        "    - Gemini: GOOGLE_API_KEY (LangChain checks this first; GEMINI_API_KEY is also supported as fallback)\n",
        "    \"\"\"\n",
        "    if provider == \"openai\":\n",
        "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "            key = getpass.getpass(\"Enter OPENAI_API_KEY (input hidden): \").strip()\n",
        "            if not key:\n",
        "                raise RuntimeError(\"OPENAI_API_KEY was not provided.\")\n",
        "            os.environ[\"OPENAI_API_KEY\"] = key\n",
        "\n",
        "    elif provider == \"gemini\":\n",
        "        # Prefer GOOGLE_API_KEY because that's what LangChain docs show; GEMINI_API_KEY is also accepted.\n",
        "        if not (os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")):\n",
        "            key = getpass.getpass(\"Enter GOOGLE_API_KEY (Gemini) (input hidden): \").strip()\n",
        "            if not key:\n",
        "                raise RuntimeError(\"GOOGLE_API_KEY was not provided.\")\n",
        "            os.environ[\"GOOGLE_API_KEY\"] = key\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unknown provider. Use 'openai' or 'gemini'.\")\n",
        "\n",
        "def pick_model_id(provider: str, override: Optional[str]) -> str:\n",
        "    if override:\n",
        "        return override\n",
        "    return OPENAI_DEFAULT_MODEL if provider == \"openai\" else GEMINI_DEFAULT_MODEL\n",
        "\n",
        "# -----------------------------\n",
        "# Storage layout: ./submissions/<student_id>/*\n",
        "# -----------------------------\n",
        "ROOT = Path(\"./submissions\").resolve()\n",
        "ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "def _student_dir(student_id: str) -> Path:\n",
        "    p = (ROOT / student_id).resolve()\n",
        "    if ROOT not in p.parents:\n",
        "        raise ValueError(\"Invalid student_id (path traversal blocked).\")\n",
        "    p.mkdir(exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def _list_files(student_id: str) -> List[str]:\n",
        "    d = _student_dir(student_id)\n",
        "    return sorted([p.name for p in d.iterdir() if p.is_file()])\n",
        "\n",
        "def _read_file(student_id: str, filename: str) -> str:\n",
        "    d = _student_dir(student_id)\n",
        "    p = (d / filename).resolve()\n",
        "    if d not in p.parents:\n",
        "        raise ValueError(\"Invalid filename (path traversal blocked).\")\n",
        "    if not p.exists():\n",
        "        return \"\"\n",
        "    return p.read_text(encoding=\"utf-8\")\n",
        "\n",
        "def _append_outbox(text: str) -> None:\n",
        "    outbox = ROOT / \"OUTBOX.txt\"\n",
        "    existing = outbox.read_text(encoding=\"utf-8\") if outbox.exists() else \"\"\n",
        "    outbox.write_text(existing + text, encoding=\"utf-8\")\n",
        "\n",
        "# -----------------------------\n",
        "# Tools (LangChain)\n",
        "# -----------------------------\n",
        "@tool\n",
        "def list_submission_files(student_id: str) -> List[str]:\n",
        "    \"\"\"List files in a student's submission folder.\"\"\"\n",
        "    return _list_files(student_id)\n",
        "\n",
        "@tool\n",
        "def read_submission_file(student_id: str, filename: str) -> str:\n",
        "    \"\"\"Read a file from a student's submission folder.\"\"\"\n",
        "    text = _read_file(student_id, filename)\n",
        "    if text == \"\":\n",
        "        return f\"(empty or missing) {filename}\"\n",
        "    return text\n",
        "\n",
        "@tool\n",
        "def auto_validate(student_id: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run simple validity checks and return a report + recommended verdict.\n",
        "    Verdict: 'valid' or 'resubmit'\n",
        "    \"\"\"\n",
        "    files = _list_files(student_id)\n",
        "    required = {\"report.md\", \"solution.py\"}\n",
        "    missing_files = sorted(list(required - set(files)))\n",
        "\n",
        "    report = _read_file(student_id, \"report.md\")\n",
        "    solution = _read_file(student_id, \"solution.py\")\n",
        "\n",
        "    required_headings = [\"# Problem\", \"# Method\", \"# Results\"]\n",
        "    missing_headings = [h for h in required_headings if h not in report]\n",
        "\n",
        "    has_required_function = \"def solve(\" in solution\n",
        "\n",
        "    issues = []\n",
        "    if missing_files:\n",
        "        issues.append(f\"Missing required files: {missing_files}\")\n",
        "    if \"report.md\" in files and missing_headings:\n",
        "        issues.append(f\"Missing required headings in report.md: {missing_headings}\")\n",
        "    if \"solution.py\" in files and not has_required_function:\n",
        "        issues.append(\"solution.py missing required function signature: def solve(...)\")\n",
        "\n",
        "    recommended_verdict = \"valid\" if not issues else \"resubmit\"\n",
        "\n",
        "    recommended_message = (\n",
        "        \"✅ Your submission looks valid. Nice work!\"\n",
        "        if recommended_verdict == \"valid\"\n",
        "        else \"⚠️ Please fix the issues listed and resubmit.\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"student_id\": student_id,\n",
        "        \"files\": files,\n",
        "        \"issues\": issues,\n",
        "        \"recommended_verdict\": recommended_verdict,\n",
        "        \"recommended_message\": recommended_message,\n",
        "    }\n",
        "\n",
        "@tool\n",
        "def record_verdict(student_id: str, verdict: str, notes: str) -> str:\n",
        "    \"\"\"\n",
        "    Record the official verdict (sensitive).\n",
        "    Writes to ./submissions/verdicts.json\n",
        "    \"\"\"\n",
        "    out = ROOT / \"verdicts.json\"\n",
        "    data = json.loads(out.read_text(encoding=\"utf-8\")) if out.exists() else {}\n",
        "    data[student_id] = {\"verdict\": verdict, \"notes\": notes}\n",
        "    out.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
        "    return f\"Recorded verdict for {student_id}: {verdict}\"\n",
        "\n",
        "@tool\n",
        "def message_student(student_id: str, message: str) -> str:\n",
        "    \"\"\"\n",
        "    Mock messaging (sensitive).\n",
        "    Appends to ./submissions/OUTBOX.txt instead of actually sending email.\n",
        "    \"\"\"\n",
        "    _append_outbox(f\"\\n=== TO {student_id} ===\\n{message}\\n\")\n",
        "    return f\"Queued message to {student_id} (see submissions/OUTBOX.txt)\"\n",
        "\n",
        "# -----------------------------\n",
        "# Console HITL \"review UI\"\n",
        "# -----------------------------\n",
        "def _prompt_decision(tool_name: str, args: Dict[str, Any], allowed: List[str]) -> Dict[str, Any]:\n",
        "    print(\"\\n--- HUMAN REVIEW REQUIRED ---\")\n",
        "    print(f\"Tool: {tool_name}\")\n",
        "    print(\"Proposed args:\")\n",
        "    print(json.dumps(args, indent=2))\n",
        "    print(f\"Allowed decisions: {allowed}\")\n",
        "\n",
        "    while True:\n",
        "        choice = input(\"Type approve / reject / edit: \").strip().lower()\n",
        "        if choice == \"approve\" and \"approve\" in allowed:\n",
        "            return {\"type\": \"approve\"}\n",
        "        if choice == \"reject\" and \"reject\" in allowed:\n",
        "            return {\"type\": \"reject\"}\n",
        "        if choice == \"edit\" and \"edit\" in allowed:\n",
        "            print(\n",
        "                \"Paste edited args as JSON \"\n",
        "                \"(e.g. {\\\"student_id\\\": \\\"student_001\\\", \\\"verdict\\\": \\\"valid\\\", \\\"notes\\\": \\\"...\\\"})\"\n",
        "            )\n",
        "            edited_args = json.loads(input(\"> \").strip())\n",
        "            return {\"type\": \"edit\", \"edited_action\": {\"name\": tool_name, \"args\": edited_args}}\n",
        "        print(\"Invalid choice for this tool. Try again.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Main runner\n",
        "# -----------------------------\n",
        "def run(student_id: str, provider: str, model_id: str) -> None:\n",
        "    # Ensure correct key exists before model init\n",
        "    ensure_api_key(provider)\n",
        "\n",
        "    checkpointer = MemorySaver()\n",
        "\n",
        "    # init_chat_model accepts provider:model identifiers like openai:... and google_genai:...\n",
        "    model = init_chat_model(model_id)\n",
        "\n",
        "    agent = create_deep_agent(\n",
        "        model=model,\n",
        "        tools=[\n",
        "            list_submission_files,\n",
        "            read_submission_file,\n",
        "            auto_validate,\n",
        "            record_verdict,\n",
        "            message_student,\n",
        "        ],\n",
        "        system_prompt=(\n",
        "            \"You are a TA agent.\\n\"\n",
        "            \"Workflow:\\n\"\n",
        "            \"1) Call auto_validate(student_id).\\n\"\n",
        "            \"2) Summarize the issues (if any).\\n\"\n",
        "            \"3) Propose record_verdict(student_id, verdict, notes).\\n\"\n",
        "            \"4) If helpful, propose message_student(student_id, message).\\n\"\n",
        "            \"Keep notes short and factual.\"\n",
        "        ),\n",
        "        interrupt_on={\n",
        "            # Sensitive: human must approve/edit/reject official verdict\n",
        "            \"record_verdict\": True,  # default allows approve/edit/reject\n",
        "            # Sensitive: outbound message needs approval (no edit allowed here)\n",
        "            \"message_student\": {\"allowed_decisions\": [\"approve\", \"reject\"]},\n",
        "            # Safe: no interrupts\n",
        "            \"auto_validate\": False,\n",
        "            \"read_submission_file\": False,\n",
        "            \"list_submission_files\": False,\n",
        "        },\n",
        "        checkpointer=checkpointer,\n",
        "    )\n",
        "\n",
        "    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
        "    user_prompt = (\n",
        "        f\"Validate {student_id}. \"\n",
        "        \"Run auto checks, then record an official verdict, and message the student with next steps.\"\n",
        "    )\n",
        "\n",
        "    result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_prompt}]}, config=config)\n",
        "\n",
        "    while result.get(\"__interrupt__\"):\n",
        "        payload = result[\"__interrupt__\"][0].value\n",
        "        action_requests = payload[\"action_requests\"]\n",
        "        review_configs = {cfg[\"action_name\"]: cfg for cfg in payload[\"review_configs\"]}\n",
        "\n",
        "        decisions = []\n",
        "        for action in action_requests:\n",
        "            name = action[\"name\"]\n",
        "            args = action[\"args\"]\n",
        "            allowed = review_configs[name][\"allowed_decisions\"]\n",
        "            decisions.append(_prompt_decision(name, args, allowed))\n",
        "\n",
        "        result = agent.invoke(Command(resume={\"decisions\": decisions}), config=config)\n",
        "\n",
        "    print(\"\\n=== FINAL ASSISTANT MESSAGE ===\")\n",
        "    print(result[\"messages\"][-1].content)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--provider\", choices=[\"openai\", \"gemini\"], help=\"Model provider\")\n",
        "    parser.add_argument(\"--model\", help=\"Override model id (e.g., openai:gpt-4o-mini or google_genai:gemini-2.5-flash-lite)\")\n",
        "    parser.add_argument(\"--student\", default=\"student_001\", help=\"Student submission folder name\")\n",
        "    parser.add_argument(\"--seed\", action=\"store_true\", help=\"Create a demo submission if missing\")\n",
        "    args, unknown = parser.parse_known_args() # Modified line\n",
        "\n",
        "    provider = choose_provider(args.provider)\n",
        "    model_id = pick_model_id(provider, args.model)\n",
        "\n",
        "    # Optional demo seed\n",
        "    if args.seed:\n",
        "        sid = args.student\n",
        "        sdir = _student_dir(sid)\n",
        "        if not (sdir / \"report.md\").exists():\n",
        "            (sdir / \"report.md\").write_text(\"# Problem\\n...\\n# Method\\n...\\n# Results\\n...\\n\", encoding=\"utf-8\")\n",
        "        if not (sdir / \"solution.py\").exists():\n",
        "            (sdir / \"solution.py\").write_text(\"def solve(x):\\n    return x\\n\", encoding=\"utf-8\")\n",
        "        print(f\"Seeded demo submission in: {sdir}\")\n",
        "\n",
        "    run(args.student, provider, model_id)\n"
      ],
      "metadata": {
        "id": "BQwZdmRlFCn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ji6U-ewWlQxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}