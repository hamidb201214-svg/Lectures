{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamidb201214-svg/Lectures/blob/main/M3_3_LanceDB_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://lancedb.github.io/lancedb/assets/ecosystem-illustration.png)"
      ],
      "metadata": {
        "id": "zTzN-m4_hUo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LanceDB is an open-source database for vector-search built with persistent storage, which greatly simplifies retrieval, filtering and management of embeddings."
      ],
      "metadata": {
        "id": "zx241F9hhr5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain_huggingface"
      ],
      "metadata": {
        "id": "VxbtGFk9Zf6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGzZTq1bhSfM"
      },
      "outputs": [],
      "source": [
        "!pip install lancedb --q\n",
        "!pip install pypdf --q\n",
        "# !pip install -qqq chromadb==0.4.10 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.299 --progress-bar off\n",
        "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lancedb\n",
        "\n",
        "uri = \"/content/data/sample-lancedb\"\n",
        "db = lancedb.connect(uri)"
      ],
      "metadata": {
        "id": "3QPenRzrhX99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = db.create_table(\"my_table\",\n",
        "                        data=[{\"vector\": [3.1, 4.1], \"item\": \"foo\", \"price\": 10.0},\n",
        "                              {\"vector\": [5.9, 26.5], \"item\": \"bar\", \"price\": 20.0}])"
      ],
      "metadata": {
        "id": "QFr9jZuUhgCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lancedb\n",
        "\n",
        "# IMPORTANT: use the same URI/path you used when creating the table\n",
        "db = lancedb.connect(\"/content/data/sample-lancedb\")   # e.g. \"./lancedb\" or \"/path/to/lancedb\"\n",
        "\n",
        "table = db.open_table(\"my_table\")   # load existing table by name\n"
      ],
      "metadata": {
        "id": "213Kly8BafzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = table.search([100, 100]).limit(2).to_list()"
      ],
      "metadata": {
        "id": "MFjYLUYjhh9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "SJeCOKwdhik1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing a Vector Database for Documents"
      ],
      "metadata": {
        "id": "EwMy0VxPGog7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGbb32o1xArw"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "loader = PyPDFLoader(\"/content/attention.pdf\")\n",
        "\n",
        "docs = loader.load()\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF1OOWyZyiy4"
      },
      "source": [
        "The Markdown file we're loading is the original Attention paper: \"Attention is all you need!\". Let's see how we can use the RecursiveCharacterTextSplitter to split the document into smaller chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc_ER7Nyx7FG"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEP3dr5Jyk6K"
      },
      "source": [
        "Splitting the document into chunks is required due to the limited number of tokens a LLM can look at once (4096 for Llama 2). Next, we'll use the HuggingFaceEmbeddings class to create embeddings for the chunks:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lancedb\n",
        "from langchain.vectorstores import LanceDB\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "\n",
        "uri = \"/content/data/paper-lancedb-\"\n",
        "db = lancedb.connect(uri)"
      ],
      "metadata": {
        "id": "DFL6v_CVDFUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain_huggingface --q"
      ],
      "metadata": {
        "id": "R4Fd_gAmazO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers --q"
      ],
      "metadata": {
        "id": "msFGA8qRa9-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        ")"
      ],
      "metadata": {
        "id": "Y90jw_mLB9R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = db.create_table(\n",
        "    \"paper_table\",\n",
        "    data=[\n",
        "        {\n",
        "            \"vector\": embeddings.embed_query(\"Hello World\"),\n",
        "            \"text\": \"Hello World\",\n",
        "            \"id\": \"1\",\n",
        "            \"source\": \"\", # Add source field\n",
        "            \"page\": 0,    # Add page field\n",
        "        }\n",
        "    ],\n",
        "    mode=\"overwrite\",\n",
        ")\n",
        "\n",
        "docsearch = LanceDB.from_documents(texts[5:20], embeddings, connection=table)"
      ],
      "metadata": {
        "id": "5WDEsH5wFDAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = docsearch.as_retriever(search_kwargs={'k': 2})"
      ],
      "metadata": {
        "id": "_ydqZeToFjsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "id": "gvcMnV02EuLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0].page_content"
      ],
      "metadata": {
        "id": "A3bjGnjjFQUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = table.search(embeddings.embed_query(texts[0].page_content)).limit(2).to_list()"
      ],
      "metadata": {
        "id": "f0aEnSm1FNlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[0].keys()"
      ],
      "metadata": {
        "id": "BnRSARRhFlyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Create a LanceDB for Two Papers and Load Each into a Table"
      ],
      "metadata": {
        "id": "0j9bhHa8HUHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChromaDB"
      ],
      "metadata": {
        "id": "l8Ro7fPmHLGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://images.datacamp.com/image/upload/v1693482377/image4_7b6910cd7c.png)"
      ],
      "metadata": {
        "id": "PZo8JjuJGkkT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0BeHxeGypah"
      },
      "source": [
        "In the spirit of using free tools, we're also using free embeddings hosted by HuggingFace. We'll use Chroma database to store/cache the embeddings and make it easy to search them:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOPi0DWfzBaJ"
      },
      "source": [
        "To combine the LLM with the database, we'll use the RetrievalQA chain:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq chromadb==0.4.10 --progress-bar off"
      ],
      "metadata": {
        "id": "vGOZDRrwF5fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9Zjb7JiymgP"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")\n",
        "results = db.similarity_search(\"Transformer models\", k=2)\n",
        "print(results[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results[0]"
      ],
      "metadata": {
        "id": "F3Hzk8WlF28c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Create a ChromaDB for Two Papers and Load Each into a collection"
      ],
      "metadata": {
        "id": "y0BUI-hhKM1N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a8ZB-o3AJ64J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}