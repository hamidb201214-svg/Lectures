{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamidb201214-svg/Lectures/blob/main/M3_3_NLG_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How Smart Are They? Understanding the Scale of GPT-3 and GPT-4\n"
      ],
      "metadata": {
        "id": "J3ZgnpAvZ-_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Assumption                                  | Description                                                                                       |\n",
        "|---------------------------------------------|---------------------------------------------------------------------------------------------------|\n",
        "| **Average Tokens per Book**                 | Estimated at 135,000 tokens per book, based on an average book length of 80,000 to 100,000 words.  |\n",
        "| **Average Reading Lifetime of an Individual** | Estimated at 510 books per lifetime, assuming a moderate reading habit of 5-12 books per year over 60 years. |\n",
        "| **Tokens per Word**                         | Estimated at 1.5 tokens per word, accounting for spaces and punctuation.                          |\n",
        "\n"
      ],
      "metadata": {
        "id": "xE_UeBjWZlQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Detail                             | GPT-3                                   | GPT-4                                   |\n",
        "|------------------------------------|-----------------------------------------|-----------------------------------------|\n",
        "| **Developed By**                   | OpenAI                                  | OpenAI                                  |\n",
        "| **Approximate Training Data Size** | 45 terabytes of text data               | Larger than GPT-3 (exact size unknown)  |\n",
        "| **Estimated Token Count**          | 300-400 billion tokens                  | Likely over 500 billion tokens          |\n",
        "| **Equivalent Number of Books**     | 2,222,222 - 2,962,963 books             | >3,703,704 books                        |\n",
        "| **Equivalent Knowledge of People** | 4,356 - 5,810 people                    | >7,263 people                           |\n"
      ],
      "metadata": {
        "id": "vxvz6Gw_ZjY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/transformermodel_architecture.png)"
      ],
      "metadata": {
        "id": "_Ra10qce4Nx2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j431XmcTOzCY"
      },
      "source": [
        "# Why adapt the language model?\n",
        "\n",
        "- LMs are trained in a task-agnostic way.\n",
        "- Downstream tasks can be very different from language modeling on the Pile.\n",
        "For example, consider the natural language inference (NLI) task (is the hypothesis entailed by the premise?):\n",
        "\n",
        "      Premise: I have never seen an apple that is not red.\n",
        "      Hypothesis: I have never seen an apple.\n",
        "      Correct output: Not entailment (the reverse direction would be entailment)\n",
        "\n",
        "- The format of such a task may not be very natural for the model.\n",
        "\n",
        "# Ways downstream tasks can be different\n",
        "\n",
        "- **Formatting**: for example, NLI takes in two sentences and compares them to produce a single binary output. This is different from generating the next token or filling in MASKs. Another example is the presence of MASK tokens in BERT training vs. no MASKs in downstream tasks.\n",
        "- **Topic shift**: the downstream task is focused on a new or very specific topic (e.g., medical records)\n",
        "- **Temporal shift**: the downstream task requires new knowledge that is unavailable during pre-training because 1) the knowledge is new (e.g., GPT3 was trained before Biden became President), 2) the knowledge for the downstream task is not publicly available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEGxCwH7-L2q"
      },
      "source": [
        "\n",
        "# Optimizing Large Language Models\n",
        "\n",
        "There are several options to optimize Large Language Models:\n",
        "\n",
        "    Prompt engineering by providing samples (In-Context Learning)\n",
        "    Prompt Tuning\n",
        "    Fine-Tuning\n",
        "       - Supervised fine-tuning (SFT): Classic fine-tuning by changing all weights\n",
        "       - Transfer Learning - PEFT fine-tuning by changing only a few weights\n",
        "       - Reinforcement Learning Human Feedback (RLHF)\n",
        "\n",
        "An important question is which of these options is the most effective one and which one can overwrite previous optimizations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Prompt Engineering, Prompt Tuning, and PEFT\n",
        "These techniques are essential for efficiently adapting large, pre-trained models like GPT or BERT to specialized tasks or domains, optimizing resource usage and reducing training time.\n",
        "\n",
        "\n",
        "1. **Prompt Engineering (In-Context Learning)**:\n",
        "   - **Definition**: Crafting input prompts to guide a Large Language Model (LLM) for desired outputs.\n",
        "   - **Application**: Uses natural language prompts to \"program\" the LLM, leveraging its contextual understanding.\n",
        "   - **Model Change**: No alteration to the model's parameters; relies on the model's existing knowledge and interpretive abilities.\n",
        "\n",
        "2. **Prompt Tuning**:\n",
        "   - **Difference from Prompt Engineering**: Involves appending a trainable tensor (prompt tokens) to the LLM's input embeddings.\n",
        "   - **Process**: Fine-tunes this tensor for a specific task and dataset, keeping other model parameters unchanged.\n",
        "   - **Example**: Adapting a general LLM for specific tasks like sentiment classification by adjusting prompt tokens.\n",
        "\n",
        "3. **Parameter-Efficient Fine-Tuning (PEFT)**:\n",
        "   - **Overview**: A set of techniques to enhance model performance on specific tasks or datasets by tuning a small subset of parameters.\n",
        "   - **Objective**: Targeted improvements without the need for full model retraining.\n",
        "   - **Relation to Prompt Tuning**: Prompt tuning is a subset of PEFT, focusing on fine-tuning specific parts of the model for task/domain adaptation.\n",
        "\n"
      ],
      "metadata": {
        "id": "5bJ78Ja2Urh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/PEFT_LLMs.png)"
      ],
      "metadata": {
        "id": "F2alv_qaRfHx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npsUW-pPKYp8"
      },
      "source": [
        "### Challenges\n",
        "\n",
        "Fine-tuning models can certainly help to get models to do what you want them to do. However, there are some potential issues:\n",
        "\n",
        "> - **Catastrophic forgetting**: This phenomenon describes a behavior when fine-tuning or prompts can overwrite the pre-trained model characteristics.\n",
        "> - **Overfitting**: If only a certain AI task has been fine-tuned, other tasks can suffer in terms of performance.\n",
        "\n",
        "In general, fine-tuning should be used wisely and best practices should be applied, for example, the quality of the data is more important than the quantity and multiple AI tasks should be fine-tuned at the same time vs after each other."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications\n",
        "\n",
        "There are many platforms that can be used for LLMs' applications:\n"
      ],
      "metadata": {
        "id": "FYi_A6S-NHXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Tool                                                                                                    | Category                             | Best For                                                                         | Type        |\n",
        "| :------------------------------------------------------------------------------------------------------ | :----------------------------------- | :------------------------------------------------------------------------------- | :---------- |\n",
        "| **[LangChain](https://docs.langchain.com)**                                                             | Orchestration                        | Agents, tools, RAG, observability                                                | Open-source |\n",
        "| **[Flowise](https://docs.flowiseai.com)**                                                               | App Builder / Orchestration (Visual) | Low-code drag-and-drop LLM apps (chatbots, RAG flows), rapid prototyping         | Open-source |\n",
        "| **[CrewAI](https://docs.crewai.com)**                                                                   | Agent Orchestration (Multi-agent)    | Role-based multi-agent workflows, task delegation, coordinated tool-using agents | Open-source |\n",
        "| **[Hugging Face](https://huggingface.co/docs)**                                                         | Model Hub                            | Open models, fine-tuning, hosting                                                | Platform    |\n",
        "| **[vLLM](https://docs.vllm.ai)** / **[SGLang](https://github.com/sgl-project/sglang)**                  | Serving                              | High-throughput / Structured generation                                          | Open-source |\n",
        "| **[Ollama](https://github.com/ollama/ollama)** / **[llama.cpp](https://github.com/ggml-org/llama.cpp)** | Local Run                            | Local inference & model management                                               | Open-source |\n",
        "| **[bitsandbytes](https://huggingface.co/docs/transformers/en/quantization/bitsandbytes)**               | Quantization (4/8-bit)               | Fit models into less VRAM; decent speed/quality tradeoffs                        | Open-source |\n",
        "| **[Pydantic](https://docs.pydantic.dev/)**                                                              | Validation / Schemas                 | Type-safe data validation; enforce structured outputs and tool I/O               | Open-source |\n",
        "| **[LlamaIndex](https://docs.llamaindex.ai)**                                                            | Data / RAG                           | Ingestion, indexing, retrieval                                                   | Open-source |\n",
        "| **[Haystack](https://haystack.deepset.ai)**                                                             | RAG Pipelines                        | Production pipelines, Doc QA                                                     | Open-source |\n",
        "| **[Semantic Kernel](https://github.com/microsoft/semantic-kernel)**                                     | Orchestration                        | Enterprise workflows (C#/Python)                                                 | Open-source |\n"
      ],
      "metadata": {
        "id": "lk2E5h829RJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "VaGwQIZsgI_V",
        "outputId": "417c9da4-2513-405e-a6c2-c0da3e23fd88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Collecting transformers\n",
            "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting huggingface-hub<2.0,>=1.3.0 (from transformers)\n",
            "  Downloading huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Downloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 0.36.2\n",
            "    Uninstalling huggingface_hub-0.36.2:\n",
            "      Successfully uninstalled huggingface_hub-0.36.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.6\n",
            "    Uninstalling transformers-4.57.6:\n",
            "      Successfully uninstalled transformers-4.57.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "vllm 0.15.1 requires protobuf>=6.33.5, but you have protobuf 5.29.6 which is incompatible.\n",
            "vllm 0.15.1 requires transformers<5,>=4.56.0, but you have transformers 5.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-1.4.1 transformers-5.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "02c364ffa9f6454eaf783bef0dfc16b3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Delete the model and any other large tensors\n",
        "del model\n",
        "del tokenizer\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# Clear the PyTorch CUDA cache\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "1wQdXJUDh_Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
        "\n",
        "# load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# prepare the model input\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content\n",
        "try:\n",
        "    # rindex finding 151668 (</think>)\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "print(\"thinking content:\", thinking_content) # no opening <think> tag\n",
        "print(\"content:\", content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590,
          "referenced_widgets": [
            "fbdbbe26472a4478b888b1119e5ebb6f",
            "64ebc412643645178c8f3ae232175dd6",
            "bb6b866443364bbda24e8bb65eba9992",
            "42cc0db0a17740ada909b8c1103ae078",
            "eecdea0b6b0844d2b575c439967f73ab",
            "83e5f8303ac44711a13b7fc9410ef0a7",
            "6796784d7dda4dd4b7d2340decc06c8f",
            "b32406297dfc4e17b2726d5d88066f89",
            "1f9e07e96b5b479c84b5ff447b1306ad",
            "8b3f20bd95354b299e3f66846e7b01b6",
            "d8fa193ce7274c02b9e12a09475cb00d"
          ]
        },
        "id": "8LmUkdoVf9ZN",
        "outputId": "def514f1-b984-465a-df94-3da8068231e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbdbbe26472a4478b888b1119e5ebb6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thinking content: \n",
            "content: Okay, the user asked for a short introduction to large language models. Let me start by recalling what I know. LLMs are a big topic, so I need to keep it concise but informative.\n",
            "\n",
            "First, I should define what an LLM is. They're AI models trained on massive amounts of text data. The key points are scale—huge datasets, billions of parameters. Maybe mention that they learn patterns and language structures.\n",
            "\n",
            "Wait, the user might not know what parameters mean. Should I explain? But they want it short. Maybe just say \"billions of parameters\" without diving into technical details. Focus on the outcomes: generating human-like text, understanding context, answering questions.\n",
            "\n",
            "I should highlight common applications to make it relatable. Things like chatbots, writing, coding, translation. That shows real-world use. Also, note that they're not perfect—can hallucinate, need careful use.\n",
            "\n",
            "The user might be a beginner, so avoid jargon. Terms like \"transformer architecture\" might be too much. Skip that unless necessary. Emphasize the training process briefly: learning from vast text to predict next words.\n",
            "\n",
            "Check the length. The user said \"short,\" so aim for 3-4 sentences. Start with a simple definition, mention scale, what they do, and a note on limitations. \n",
            "\n",
            "Also, consider why they're asking. Maybe they heard the term and want a quick overview. Could be for a school project, personal interest, or work. Keeping it neutral and factual is safe.\n",
            "\n",
            "Avoid mentioning specific models like GPT or BERT unless asked. Focus on the general concept. Don't want to overload with examples. \n",
            "\n",
            "Make sure to clarify that LLMs are a type of AI, not the only type. But since the question is specific, stick to LLMs. \n",
            "\n",
            "Double-check for accuracy: LLMs do generate text, but they're also used for tasks beyond generation, like summarization. Should I include that? Maybe in the applications part. \n",
            "\n",
            "Final structure: Definition, scale, capabilities, common uses, and a brief caveat about limitations. Keep it under 5 sentences. \n",
            "\n",
            "Wait, the user said \"short,\" so maybe 3-4 sentences max. Let me draft: \"Large language models (LLMs) are AI systems trained on vast amounts of text data to understand and generate human-like language. They learn patterns from billions of words, enabling tasks like answering questions, writing stories, coding, and translating text. While powerful, they can sometimes produce\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1M7YDGxgr74",
        "outputId": "13acb813-6674-4ac8-b1d2-e5c2fe5cc4ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb  8 09:25:19 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0             27W /   70W |    8042MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes>=0.46.1"
      ],
      "metadata": {
        "id": "NKaVLy9NIYQo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,   # <-- match fp16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,              # <-- match fp16\n",
        "    quantization_config=bnb_config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "7eba8c4c3a194d0386eda55e6fadb48c",
            "b956c2c66e994d0eb6047bedb2abe0eb",
            "5614c749da944b948c4143a6aa2b4dce",
            "f145925a848342d5899d0ac95eb2cb7e",
            "0233cb6619e14cd88e4d7092ae27b2d9",
            "fb94ba4694584c6e8114ab2d2a9ec193",
            "8649cf499a07490d97b8209db9282371",
            "67675b0a90da4204b5fd362cd9bba301",
            "71c1986235d44e63b2b57ad1e46d85f6",
            "697efb4fe87640569401ba939376367a",
            "7cc6f2281cd24295bec1f84038048b1b",
            "c07072cc50304c8289c1398f950a802c",
            "4456869fbaf44fd2a00bc64271a6e4ca",
            "1c78ab1b3c474f959b3917705c440297",
            "ed072ba716ef4a5f9d28e0fa61f7b3f4",
            "7f87c3e2af3c42c08e567950c8503d39",
            "c5d401985ab047d1a8a3c44138c11de6",
            "e1566f9df53e47dc9e2b397c0474c75a",
            "436650f88ba343f2b3a8821c7470aa37",
            "91ca98dbe30c45f7af07039a438a662a",
            "cb121df5a59f4782b43690a64859a72d",
            "997b01949e5a48d9a56ae753e4aa7b1b",
            "2047ea2a8b3e49e0b45cef60577231d9",
            "608907aada4e4999816b5e9291dc79cf",
            "80eea99b3385495c8209e66012632e95",
            "2f643706e6664a37b357b3fbc419024c",
            "6aa028885a2149d38cf961f99a83497f",
            "73598dc8c64245bf8fa2088ca418b11e",
            "b0533cb323424454afdc4ef0bfc39f98",
            "45c4026ff9d9401fb81eb047b06f57a9",
            "3e023f4a137048f48052d2282238f225",
            "0fb4572528dc45b38340a3ad902fe4b7",
            "dd7ba8e929b84fcc85c294dce35ddf5f",
            "ebd00cb981654cdca7d42738bda0722c",
            "b331987fbef9465c957ea0e04cdd1a9d",
            "e3d453d316db4621afa454ccefaf4e70",
            "c3890ff973f841f58ceb7534fba36a0a",
            "7fb321cd894e4a1987923553ad8964e8",
            "5cca4ae1659b4a1b9f65ac7453706445",
            "8cbf173fd539498ba30e13d53403a791",
            "74ca5d74bc734319bd7e11baa0afda8b",
            "40709d4bdf3345a4bac49a20c80f4b13",
            "c8c7cd48cc874af0a74438178f9d9fd3",
            "64688589eb8f4f50a977edc37d710884",
            "dae08d7081884f53ba511e1836555183",
            "b24205bc0f12470fb3ffe0844eb5c5aa",
            "6227af8efa854749befddbb1fef2e0df",
            "48bd5d36f9974035b9c53566e07ed2a5",
            "66b79777b390495dacf02723c0817e30",
            "0ebbfd64290c4b0d868df673a2af0338",
            "00b87d30a13f4845bae222d88141d57a",
            "f815cb4faf934a72891e893be9077911",
            "52a450a945184072b2a230966c7e44f6",
            "fac4c93fb3184a3eaa105c0021d57fd3",
            "a691fc6ce4e34d0eb0634aef23fbc221"
          ]
        },
        "id": "tFWt5k9lITEF",
        "outputId": "0c95614d-413c-46e0-e7db-211b311bfc98"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7eba8c4c3a194d0386eda55e6fadb48c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c07072cc50304c8289c1398f950a802c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2047ea2a8b3e49e0b45cef60577231d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebd00cb981654cdca7d42738bda0722c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dae08d7081884f53ba511e1836555183"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# prepare the model input\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content\n",
        "try:\n",
        "    # rindex finding 151668 (</think>)\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "print(\"thinking content:\", thinking_content) # no opening <think> tag\n",
        "print(\"content:\", content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycnfbKDAGCWY",
        "outputId": "6e0df06f-5ad9-4393-cebe-814cc9b3821b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thinking content: Okay, the user asked for a short introduction to large language models. Hmm, they probably want something concise but informative—no jargon overload. \n",
            "\n",
            "First, I should clarify what LLMs *are* without drowning them in technicalities. The \"large\" part is key—they're not tiny models. Gotta emphasize scale (parameters) and training data (billions of words). \n",
            "\n",
            "User might be a beginner, so I'll avoid terms like \"transformers\" or \"self-attention\" unless I can explain them simply. Wait, they said \"short,\" so I'll skip deep dives. \n",
            "\n",
            "Also, should I mention applications? Yeah, briefly—like writing, coding—to show real-world relevance. User might care about *why* this matters. \n",
            "\n",
            "*Double-checking*: Must not confuse LLMs with chatbots. Important distinction! LLMs are the *underlying tech*, not the chat interfaces themselves. \n",
            "\n",
            "...Did they mean \"large\" as in size or \"language\" as in multilingual? Probably size, since \"large language model\" is a standard term. \n",
            "\n",
            "*Structure thoughts*: \n",
            "- Start with \"what it is\" (AI system) \n",
            "- Highlight scale (parameters, data) \n",
            "- Core purpose (understand/predict language) \n",
            "- Real-world use cases (writing, coding, etc.) \n",
            "- Quick note on limitations (not perfect, needs training) \n",
            "\n",
            "*Avoid*: \n",
            "- Math details \n",
            "- Specific model names (GPT, etc.) unless relevant \n",
            "- Overpromising (\"can do anything\") \n",
            "\n",
            "*User vibe check*: Feels like someone curious but not technical—maybe a student, writer, or tech enthusiast. They want clarity, not a lecture. \n",
            "\n",
            "*Final polish*: Keep it under 100 words? User said \"short.\" I'll aim for 3-4 sentences max. \n",
            "\n",
            "...Wait, should I add \"trained on vast text\" or \"learned from internet text\"? \"Vast\" sounds more natural than \"billions of words.\" \n",
            "\n",
            "*Decision*: Go with \"trained on vast amounts of text from the internet\" for clarity. \n",
            "\n",
            "*Double-check*: Yes, this covers what it is, why it's \"large,\" and why it matters—without fluff.\n",
            "</think>\n",
            "content: Here's a concise introduction to **Large Language Models (LLMs)**:\n",
            "\n",
            "> **Large Language Models (LLMs)** are advanced AI systems trained on vast amounts of text from the internet, books, and other sources. They learn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "yTuKrWMMGSuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d55230-97df-4a1f-9dac-cd5fef3f3d80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb  8 16:46:42 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0             35W /   70W |    2938MiB /  15360MiB |     39%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Tool                                                                                                                     | Category                     | Best For                                                                         | Type        |\n",
        "| :----------------------------------------------------------------------------------------------------------------------- | :--------------------------- | :------------------------------------------------------------------------------- | :---------- |\n",
        "| **[Transformers](https://huggingface.co/docs/transformers)**                                                             | Python Inference (HF-native) | `pipeline()` / `generate()`, chat templates, quick prototyping                   | Open-source |\n",
        "| **[Accelerate](https://huggingface.co/docs/accelerate)**                                                                 | Loading / Sharding           | `device_map=\"auto\"`, CPU/GPU split, offload big models that don’t fit            | Open-source |\n",
        "| **[huggingface_hub](https://huggingface.co/docs/huggingface_hub)**                                                       | Download / Caching           | `snapshot_download()` / caching / pinned revisions for reproducible loads        | Open-source |\n",
        "| **[PEFT](https://huggingface.co/docs/peft)**                                                                             | Adapters                     | Load LoRA/adapters on top of a base Mistral (cheap “fine-tune” deployments)      | Open-source |\n",
        "| **[bitsandbytes](https://huggingface.co/docs/transformers/en/quantization/bitsandbytes)**                                | Quantization (4/8-bit)       | Fit models into less VRAM; decent speed/quality tradeoffs                        | Open-source |\n",
        "| **[Transformers Quantization (AWQ/GPTQ)](https://huggingface.co/docs/transformers/en/main_classes/quantization)**        | Quantization (algos)         | Using AWQ/GPTQ paths supported by Transformers for inference workflows           | Open-source |\n",
        "| **[AutoAWQ](https://github.com/casper-hansen/AutoAWQ)**                                                                  | Quantization + Kernels       | INT4 AWQ quantization and fast inference for AWQ checkpoints                     | Open-source |\n",
        "| **[vLLM](https://docs.vllm.ai)**                                                                                         | Serving                      | High-throughput serving + OpenAI-compatible API server                           | Open-source |\n",
        "| **[SGLang](https://github.com/sgl-project/sglang)**                                                                      | Serving / Structured Gen     | Low-latency serving + structured generation runtime patterns                     | Open-source |\n",
        "| **[Text Generation Inference (TGI)](https://huggingface.co/docs/text-generation-inference/en/index)**                    | Serving                      | Classic HF inference server (see note on maintenance mode)                       | Open-source |\n",
        "| **[Ollama](https://github.com/ollama/ollama)**                                                                           | Local Run / Model Mgmt       | “Just run it locally” experience + simple local API                              | Open-source |\n",
        "| **[llama.cpp](https://github.com/ggml-org/llama.cpp)**                                                                   | Local Run (GGUF)             | CPU-friendly / wide-hardware inference via GGUF models                           | Open-source |\n",
        "| **[HF Inference Endpoints](https://huggingface.co/docs/inference-endpoints/en/index)**                                   | Managed Serving              | Deploy HF Hub models on managed infra (autoscaling, logs/metrics)                | Managed     |\n",
        "| **[HF Inference Client / Providers](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client)** | Hosted Inference API         | Call hosted endpoints/providers (and also talk to local servers) with one client | Platform    |\n"
      ],
      "metadata": {
        "id": "x1TxySqqDVR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Literal, Optional\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Pydantic models (schemas)\n",
        "# -----------------------------\n",
        "class ChatMessage(BaseModel):\n",
        "    role: Literal[\"system\", \"user\", \"assistant\"]\n",
        "    content: str = Field(min_length=1)\n",
        "\n",
        "class GenerationRequest(BaseModel):\n",
        "    prompt: str = Field(min_length=1)\n",
        "    max_new_tokens: int = Field(default=512, ge=1, le=4096)\n",
        "\n",
        "class GenerationResult(BaseModel):\n",
        "    thinking: str = \"\"\n",
        "    answer: str = Field(min_length=1)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Your code, with validation\n",
        "# -----------------------------\n",
        "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "req = GenerationRequest(prompt=\"Give me a short introduction to large language model.\", max_new_tokens=512)\n",
        "\n",
        "# validate messages\n",
        "messages: List[ChatMessage] = [ChatMessage(role=\"user\", content=req.prompt)]\n",
        "messages_dicts = [m.model_dump() for m in messages]  # convert to plain dicts for HF\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages_dicts,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=req.max_new_tokens)\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content (your logic)\n",
        "try:\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token id\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "# validate / package output\n",
        "try:\n",
        "    result = GenerationResult(thinking=thinking_content, answer=content)\n",
        "except ValidationError as e:\n",
        "    # e.g. answer empty -> you get a clean error instead of silent bad data\n",
        "    raise\n",
        "\n",
        "print(\"thinking content:\", result.thinking)\n",
        "print(\"content:\", result.answer)\n"
      ],
      "metadata": {
        "id": "NrK-z6q7QpPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain\n",
        "## Deep Agents overview\n",
        "\n"
      ],
      "metadata": {
        "id": "V_fWlscoU42h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Install dependencies"
      ],
      "metadata": {
        "id": "AzdWG_F0PULe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepagents tavily-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XztJAgs7OZoV",
        "outputId": "4ad1be09-8741-4b56-e874-72e7a01e22dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepagents in /usr/local/lib/python3.12/dist-packages (0.3.12)\n",
            "Requirement already satisfied: tavily-python in /usr/local/lib/python3.12/dist-packages (0.7.21)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.7 in /usr/local/lib/python3.12/dist-packages (from deepagents) (1.2.9)\n",
            "Requirement already satisfied: langchain<2.0.0,>=1.2.7 in /usr/local/lib/python3.12/dist-packages (from deepagents) (1.2.9)\n",
            "Requirement already satisfied: langchain-anthropic<2.0.0,>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from deepagents) (1.3.2)\n",
            "Requirement already satisfied: langchain-google-genai<5.0.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from deepagents) (4.2.0)\n",
            "Requirement already satisfied: wcmatch in /usr/local/lib/python3.12/dist-packages (from deepagents) (10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from tavily-python) (2.32.4)\n",
            "Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.12.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.28.1)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=1.2.7->deepagents) (1.0.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=1.2.7->deepagents) (2.12.3)\n",
            "Requirement already satisfied: anthropic<1.0.0,>=0.78.0 in /usr/local/lib/python3.12/dist-packages (from langchain-anthropic<2.0.0,>=1.3.1->deepagents) (0.79.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->deepagents) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->deepagents) (0.6.8)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->deepagents) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->deepagents) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->deepagents) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->deepagents) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->deepagents) (0.14.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai<5.0.0,>=4.2.0->deepagents) (1.2.0)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai<5.0.0,>=4.2.0->deepagents) (1.61.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.5.1->tavily-python) (2025.11.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (2026.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.16.0)\n",
            "Requirement already satisfied: bracex>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from wcmatch->deepagents) (2.6)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (0.17.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (1.3.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (2.47.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (15.0.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.7->deepagents) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain<2.0.0,>=1.2.7->deepagents) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain<2.0.0,>=1.2.7->deepagents) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain<2.0.0,>=1.2.7->deepagents) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain<2.0.0,>=1.2.7->deepagents) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->deepagents) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->deepagents) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->deepagents) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=1.2.7->deepagents) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=1.2.7->deepagents) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=1.2.7->deepagents) (0.4.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (4.9.1)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain<2.0.0,>=1.2.7->deepagents) (1.12.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Set up your API keys"
      ],
      "metadata": {
        "id": "GQDqA7z8PXXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = getpass(\"Enter GEMINI_API_KEY: \").strip()\n",
        "os.environ[\"TAVILY_API_KEY\"] = getpass(\"Enter TAVILY_API_KEY: \").strip()\n",
        "\n",
        "print('export GEMINI_API_KEY=\"***\"')\n",
        "print('export TAVILY_API_KEY=\"***\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9AqK5JZOvXz",
        "outputId": "3068b5b9-d524-49b1-f064-e85b97b6500b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter GEMINI_API_KEY: ··········\n",
            "Enter TAVILY_API_KEY: ··········\n",
            "export GEMINI_API_KEY=\"***\"\n",
            "export TAVILY_API_KEY=\"***\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Create a search tool"
      ],
      "metadata": {
        "id": "7NJ5pWDBPce_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Literal\n",
        "from tavily import TavilyClient\n",
        "from deepagents import create_deep_agent\n",
        "\n",
        "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "def internet_search(\n",
        "    query: str,\n",
        "    max_results: int = 5,\n",
        "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
        "    include_raw_content: bool = False,\n",
        "):\n",
        "    \"\"\"Run a web search\"\"\"\n",
        "    return tavily_client.search(\n",
        "        query,\n",
        "        max_results=max_results,\n",
        "        include_raw_content=include_raw_content,\n",
        "        topic=topic,\n",
        "    )"
      ],
      "metadata": {
        "id": "5Ve75I95PL8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Create a deep agent"
      ],
      "metadata": {
        "id": "JSrJ3xK3PgCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# System prompt to steer the agent to be an expert researcher\n",
        "research_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n",
        "\n",
        "You have access to an internet search tool as your primary means of gathering information.\n",
        "\n",
        "## `internet_search`\n",
        "\n",
        "Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the Gemini model using the GEMINI_API_KEY set earlier\n",
        "# The model name 'gemini-1.5-flash' is a common and capable choice for general tasks.\n",
        "model = init_chat_model(model=\"google_genai:gemini-2.5-flash-lite\")\n",
        "\n",
        "agent = create_deep_agent(\n",
        "    model=model, # Explicitly pass the initialized model\n",
        "    tools=[internet_search],\n",
        "    system_prompt=research_instructions\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "xj0cSriiPi9B",
        "outputId": "ae83437a-405e-43b9-c614-9b052c8defc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Qwen3ForCausalLM' object has no attribute 'profile'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3186825550.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# The model name 'gemini-1.5-flash' is a common and capable choice for general tasks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m agent = create_deep_agent(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Explicitly pass the initialized model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minternet_search\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deepagents/graph.py\u001b[0m in \u001b[0;36mcreate_deep_agent\u001b[0;34m(model, tools, system_prompt, middleware, subagents, skills, memory, response_format, context_schema, checkpointer, store, backend, interrupt_on, debug, name, cache)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;31m# Compute summarization defaults based on model profile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0msummarization_defaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compute_summarization_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mStateBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/deepagents/middleware/summarization.py\u001b[0m in \u001b[0;36m_compute_summarization_defaults\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m    104\u001b[0m     has_profile = (\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;34m\"max_input_tokens\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1965\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Qwen3ForCausalLM' object has no attribute 'profile'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Run the agent"
      ],
      "metadata": {
        "id": "pKqtAdNVPnvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]})\n",
        "\n",
        "# Print the agent's response\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AWOHMC7Po_L",
        "outputId": "24925b50-615e-4bc2-b8de-821daf330986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph is an open-source framework developed by LangChain. It is designed to simplify the creation, deployment, and management of complex AI agent workflows. LangGraph utilizes graph-based architectures to model the relationships between different components within an AI agent workflow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNewzQ6zUkSD",
        "outputId": "75e05c09-99fb-4200-d148-df3a7f751ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is langgraph?', additional_kwargs={}, response_metadata={}, id='74722ffb-c7a3-4092-acf6-af976da53124'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'internet_search', 'arguments': '{\"max_results\": 2, \"query\": \"langgraph\"}'}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c3c52-ce01-7481-8ca4-e1e6b6bc5ed2-0', tool_calls=[{'name': 'internet_search', 'args': {'max_results': 2, 'query': 'langgraph'}, 'id': '46bc2c48-a59f-472c-b002-c5bc267192e0', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 5195, 'output_tokens': 23, 'total_tokens': 5218, 'input_token_details': {'cache_read': 0}}),\n",
              "  ToolMessage(content='{\"query\": \"langgraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/\", \"title\": \"What is LangGraph?\", \"content\": \"LangGraph is an open-source framework built by LangChain that streamlines the creation and management of AI agent workflows.\", \"score\": 0.9296516, \"raw_content\": null}, {\"url\": \"https://www.ibm.com/think/topics/langgraph\", \"title\": \"What is LangGraph?\", \"content\": \"*   [Overview](https://www.ibm.com/think/topics/ai-agents#7281535). *   [What is agentic AI?](https://www.ibm.com/think/topics/agentic-ai#2095054954). *   [What is AI agent development?](https://www.ibm.com/think/topics/ai-agent-development#1083937704). *   [How to build an AI agent](https://www.ibm.com/think/topics/how-to-build-an-ai-agent#1083937707). *   [Overview](https://www.ibm.com/think/topics/ai-agent-types#72820454). *   [Overview](https://www.ibm.com/think/topics/components-of-ai-agents#498277090). *   [Communication](https://www.ibm.com/think/topics/ai-agent-communication#498277088). *   [Learning](https://www.ibm.com/think/topics/ai-agent-learning#498277087). *   [Memory](https://www.ibm.com/think/topics/ai-agent-memory#498277086). *   [Perception](https://www.ibm.com/think/topics/ai-agent-perception#498277085). *   [Planning](https://www.ibm.com/think/topics/ai-agent-planning#498277084). *   [What is agent orchestration?](https://www.ibm.com/think/topics/ai-agent-orchestration#228874317). *   [Overview](https://www.ibm.com/think/topics/ai-agent-protocols#1509394340). *   [What is agentic orchestration?](https://www.ibm.com/think/topics/ai-agent-orchestration#2142864945). *   [Tutorial: LangGraph SQL agent](https://www.ibm.com/think/tutorials/build-sql-agent-langgraph-mistral-medium-3-watsonx-ai#229412983). *   [Overview](https://www.ibm.com/think/topics/ai-agent-use-cases#257779831). *   [Finance](https://www.ibm.com/think/topics/ai-agents-in-finance#257779834). *   [Human resources](https://www.ibm.com/think/topics/ai-agents-in-human-resources#257779835). *   [Marketing](https://www.ibm.com/think/topics/ai-agents-in-marketing#257779836). *   [Procurement](https://www.ibm.com/think/topics/ai-agents-in-procurement#257779837). *   [RevOps](https://www.ibm.com/think/topics/ai-agents-revops#257779838). *   [Sales](https://www.ibm.com/think/topics/ai-agents-in-sales#257779839). LangGraph, created by [LangChain](https://www.ibm.com/think/topics/langchain), is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an [AI agent workflow](https://www.ibm.com/think/topics/ai-agents). LangGraph is also built on several key technologies, including [LangChain,](https://www.ibm.com/think/topics/langchain) a Python framework for building AI applications. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including [chatbots](https://www.ibm.com/think/topics/chatbots), state graphs and [other agent-based systems](https://www.ibm.com/think/topics/multiagent-system).\", \"score\": 0.917251, \"raw_content\": null}], \"response_time\": 0.83, \"request_id\": \"6e515523-2a03-459b-90ff-300ffe5115b6\"}', name='internet_search', id='b8d5bd15-7f59-4e80-badf-654e3a4565f0', tool_call_id='46bc2c48-a59f-472c-b002-c5bc267192e0'),\n",
              "  AIMessage(content='LangGraph is an open-source framework developed by LangChain. It is designed to simplify the creation, deployment, and management of complex AI agent workflows. LangGraph utilizes graph-based architectures to model the relationships between different components within an AI agent workflow.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c3c52-d51b-7011-babe-05fe21290a89-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 6447, 'output_tokens': 51, 'total_tokens': 6498, 'input_token_details': {'cache_read': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip -q install -U google-genai\n",
        "\n",
        "import os\n",
        "from google import genai\n",
        "\n",
        "api_key = os.environ.get(\"GOOGLE_API_KEY\") or os.environ.get(\"GEMINI_API_KEY\")\n",
        "client = genai.Client(api_key=api_key)\n",
        "\n",
        "available = []\n",
        "for m in client.models.list():\n",
        "    # Docs example uses m.supported_actions and checks for \"generateContent\"\n",
        "    if \"generateContent\" in getattr(m, \"supported_actions\", []):\n",
        "        available.append(m.name.replace(\"models/\", \"\"))\n",
        "\n",
        "print(\"Models that support generateContent:\")\n",
        "for name in available[:50]:\n",
        "    print(\" -\", name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1Dm-6vsR6_C",
        "outputId": "0b70228f-daad-4d5a-be05-055d8a2fb07f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai._api_client:Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models that support generateContent:\n",
            " - gemini-2.5-flash\n",
            " - gemini-2.5-pro\n",
            " - gemini-2.0-flash\n",
            " - gemini-2.0-flash-001\n",
            " - gemini-2.0-flash-exp-image-generation\n",
            " - gemini-2.0-flash-lite-001\n",
            " - gemini-2.0-flash-lite\n",
            " - gemini-exp-1206\n",
            " - gemini-2.5-flash-preview-tts\n",
            " - gemini-2.5-pro-preview-tts\n",
            " - gemma-3-1b-it\n",
            " - gemma-3-4b-it\n",
            " - gemma-3-12b-it\n",
            " - gemma-3-27b-it\n",
            " - gemma-3n-e4b-it\n",
            " - gemma-3n-e2b-it\n",
            " - gemini-flash-latest\n",
            " - gemini-flash-lite-latest\n",
            " - gemini-pro-latest\n",
            " - gemini-2.5-flash-lite\n",
            " - gemini-2.5-flash-image\n",
            " - gemini-2.5-flash-preview-09-2025\n",
            " - gemini-2.5-flash-lite-preview-09-2025\n",
            " - gemini-3-pro-preview\n",
            " - gemini-3-flash-preview\n",
            " - gemini-3-pro-image-preview\n",
            " - nano-banana-pro-preview\n",
            " - gemini-robotics-er-1.5-preview\n",
            " - gemini-2.5-computer-use-preview-10-2025\n",
            " - deep-research-pro-preview-12-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwzPi19qtyaF",
        "outputId": "7ab1bfa2-0ae9-45f5-fe27-70cad9922f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting huggingface-hub<1.0.0,>=0.33.4 (from langchain_huggingface)\n",
            "  Using cached huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (1.2.9)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.22.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (1.2.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.67.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.15.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.6.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (2.12.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2026.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.16.0)\n",
            "Downloading langchain_huggingface-1.2.0-py3-none-any.whl (30 kB)\n",
            "Using cached huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
            "Installing collected packages: huggingface-hub, langchain_huggingface\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 1.4.1\n",
            "    Uninstalling huggingface_hub-1.4.1:\n",
            "      Successfully uninstalled huggingface_hub-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 5.1.0 requires huggingface-hub<2.0,>=1.3.0, but you have huggingface-hub 0.36.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.36.2 langchain_huggingface-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install -U \"protobuf>=5.26.1,<6\" \"grpcio-status>=1.71.2,<2\" jedi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfNvZN12aG1Y",
        "outputId": "b86ff346-9240-4325-c45c-3201a8f7d291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/320.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.5/320.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "vllm 0.15.1 requires protobuf>=6.33.5, but you have protobuf 5.29.6 which is incompatible.\n",
            "grpcio-reflection 1.78.0 requires protobuf<7.0.0,>=6.31.1, but you have protobuf 5.29.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f vllm || true\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ABkLltGaPDw",
        "outputId": "97093a7d-1081-49ff-9652-b1727b30f605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "Sun Feb  8 08:40:37 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langgraph deepagents \"langchain[openai]\" \"langchain[google-genai]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wzb2xnZhHfuX",
        "outputId": "e1272f24-53cf-4e3d-9044-b42a3b550e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.7)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-1.0.8-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: deepagents in /usr/local/lib/python3.12/dist-packages (0.3.12)\n",
            "Requirement already satisfied: langchain[openai] in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Collecting langchain[openai]\n",
            "  Downloading langchain-1.2.9-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.2.9)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.3)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.12.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: langchain-anthropic<2.0.0,>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from deepagents) (1.3.2)\n",
            "Requirement already satisfied: langchain-google-genai<5.0.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from deepagents) (4.2.0)\n",
            "Requirement already satisfied: wcmatch in /usr/local/lib/python3.12/dist-packages (from deepagents) (10.1)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (from langchain[openai]) (1.1.7)\n",
            "Requirement already satisfied: anthropic<1.0.0,>=0.78.0 in /usr/local/lib/python3.12/dist-packages (from langchain-anthropic<2.0.0,>=1.3.1->deepagents) (0.79.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.6.8)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.14.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai<5.0.0,>=4.2.0->deepagents) (1.2.0)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai<5.0.0,>=4.2.0->deepagents) (1.61.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai->langchain[openai]) (2.16.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai->langchain[openai]) (0.12.0)\n",
            "Requirement already satisfied: bracex>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from wcmatch->deepagents) (2.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (0.17.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic<2.0.0,>=1.3.1->deepagents) (1.3.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (2.47.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (2.32.4)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (15.0.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai->langchain[openai]) (4.67.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai->langchain[openai]) (2025.11.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (4.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai<5.0.0,>=4.2.0->deepagents) (0.6.2)\n",
            "Downloading langgraph-1.0.8-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-1.2.9-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.2/111.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langgraph, langchain\n",
            "  Attempting uninstall: langgraph\n",
            "    Found existing installation: langgraph 1.0.7\n",
            "    Uninstalling langgraph-1.0.7:\n",
            "      Successfully uninstalled langgraph-1.0.7\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 1.2.8\n",
            "    Uninstalling langchain-1.2.8:\n",
            "      Successfully uninstalled langchain-1.2.8\n",
            "Successfully installed langchain-1.2.9 langgraph-1.0.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langchain",
                  "langgraph"
                ]
              },
              "id": "eabe1b7572334e15827c5fbfc0bbf9a1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Human-in-the-loop"
      ],
      "metadata": {
        "id": "ka5gzDAhdmSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learn how to configure human approval for sensitive tool operations\n",
        "\n",
        "Some tool operations may be sensitive and require human approval before execution. Deep agents support human-in-the-loop workflows through LangGraph’s interrupt capabilities. You can configure which tools require approval using the interrupt_on parameter."
      ],
      "metadata": {
        "id": "ygvDnWf7dpeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import uuid\n",
        "import getpass\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from langchain.tools import tool\n",
        "from langchain.chat_models import init_chat_model\n",
        "from deepagents import create_deep_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.types import Command\n",
        "\n",
        "# -----------------------------\n",
        "# 0) Provider selection + API key prompting\n",
        "# -----------------------------\n",
        "OPENAI_DEFAULT_MODEL = \"openai:gpt-4o-mini\"\n",
        "GEMINI_DEFAULT_MODEL = \"google_genai:gemini-2.5-flash-lite\"\n",
        "\n",
        "def choose_provider(cli_value: Optional[str]) -> str:\n",
        "    if cli_value in {\"openai\", \"gemini\"}:\n",
        "        return cli_value\n",
        "\n",
        "    # Interactive prompt if not provided\n",
        "    while True:\n",
        "        choice = input(\"Choose provider [openai/gemini] (default: openai): \").strip().lower()\n",
        "        if choice == \"\":\n",
        "            return \"openai\"\n",
        "        if choice in {\"openai\", \"gemini\"}:\n",
        "            return choice\n",
        "        print(\"Please type 'openai' or 'gemini'.\")\n",
        "\n",
        "def ensure_api_key(provider: str) -> None:\n",
        "    \"\"\"\n",
        "    Prompt for the provider's API key if missing, and store in env.\n",
        "    - OpenAI: OPENAI_API_KEY\n",
        "    - Gemini: GOOGLE_API_KEY (LangChain checks this first; GEMINI_API_KEY is also supported as fallback)\n",
        "    \"\"\"\n",
        "    if provider == \"openai\":\n",
        "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "            key = getpass.getpass(\"Enter OPENAI_API_KEY (input hidden): \").strip()\n",
        "            if not key:\n",
        "                raise RuntimeError(\"OPENAI_API_KEY was not provided.\")\n",
        "            os.environ[\"OPENAI_API_KEY\"] = key\n",
        "\n",
        "    elif provider == \"gemini\":\n",
        "        # Prefer GOOGLE_API_KEY because that's what LangChain docs show; GEMINI_API_KEY is also accepted.\n",
        "        if not (os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")):\n",
        "            key = getpass.getpass(\"Enter GOOGLE_API_KEY (Gemini) (input hidden): \").strip()\n",
        "            if not key:\n",
        "                raise RuntimeError(\"GOOGLE_API_KEY was not provided.\")\n",
        "            os.environ[\"GOOGLE_API_KEY\"] = key\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unknown provider. Use 'openai' or 'gemini'.\")\n",
        "\n",
        "def pick_model_id(provider: str, override: Optional[str]) -> str:\n",
        "    if override:\n",
        "        return override\n",
        "    return OPENAI_DEFAULT_MODEL if provider == \"openai\" else GEMINI_DEFAULT_MODEL\n",
        "\n",
        "# -----------------------------\n",
        "# Storage layout: ./submissions/<student_id>/*\n",
        "# -----------------------------\n",
        "ROOT = Path(\"./submissions\").resolve()\n",
        "ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "def _student_dir(student_id: str) -> Path:\n",
        "    p = (ROOT / student_id).resolve()\n",
        "    if ROOT not in p.parents:\n",
        "        raise ValueError(\"Invalid student_id (path traversal blocked).\")\n",
        "    p.mkdir(exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def _list_files(student_id: str) -> List[str]:\n",
        "    d = _student_dir(student_id)\n",
        "    return sorted([p.name for p in d.iterdir() if p.is_file()])\n",
        "\n",
        "def _read_file(student_id: str, filename: str) -> str:\n",
        "    d = _student_dir(student_id)\n",
        "    p = (d / filename).resolve()\n",
        "    if d not in p.parents:\n",
        "        raise ValueError(\"Invalid filename (path traversal blocked).\")\n",
        "    if not p.exists():\n",
        "        return \"\"\n",
        "    return p.read_text(encoding=\"utf-8\")\n",
        "\n",
        "def _append_outbox(text: str) -> None:\n",
        "    outbox = ROOT / \"OUTBOX.txt\"\n",
        "    existing = outbox.read_text(encoding=\"utf-8\") if outbox.exists() else \"\"\n",
        "    outbox.write_text(existing + text, encoding=\"utf-8\")\n",
        "\n",
        "# -----------------------------\n",
        "# Tools (LangChain)\n",
        "# -----------------------------\n",
        "@tool\n",
        "def list_submission_files(student_id: str) -> List[str]:\n",
        "    \"\"\"List files in a student's submission folder.\"\"\"\n",
        "    return _list_files(student_id)\n",
        "\n",
        "@tool\n",
        "def read_submission_file(student_id: str, filename: str) -> str:\n",
        "    \"\"\"Read a file from a student's submission folder.\"\"\"\n",
        "    text = _read_file(student_id, filename)\n",
        "    if text == \"\":\n",
        "        return f\"(empty or missing) {filename}\"\n",
        "    return text\n",
        "\n",
        "@tool\n",
        "def auto_validate(student_id: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run simple validity checks and return a report + recommended verdict.\n",
        "    Verdict: 'valid' or 'resubmit'\n",
        "    \"\"\"\n",
        "    files = _list_files(student_id)\n",
        "    required = {\"report.md\", \"solution.py\"}\n",
        "    missing_files = sorted(list(required - set(files)))\n",
        "\n",
        "    report = _read_file(student_id, \"report.md\")\n",
        "    solution = _read_file(student_id, \"solution.py\")\n",
        "\n",
        "    required_headings = [\"# Problem\", \"# Method\", \"# Results\"]\n",
        "    missing_headings = [h for h in required_headings if h not in report]\n",
        "\n",
        "    has_required_function = \"def solve(\" in solution\n",
        "\n",
        "    issues = []\n",
        "    if missing_files:\n",
        "        issues.append(f\"Missing required files: {missing_files}\")\n",
        "    if \"report.md\" in files and missing_headings:\n",
        "        issues.append(f\"Missing required headings in report.md: {missing_headings}\")\n",
        "    if \"solution.py\" in files and not has_required_function:\n",
        "        issues.append(\"solution.py missing required function signature: def solve(...)\")\n",
        "\n",
        "    recommended_verdict = \"valid\" if not issues else \"resubmit\"\n",
        "\n",
        "    recommended_message = (\n",
        "        \"✅ Your submission looks valid. Nice work!\"\n",
        "        if recommended_verdict == \"valid\"\n",
        "        else \"⚠️ Please fix the issues listed and resubmit.\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"student_id\": student_id,\n",
        "        \"files\": files,\n",
        "        \"issues\": issues,\n",
        "        \"recommended_verdict\": recommended_verdict,\n",
        "        \"recommended_message\": recommended_message,\n",
        "    }\n",
        "\n",
        "@tool\n",
        "def record_verdict(student_id: str, verdict: str, notes: str) -> str:\n",
        "    \"\"\"\n",
        "    Record the official verdict (sensitive).\n",
        "    Writes to ./submissions/verdicts.json\n",
        "    \"\"\"\n",
        "    out = ROOT / \"verdicts.json\"\n",
        "    data = json.loads(out.read_text(encoding=\"utf-8\")) if out.exists() else {}\n",
        "    data[student_id] = {\"verdict\": verdict, \"notes\": notes}\n",
        "    out.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
        "    return f\"Recorded verdict for {student_id}: {verdict}\"\n",
        "\n",
        "@tool\n",
        "def message_student(student_id: str, message: str) -> str:\n",
        "    \"\"\"\n",
        "    Mock messaging (sensitive).\n",
        "    Appends to ./submissions/OUTBOX.txt instead of actually sending email.\n",
        "    \"\"\"\n",
        "    _append_outbox(f\"\\n=== TO {student_id} ===\\n{message}\\n\")\n",
        "    return f\"Queued message to {student_id} (see submissions/OUTBOX.txt)\"\n",
        "\n",
        "# -----------------------------\n",
        "# Console HITL \"review UI\"\n",
        "# -----------------------------\n",
        "def _prompt_decision(tool_name: str, args: Dict[str, Any], allowed: List[str]) -> Dict[str, Any]:\n",
        "    print(\"\\n--- HUMAN REVIEW REQUIRED ---\")\n",
        "    print(f\"Tool: {tool_name}\")\n",
        "    print(\"Proposed args:\")\n",
        "    print(json.dumps(args, indent=2))\n",
        "    print(f\"Allowed decisions: {allowed}\")\n",
        "\n",
        "    while True:\n",
        "        choice = input(\"Type approve / reject / edit: \").strip().lower()\n",
        "        if choice == \"approve\" and \"approve\" in allowed:\n",
        "            return {\"type\": \"approve\"}\n",
        "        if choice == \"reject\" and \"reject\" in allowed:\n",
        "            return {\"type\": \"reject\"}\n",
        "        if choice == \"edit\" and \"edit\" in allowed:\n",
        "            print(\n",
        "                \"Paste edited args as JSON \"\n",
        "                \"(e.g. {\\\"student_id\\\": \\\"student_001\\\", \\\"verdict\\\": \\\"valid\\\", \\\"notes\\\": \\\"...\\\"})\"\n",
        "            )\n",
        "            edited_args = json.loads(input(\"> \").strip())\n",
        "            return {\"type\": \"edit\", \"edited_action\": {\"name\": tool_name, \"args\": edited_args}}\n",
        "        print(\"Invalid choice for this tool. Try again.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Main runner\n",
        "# -----------------------------\n",
        "def run(student_id: str, provider: str, model_id: str) -> None:\n",
        "    # Ensure correct key exists before model init\n",
        "    ensure_api_key(provider)\n",
        "\n",
        "    checkpointer = MemorySaver()\n",
        "\n",
        "    # init_chat_model accepts provider:model identifiers like openai:... and google_genai:...\n",
        "    model = init_chat_model(model_id)\n",
        "\n",
        "    agent = create_deep_agent(\n",
        "        model=model,\n",
        "        tools=[\n",
        "            list_submission_files,\n",
        "            read_submission_file,\n",
        "            auto_validate,\n",
        "            record_verdict,\n",
        "            message_student,\n",
        "        ],\n",
        "        system_prompt=(\n",
        "            \"You are a TA agent.\\n\"\n",
        "            \"Workflow:\\n\"\n",
        "            \"1) Call auto_validate(student_id).\\n\"\n",
        "            \"2) Summarize the issues (if any).\\n\"\n",
        "            \"3) Propose record_verdict(student_id, verdict, notes).\\n\"\n",
        "            \"4) If helpful, propose message_student(student_id, message).\\n\"\n",
        "            \"Keep notes short and factual.\"\n",
        "        ),\n",
        "        interrupt_on={\n",
        "            # Sensitive: human must approve/edit/reject official verdict\n",
        "            \"record_verdict\": True,  # default allows approve/edit/reject\n",
        "            # Sensitive: outbound message needs approval (no edit allowed here)\n",
        "            \"message_student\": {\"allowed_decisions\": [\"approve\", \"reject\"]},\n",
        "            # Safe: no interrupts\n",
        "            \"auto_validate\": False,\n",
        "            \"read_submission_file\": False,\n",
        "            \"list_submission_files\": False,\n",
        "        },\n",
        "        checkpointer=checkpointer,\n",
        "    )\n",
        "\n",
        "    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
        "    user_prompt = (\n",
        "        f\"Validate {student_id}. \"\n",
        "        \"Run auto checks, then record an official verdict, and message the student with next steps.\"\n",
        "    )\n",
        "\n",
        "    result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_prompt}]}, config=config)\n",
        "\n",
        "    while result.get(\"__interrupt__\"):\n",
        "        payload = result[\"__interrupt__\"][0].value\n",
        "        action_requests = payload[\"action_requests\"]\n",
        "        review_configs = {cfg[\"action_name\"]: cfg for cfg in payload[\"review_configs\"]}\n",
        "\n",
        "        decisions = []\n",
        "        for action in action_requests:\n",
        "            name = action[\"name\"]\n",
        "            args = action[\"args\"]\n",
        "            allowed = review_configs[name][\"allowed_decisions\"]\n",
        "            decisions.append(_prompt_decision(name, args, allowed))\n",
        "\n",
        "        result = agent.invoke(Command(resume={\"decisions\": decisions}), config=config)\n",
        "\n",
        "    print(\"\\n=== FINAL ASSISTANT MESSAGE ===\")\n",
        "    print(result[\"messages\"][-1].content)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--provider\", choices=[\"openai\", \"gemini\"], help=\"Model provider\")\n",
        "    parser.add_argument(\"--model\", help=\"Override model id (e.g., openai:gpt-4o-mini or google_genai:gemini-2.5-flash-lite)\")\n",
        "    parser.add_argument(\"--student\", default=\"student_001\", help=\"Student submission folder name\")\n",
        "    parser.add_argument(\"--seed\", action=\"store_true\", help=\"Create a demo submission if missing\")\n",
        "    args, unknown = parser.parse_known_args() # Modified line\n",
        "\n",
        "    provider = choose_provider(args.provider)\n",
        "    model_id = pick_model_id(provider, args.model)\n",
        "\n",
        "    # Optional demo seed\n",
        "    if args.seed:\n",
        "        sid = args.student\n",
        "        sdir = _student_dir(sid)\n",
        "        if not (sdir / \"report.md\").exists():\n",
        "            (sdir / \"report.md\").write_text(\"# Problem\\n...\\n# Method\\n...\\n# Results\\n...\\n\", encoding=\"utf-8\")\n",
        "        if not (sdir / \"solution.py\").exists():\n",
        "            (sdir / \"solution.py\").write_text(\"def solve(x):\\n    return x\\n\", encoding=\"utf-8\")\n",
        "        print(f\"Seeded demo submission in: {sdir}\")\n",
        "\n",
        "    run(args.student, provider, model_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQwZdmRlFCn1",
        "outputId": "169ca81b-5971-4872-f0c9-8f11532841fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose provider [openai/gemini] (default: openai): gemini\n",
            "Enter GOOGLE_API_KEY (Gemini) (input hidden): ··········\n",
            "\n",
            "--- HUMAN REVIEW REQUIRED ---\n",
            "Tool: record_verdict\n",
            "Proposed args:\n",
            "{\n",
            "  \"verdict\": \"valid\",\n",
            "  \"student_id\": \"student_001\",\n",
            "  \"notes\": \"Auto validation passed with no issues.\"\n",
            "}\n",
            "Allowed decisions: ['approve', 'edit', 'reject']\n",
            "Type approve / reject / edit: a\n",
            "Invalid choice for this tool. Try again.\n",
            "Type approve / reject / edit: approve\n",
            "\n",
            "--- HUMAN REVIEW REQUIRED ---\n",
            "Tool: message_student\n",
            "Proposed args:\n",
            "{\n",
            "  \"student_id\": \"student_001\",\n",
            "  \"message\": \"\\u2705 Your submission looks valid. Nice work!\"\n",
            "}\n",
            "Allowed decisions: ['approve', 'reject']\n",
            "Type approve / reject / edit: approve\n",
            "\n",
            "=== FINAL ASSISTANT MESSAGE ===\n",
            "The student's submission has been validated and recorded as 'valid'. They have also been messaged with the next steps.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepagents langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZQn5NbWE63v",
        "outputId": "bce2ba3a-4a2b-41b8-e1f0-0abe0eaeca9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.6 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.2.9)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.16.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.6.8)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (26.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n",
            "Downloading langchain_openai-1.1.7-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-1.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uTdkb3xeFKP"
      },
      "source": [
        "# LangChain\n",
        "\n",
        "    Build simple application with LangChain\n",
        "    Trace your application with LangSmith\n",
        "    Serve your application with LangServe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN5n9sr2eSJX"
      },
      "source": [
        "The simplest and most common chain contains three things:\n",
        "\n",
        "- **Model/Chat (LLM) Wrappers**: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\n",
        "\n",
        "- **Prompt Template**: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.\n",
        "\n",
        "- **Memory**: Provides a construct for storing and retrieving messages during a conversation which can be either short term or long term.\n",
        "\n",
        "- **Indexes**: Help LLMs interact with documents by providing a way to structure them. LangChain provides Document Loaders to load documents, Text Splitters to split documents into smaller chunks, Vector Stores to store documents as embeddings, and Retrievers to fetch relevant documents.\n",
        "\n",
        "- **Chain**: Probably the most important component of LangChain is the Chain class. It's a wrapper around the LLM that allows you to create a chain of actions.\n",
        "\n",
        "- **Agents**:: Agents are the most powerful feature of LangChain. They allow you to combine LLMs with external data and tools.\n",
        "\n",
        "- **Callbacks**: Callbacks mechanism allows you to go back to different stages of your LLM application using ‘callbacks’ argument of the API. It is used for logging, monitoring, streaming etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDXn_5CQeZSU"
      },
      "source": [
        "In this guide we'll cover those three components individually, and then go over how to combine them. Understanding these concepts will set you up well for being able to use and customize LangChain applications. Most LangChain applications allow you to configure the model and/or the prompt, so knowing how to take advantage of this will be a big enabler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMQ38X_A0jbg"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Installing LangChain is easy. You can install it with pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYUgdL8xKXF6",
        "outputId": "9c0b053a-9bb3-4bea-e3f2-2434181e1048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 7.39 µs\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%time\n",
        "!pip install langchain langchain_community -qqq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyK7F2gr0m3v"
      },
      "source": [
        "Note that we're also installing a few other libraries that we'll be using in this tutorial.\n",
        "\n",
        "## Model (LLM) Wrappers\n",
        "\n",
        "Using Llama 2 is as easy as using any other HuggingFace model. We'll be using the HuggingFacePipeline wrapper (from LangChain) to make it even easier to use. To load the 13B version of the model, we'll use a GPTQ (Generative Pre-trained Transformer Quantization) version of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEC0V1DcpG1U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417,
          "referenced_widgets": [
            "e6a9c672a25b4be58bedabee6393d0da",
            "87ba9f08619d403d9bf284339f7f3e40",
            "0253c0b71deb47c48cfccc3395bd0a86",
            "16653e78085e45a0865c71f9f3f42e0f",
            "195e610854a64b259726d085639ddded",
            "b8e8725fc4434c7b918b46f5ffda3ec6",
            "fe9d68179ac94966bba092dbc8b9b8a7",
            "80957d1a00484e3aa1eb65512ba756d7",
            "82eaafdbc4d94103a3b5e28f0268d053",
            "221abe9c34db48b68db9b38bb8af6682",
            "9201e90b153b4c6c8190cce5cf861870",
            "e8be8689614c45f183ba7907a9ba57a7",
            "ac7ed5ff09d84ff1ade838e0c90008fb",
            "18a3bca5e88c48eead88255a7bfc1369",
            "85806d33159c42a89f23e44cbfd0b753",
            "1eb46ea232994ee5bc9325bc04f10a3d",
            "cc9f8aec86f04e579d1dd08847ae1c9b",
            "4cece3809b2d4a2cbcdd6afafbd421fd",
            "807ce9db05f148e08abfde9c089a5832",
            "47166082cda0435a932cad37382940d0",
            "ff3fb38695d6453f90843bbd374bb536",
            "c62695f4b9874f6393e852121883e795",
            "ac80170b45164d0f99a4d5879387408c",
            "35c2c90383114ac091e2b4f144ffce3d",
            "4f7138f9c210431baa03b8021af832ce",
            "14b2c9a1e70049a088f0934a076bc060",
            "f05b51d71ec34e639959480a1c041f70",
            "071942fd9b9c44be9171d97c9feb453f",
            "ccb2dce0e5654676b874cd37eced7b1d",
            "727175d5704641b1b21cd6305c6bad9e",
            "be38050f0a514fd9b5e127a4baa1cf52",
            "99176259b134401b81912ff9be362932",
            "418a37e18c944a2b97d44f00618b0881",
            "e18385883d42446baf4b83a74d5ad52e",
            "17962cd91177427da7fabda73bfeac9b",
            "fa33c2d126c34b95bf46be468e57dd3a",
            "20fa8e9660e6427b8898fccca82e21fd",
            "409e8c8c76264e0fa729a87760260213",
            "bdf82d1bedb94091b11c727ced7f8920",
            "0e4f82d64680484eb1fe2f87160039f1",
            "13bb32bec3274d35bcfe466ccb002801",
            "55e7887328884c12a9df3220f5644c0f",
            "74dffbc94d0b42c8a6a2a334562e8f46",
            "083af5bb2206485b9dde342802b9ed62",
            "6f26d087c8da480fbe7d8e2cfdd5ea87",
            "d998eae6b6ec4263b6d978dc64e776bc",
            "1f9381517ab940c7a16a4b7a104c8b03",
            "11730561850541cba697bc3afed7baff",
            "3595a4b5885e40eba72254c590deba55",
            "d896e50beba94344b2321c4fdac2a4bb",
            "c4bc56d8ad8f4706a58f0879167980b8",
            "6b1081ffd11f4daa9f6b4e1e2adbc988",
            "4dc43a3873154fa2b49c9bd0d87f55a1",
            "3846bbef82a74b00b99b3fb49e1b9d1b",
            "e93b5d44fda24996a9889b29357a4d43",
            "841f9e5683824a3a8200754fb3aa83b4",
            "b65ad0faf6a04e49bbe1a9c00421eb7c",
            "d84997b7658449cb850d5708d7f6aae8",
            "31cf726d606a4708ada81e2d028061b6",
            "702d7e5e362148709809c04d10301a05",
            "08d09d7236164e41ba3d06f8cb487cd2",
            "ff3aeab38e91462da137930ddcdc8bd8",
            "551a1a7fad2e49f19d23c91a920ebbb2",
            "93f05e525b7f4bdcad7e7b2224c4af33",
            "2d66b46d98514a3183117deea0dcbc4c",
            "a53de60338a24ed3abe5369b3064872f",
            "c73052e08fc14732b065a6a3f0f92fa1",
            "1b69450f87e944608b625e8223d8512f",
            "21a028881aa44dad9f86853c432fe0d4",
            "f67dbd7180cb498eb127d37a7e8e0859",
            "d390028fd15e4ac59950ceb579b2e283",
            "6586fa7990e146fb9b752cdc79b1ed7f",
            "cca49a29536d4e66a57c58892628347e",
            "3feab53c537c484ca1082561fcb51cd8",
            "7dff0ab5fbb94ebcb4ac8b6c4195397f",
            "c756fd0143ae4e429ecacc1f6f510a53",
            "f96df489063146f8954befcbaaec06ea"
          ]
        },
        "outputId": "e3fa02cf-4ba3-41e2-e872-244f88deb04c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6a9c672a25b4be58bedabee6393d0da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8be8689614c45f183ba7907a9ba57a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac80170b45164d0f99a4d5879387408c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e18385883d42446baf4b83a74d5ad52e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f26d087c8da480fbe7d8e2cfdd5ea87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "841f9e5683824a3a8200754fb3aa83b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c73052e08fc14732b065a6a3f0f92fa1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "<ipython-input-4-3ab2c72baca3>:29: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe)\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "# Load the model and tokenizer from Hugging Face\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create a Hugging Face text-generation pipeline with desired parameters\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.01,        # More deterministic output\n",
        "    top_p=0.95,              # Focus on the top 95% of the probability distribution\n",
        "    do_sample=True,          # Enable sampling for randomness\n",
        "    repetition_penalty=1.15  # Discourage repetitive outputs\n",
        ")\n",
        "\n",
        "# Wrap the pipeline in LangChain's HuggingFacePipeline LLM wrapper\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPTQ has been shown to be able to quantize GPTs down to 4-bit weights with minimal loss of accuracy. This means that GPTQs can be run on much smaller and cheaper hardware, such as smartphones and laptops.\n",
        "\n",
        "GPTQ is a promising new technology that could make LLMs more accessible to a wider range of users.\n",
        "\n",
        "Here are some of the benefits of using GPTQ:\n",
        "\n",
        "> - Smaller model size: GPTQ can reduce the model size by up to 90%, without sacrificing too much accuracy. This makes it possible to deploy GPTs on smaller and cheaper hardware.\n",
        "- Faster inference: GPTQ can also speed up inference by up to 4x. This makes it possible to use GPTs in more real-time applications.\n",
        "- Lower power consumption: GPTQ can also reduce power consumption by up to 80%. This makes it possible to use GPTs on battery-powered devices."
      ],
      "metadata": {
        "id": "wX33SZLwWYQs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EdDMkKO0sD4"
      },
      "source": [
        "Good thing is that the transformers library supports loading models in GPTQ format using the AutoGPTQ library. Let's try out our LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2FPyRZsrFE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1aa4087-be58-4433-896d-e6380ab73417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-b93a0caf1cda>:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = llm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain the difference between ChatGPT and open source LLMs in a couple of lines. \n",
            "ChatGPT is an AI language model created by Anthropic, while open-source LLMs are software libraries that allow developers to create their own large-scale language models without having to build them from scratch.\n",
            "Open-source LLMs can be used for various purposes such as text generation, translation, summarization, etc., whereas ChatGPT has specific applications like customer service chatbots, virtual assistants, etc. Open-source LLMs require more technical expertise and resources compared to ChatGPT which provides pre-trained models with high accuracy and efficiency. Additionally, ChatGPT updates its models regularly based on user feedback, making it more accurate over time than older versions of open-source LLMs. In summary, ChatGPT offers faster deployment and easier integration into existing systems, while open-source LLMs provide greater flexibility and customization options but may take longer to train or update.\n"
          ]
        }
      ],
      "source": [
        "result = llm(\n",
        "    \"Explain the difference between ChatGPT and open source LLMs in a couple of lines.\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 1:\n",
        "\n",
        "Check the results of different settings for a prompt. You can change temperature, top_p, do_sample, and repetition_penalty in the model configuration and compare the results."
      ],
      "metadata": {
        "id": "9xYlQ4rtbQ20"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnFu5nIK3QV1"
      },
      "source": [
        "## Prompts and Prompt Templates\n",
        "\n",
        "One of the most useful features of LangChain is the ability to create prompt templates. A prompt template is a string that contains a placeholder for input variable(s). Let's see how we can use them:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define the template for generating prompts\n",
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Behave as a teacher and provide an explanation for the following query:\n",
        "<</SYS>>\n",
        "\n",
        "{text} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the PromptTemplate with the specified variables and template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],  # Specify the variables to be included in the prompt\n",
        "    template=template,  # Define the template structure\n",
        ")"
      ],
      "metadata": {
        "id": "d43v3tW6i-8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"How does attention mechanism work? Let's think step by step\""
      ],
      "metadata": {
        "id": "fb3RejM3jXdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format(text=text))"
      ],
      "metadata": {
        "id": "6s8keQGkjZML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7c7c69-0926-4874-ecb7-8cbc9d932225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<s>[INST] <<SYS>>\n",
            "Behave as a teacher and provide an explanation for the following query:\n",
            "<</SYS>>\n",
            "\n",
            "How does attention mechanism work? Let's think step by step [/INST]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm(prompt.format(text=text))\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ZnUHNqifjc2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97dae624-7073-44c0-db4f-ca173749f543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<s>[INST] <<SYS>>\n",
            "Behave as a teacher and provide an explanation for the following query:\n",
            "<</SYS>>\n",
            "\n",
            "How does attention mechanism work? Let's think step by step [/INST]\n",
            "Attention mechanisms are used in machine learning models to focus on specific parts of input data during processing. They help improve model performance, especially when dealing with large amounts of unstructured or semi-structured data.\n",
            "\n",
            "### Step 1: Understanding Attention Mechanisms\n",
            "\n",
            "**Definition:** An attention mechanism is a technique that allows neural networks to selectively attend to different parts of their inputs while they're being processed. This helps the network learn more relevant features from its input without having to explicitly define which features it should pay attention to at each time step.\n",
            "\n",
            "### Step 2: Types of Attention Mechanisms\n",
            "\n",
            "There are several types of attention mechanisms:\n",
            "\n",
            "#### Self-Attention (also known as Intra-Sequence Attention)\n",
            "- **Usage:** Used within the same sequence.\n",
            "- **Process:** Each token in the sequence attends to every other token in the sequence based on similarity scores calculated using dot products between vectors representing tokens.\n",
            "\n",
            "#### Global Attention\n",
            "- **Usage:** Applied across multiple sequences.\n",
            "- **Process:** Tokens in one sequence can \"attend\" to all tokens in another sequence simultaneously, allowing them to capture global dependencies among sequences.\n",
            "\n",
            "### Step 3: How It Works\n",
            "\n",
            "The core idea behind attention mechanisms involves calculating weights for each element in the input tensor. These weights represent how much importance each part of the input has towards generating the output. The process typically includes these steps:\n",
            "\n",
            "1. **Vector Representation**: Convert raw text into numerical representations (e.g., word embeddings).\n",
            "   \n",
            "2. **Dot Product Calculation**: Compute the dot product between the vector representation of the current position and the entire context vector. This gives a measure of relevance.\n",
            "\n",
            "3. **Normalization**: Normalize the resulting values so that they sum up to 1, making sure no single feature dominates over others.\n",
            "\n",
            "4. **Weighted Summation**: Multiply each weight value by the corresponding context vector component and then add them together to get a weighted average.\n",
            "\n",
            "5. **Output Generation**: Use this weighted average as the final output for the given position.\n",
            "\n",
            "### Step 4: Benefits\n",
            "\n",
            "Using attention mechanisms improves efficiency because they allow the model to ignore irrelevant information quickly. Additionally, they enable better handling of long-range dependencies in sequential data, leading to improved accuracy and generalization capabilities.\n",
            "\n",
            "By understanding how attention works, you gain insight into why certain architectures perform well in various tasks like language modeling, natural language processing, computer vision, etc. Remember, mastering complex concepts often requires practice through hands-on coding exercises! [/INST]\n",
            "\n",
            "If you have any further questions about attention mechanisms or need additional explanations, feel free to ask!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Basic Prompt Formatting for Sum Calculation\n",
        "\n",
        "Define a PromptTemplate acting as a calculator that takes two input values and formats a prompt to calculate their sum."
      ],
      "metadata": {
        "id": "XOpXU964jfgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3:\n",
        "\n",
        "Modify the prompt or question to explore how we can improve the model's performance."
      ],
      "metadata": {
        "id": "R65w6GODPmSv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofLPTJDat4UM"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Act as a Machine Learning engineer who is teaching high school students.\n",
        "<</SYS>>\n",
        "\n",
        "{text} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hylEBPA23Tn6"
      },
      "source": [
        "The variable must be surrounded by {}. The input_variables argument is a list of variable names that will be used to format the template. Let's see how we can use it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8mxfp8TuDE8"
      },
      "outputs": [],
      "source": [
        "text = \"Explain what are Deep Neural Networks in 2-3 sentences\"\n",
        "print(prompt.format(text=text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EkGBnY73Wd_"
      },
      "source": [
        "You just have to use the format method of the PromptTemplate instance. The format method returns a string that can be used as input to the LLM. Let's see how we can use it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWync3332POR"
      },
      "outputs": [],
      "source": [
        "result = llm(prompt.format(text=text))\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfcew-I-uelc"
      },
      "source": [
        "## Chain\n",
        "\n",
        "Probably the most important component of LangChain is the Chain class. It's a wrapper around the LLM that allows you to create a chain of actions. Here's how you can use the simplest chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP018xXHuT92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee08847-ee8a-40ea-8c54-eeba698b8a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-79b7c0bb5f25>:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt)\n",
            "<ipython-input-10-79b7c0bb5f25>:4: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = chain.run(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<s>[INST] <<SYS>>\n",
            "Behave as a teacher and provide an explanation for the following query:\n",
            "<</SYS>>\n",
            "\n",
            "How does attention mechanism work? Let's think step by step [/INST]\n",
            "Attention mechanisms are used in machine learning models, particularly those that process sequential data like natural language or time series. They allow these systems to focus on different parts of their input during processing.\n",
            "\n",
            "### Step 1: Understanding Attention Mechanisms\n",
            "\n",
            "**Definition:** An attention mechanism is a technique where a model can selectively attend to specific features within its inputs while it processes them. This allows the system to weigh certain elements more heavily than others based on relevance.\n",
            "\n",
            "### Step 2: Components of Attention\n",
            "\n",
            "- **Query (Q):** Represents what the model wants to extract from the input.\n",
            "- **Key (K):** Provides information about how each element in the input relates to the query.\n",
            "- **Value (V):** Contains the actual content being processed.\n",
            "\n",
            "### Step 3: Calculating Attention Scores\n",
            "\n",
            "The first step involves calculating scores between the Query and Key vectors using dot products:\n",
            "\n",
            "\\[ \\text{Score} = Q^T K \\]\n",
            "\n",
            "These scores indicate how well each key matches the query.\n",
            "\n",
            "### Step 4: Normalizing Scores\n",
            "\n",
            "To ensure all keys receive equal weight when combined with values, we normalize the scores:\n",
            "\n",
            "\\[ \\text{Softmax}(S) = \\frac{\\exp(S)}{\\sum_{i=1}^{n}\\exp(S_i)} \\]\n",
            "\n",
            "Where \\( S \\) represents the normalized score matrix after applying softmax function.\n",
            "\n",
            "### Step 5: Weighting Values Based on Attention Scores\n",
            "\n",
            "Next, multiply the Value vector by the Softmaxed Score Matrix:\n",
            "\n",
            "\\[ V' = V \\times \\text{Softmax}(S) \\]\n",
            "\n",
            "This results in weighted sums across the entire sequence, giving higher weights to relevant segments.\n",
            "\n",
            "### Step 6: Selective Processing\n",
            "\n",
            "Finally, sum up the weighted values over the dimension corresponding to the original queries:\n",
            "\n",
            "\\[ Output = \\sum_j V'_j \\]\n",
            "\n",
            "This output effectively captures only the most important portions of the input according to the initial query.\n",
            "\n",
            "### Conclusion\n",
            "\n",
            "In summary, attention mechanisms enable deep neural networks to dynamically adjust which aspects of their input they pay close attention to at any given moment, making them highly effective for tasks requiring context-aware reasoning such as translation, summarization, and understanding complex sequences. By focusing on critical areas through selective weighting, these techniques enhance both efficiency and accuracy in various applications involving sequential data analysis. [/INST]\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "result = chain.run(text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoVl1RPOuotZ"
      },
      "source": [
        "The arguments to the LLMChain class are the LLM instance and the prompt template."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Use the LLMChain for Direct Response Generation\n",
        "Task: Create a new PromptTemplate for a fitness coach explaining the benefits of regular exercise and use LLMChain to generate a response."
      ],
      "metadata": {
        "id": "5XmD8tmzRCKp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcoh94ATwaLZ"
      },
      "source": [
        "#### Chaining Chains\n",
        "\n",
        "The LLMChain is not that different from using the LLM directly. Let's see how we can chain multiple chains together. We'll create a chain that will first explain what are Deep Neural Networks and then give a few examples of practical applications. Let's start by creating the second chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ3DzjryukM5"
      },
      "outputs": [],
      "source": [
        "template = \"<s>[INST] Use the summary {summary} and give 3 examples of practical applications with 1 sentence explaining each [/INST]\"\n",
        "\n",
        "examples_prompt = PromptTemplate(\n",
        "    input_variables=[\"summary\"],\n",
        "    template=template,\n",
        ")\n",
        "examples_chain = LLMChain(llm=llm, prompt=examples_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0GVll2fwfFM"
      },
      "source": [
        "Now we can reuse our first chain along with the examples_chain and combine them into a single chain using the SimpleSequentialChain class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bxlvPFUwdLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b3247e-21c6-4760-f5bb-a18c697cbc9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "<s>[INST] <<SYS>>\n",
            "Behave as a teacher and provide an explanation for the following query:\n",
            "<</SYS>>\n",
            "\n",
            "How does attention mechanism work? Let's think step by step [/INST]\n",
            "Attention mechanisms are used in machine learning models to focus on specific parts of input data during processing. They help improve model performance, especially when dealing with large amounts of unstructured or semi-structured data.\n",
            "\n",
            "### Step 1: Understanding Attention Mechanisms\n",
            "\n",
            "**Definition:** An attention mechanism is a technique that allows neural networks to selectively attend to different parts of their inputs while they're being processed. This helps the network learn more relevant features from its input without having to explicitly define which features it should pay attention to at each time step.\n",
            "\n",
            "### Step 2: Types of Attention Mechanisms\n",
            "\n",
            "There are several types of attention mechanisms:\n",
            "\n",
            "#### Self-Attention (also known as Intra-Sequence Attention)\n",
            "- **Usage:** Used within the same sequence.\n",
            "- **Process:** Each token in the sequence attends to every other token in the sequence based on similarity scores calculated using dot products between vectors representing tokens.\n",
            "\n",
            "#### Global Attention\n",
            "- **Usage:** Applied across multiple sequences.\n",
            "- **Process:** Tokens in one sequence can \"attend\" to all tokens in another sequence simultaneously, allowing them to capture global dependencies among sequences.\n",
            "\n",
            "### Step 3: How It Works\n",
            "\n",
            "The core idea behind attention mechanisms involves calculating weights for each element in the input tensor. These weights represent how much importance each part of the input has towards generating the output. The process typically includes two main steps:\n",
            "\n",
            "1. **Weight Calculation**: For each position \\(i\\) in the input vector \\(\\mathbf{X}\\), calculate a weight \\(w_i\\) such that \\(0 < w_i \\leq 1\\). This weight reflects how important the corresponding element in \\(\\mathbf{X}\\) is for predicting the next word/token.\n",
            "\n",
            "   Mathematically, this could be represented as:\n",
            "   \n",
            "   \\[\n",
            "   w_i = f_{\\text{att}}(x_i; h_1,..., h_n)\n",
            "   \\]\n",
            "\n",
            "   where \\(f_{\\text{att}}\\) represents some function computing these weights, and \\(h_1,..., h_n\\) are hidden states computed over previous layers.\n",
            "\n",
            "2. **Output Computation**: Use the weighted sum of the input elements to compute the final prediction. Typically, this results in concatenating the weighted sums along dimension zero before passing through a linear layer followed by a softmax activation function.\n",
            "\n",
            "   Mathematically, if we have a batch size \\(B\\) and feature dimensions \\(D\\):\n",
            "\n",
            "   \\[\n",
            "   y_t = \\sigma(W_y [W_x^T x + b_x]^{\\top} +\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m<s>[INST] Use the summary \n",
            "<s>[INST] <<SYS>>\n",
            "Behave as a teacher and provide an explanation for the following query:\n",
            "<</SYS>>\n",
            "\n",
            "How does attention mechanism work? Let's think step by step [/INST]\n",
            "Attention mechanisms are used in machine learning models to focus on specific parts of input data during processing. They help improve model performance, especially when dealing with large amounts of unstructured or semi-structured data.\n",
            "\n",
            "### Step 1: Understanding Attention Mechanisms\n",
            "\n",
            "**Definition:** An attention mechanism is a technique that allows neural networks to selectively attend to different parts of their inputs while they're being processed. This helps the network learn more relevant features from its input without having to explicitly define which features it should pay attention to at each time step.\n",
            "\n",
            "### Step 2: Types of Attention Mechanisms\n",
            "\n",
            "There are several types of attention mechanisms:\n",
            "\n",
            "#### Self-Attention (also known as Intra-Sequence Attention)\n",
            "- **Usage:** Used within the same sequence.\n",
            "- **Process:** Each token in the sequence attends to every other token in the sequence based on similarity scores calculated using dot products between vectors representing tokens.\n",
            "\n",
            "#### Global Attention\n",
            "- **Usage:** Applied across multiple sequences.\n",
            "- **Process:** Tokens in one sequence can \"attend\" to all tokens in another sequence simultaneously, allowing them to capture global dependencies among sequences.\n",
            "\n",
            "### Step 3: How It Works\n",
            "\n",
            "The core idea behind attention mechanisms involves calculating weights for each element in the input tensor. These weights represent how much importance each part of the input has towards generating the output. The process typically includes two main steps:\n",
            "\n",
            "1. **Weight Calculation**: For each position \\(i\\) in the input vector \\(\\mathbf{X}\\), calculate a weight \\(w_i\\) such that \\(0 < w_i \\leq 1\\). This weight reflects how important the corresponding element in \\(\\mathbf{X}\\) is for predicting the next word/token.\n",
            "\n",
            "   Mathematically, this could be represented as:\n",
            "   \n",
            "   \\[\n",
            "   w_i = f_{\\text{att}}(x_i; h_1,..., h_n)\n",
            "   \\]\n",
            "\n",
            "   where \\(f_{\\text{att}}\\) represents some function computing these weights, and \\(h_1,..., h_n\\) are hidden states computed over previous layers.\n",
            "\n",
            "2. **Output Computation**: Use the weighted sum of the input elements to compute the final prediction. Typically, this results in concatenating the weighted sums along dimension zero before passing through a linear layer followed by a softmax activation function.\n",
            "\n",
            "   Mathematically, if we have a batch size \\(B\\) and feature dimensions \\(D\\):\n",
            "\n",
            "   \\[\n",
            "   y_t = \\sigma(W_y [W_x^T x + b_x]^{\\top} + and give 3 examples of practical applications with 1 sentence explaining each [/INST] \n",
            "\n",
            "  ### Practical Applications of Attention Mechanism\n",
            "\n",
            "1. **Machine Translation**\n",
            "   - **Example:** Google Translate uses self-attention to align words from source language sentences into target language translations efficiently. By focusing on key phrases like nouns, verbs, adjectives, etc., translation becomes faster and more accurate compared to traditional methods.\n",
            "\n",
            "2. **Natural Language Processing (NLP) Tasks**\n",
            "   - **Example:** Sentiment Analysis – When analyzing text sentiment, NLP systems use attention mechanisms to identify positive/negative keywords rather than treating entire sentences equally. This improves accuracy since sentiments expressed differently may not always correlate directly.\n",
            "\n",
            "3. **Image Captioning**\n",
            "   - **Example:** Models trained for image caption generation often employ multi-head attention to understand complex visual scenes better. Different heads might focus on various aspects of images—like objects, people, places—and combine information effectively to generate coherent descriptions.\n",
            "\n",
            "These applications showcase how attention mechanisms enhance computational efficiency and effectiveness in diverse AI tasks, making them indispensable tools in modern ML research and development.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "<s>[INST] Use the summary \n",
            "<s>[INST] <<SYS>>\n",
            "Behave as a teacher and provide an explanation for the following query:\n",
            "<</SYS>>\n",
            "\n",
            "How does attention mechanism work? Let's think step by step [/INST]\n",
            "Attention mechanisms are used in machine learning models to focus on specific parts of input data during processing. They help improve model performance, especially when dealing with large amounts of unstructured or semi-structured data.\n",
            "\n",
            "### Step 1: Understanding Attention Mechanisms\n",
            "\n",
            "**Definition:** An attention mechanism is a technique that allows neural networks to selectively attend to different parts of their inputs while they're being processed. This helps the network learn more relevant features from its input without having to explicitly define which features it should pay attention to at each time step.\n",
            "\n",
            "### Step 2: Types of Attention Mechanisms\n",
            "\n",
            "There are several types of attention mechanisms:\n",
            "\n",
            "#### Self-Attention (also known as Intra-Sequence Attention)\n",
            "- **Usage:** Used within the same sequence.\n",
            "- **Process:** Each token in the sequence attends to every other token in the sequence based on similarity scores calculated using dot products between vectors representing tokens.\n",
            "\n",
            "#### Global Attention\n",
            "- **Usage:** Applied across multiple sequences.\n",
            "- **Process:** Tokens in one sequence can \"attend\" to all tokens in another sequence simultaneously, allowing them to capture global dependencies among sequences.\n",
            "\n",
            "### Step 3: How It Works\n",
            "\n",
            "The core idea behind attention mechanisms involves calculating weights for each element in the input tensor. These weights represent how much importance each part of the input has towards generating the output. The process typically includes two main steps:\n",
            "\n",
            "1. **Weight Calculation**: For each position \\(i\\) in the input vector \\(\\mathbf{X}\\), calculate a weight \\(w_i\\) such that \\(0 < w_i \\leq 1\\). This weight reflects how important the corresponding element in \\(\\mathbf{X}\\) is for predicting the next word/token.\n",
            "\n",
            "   Mathematically, this could be represented as:\n",
            "   \n",
            "   \\[\n",
            "   w_i = f_{\\text{att}}(x_i; h_1,..., h_n)\n",
            "   \\]\n",
            "\n",
            "   where \\(f_{\\text{att}}\\) represents some function computing these weights, and \\(h_1,..., h_n\\) are hidden states computed over previous layers.\n",
            "\n",
            "2. **Output Computation**: Use the weighted sum of the input elements to compute the final prediction. Typically, this results in concatenating the weighted sums along dimension zero before passing through a linear layer followed by a softmax activation function.\n",
            "\n",
            "   Mathematically, if we have a batch size \\(B\\) and feature dimensions \\(D\\):\n",
            "\n",
            "   \\[\n",
            "   y_t = \\sigma(W_y [W_x^T x + b_x]^{\\top} + and give 3 examples of practical applications with 1 sentence explaining each [/INST] \n",
            "\n",
            "  ### Practical Applications of Attention Mechanism\n",
            "\n",
            "1. **Machine Translation**\n",
            "   - **Example:** Google Translate uses self-attention to align words from source language sentences into target language translations efficiently. By focusing on key phrases like nouns, verbs, adjectives, etc., translation becomes faster and more accurate compared to traditional methods.\n",
            "\n",
            "2. **Natural Language Processing (NLP) Tasks**\n",
            "   - **Example:** Sentiment Analysis – When analyzing text sentiment, NLP systems use attention mechanisms to identify positive/negative keywords rather than treating entire sentences equally. This improves accuracy since sentiments expressed differently may not always correlate directly.\n",
            "\n",
            "3. **Image Captioning**\n",
            "   - **Example:** Models trained for image caption generation often employ multi-head attention to understand complex visual scenes better. Different heads might focus on various aspects of images—like objects, people, places—and combine information effectively to generate coherent descriptions.\n",
            "\n",
            "These applications showcase how attention mechanisms enhance computational efficiency and effectiveness in diverse AI tasks, making them indispensable tools in modern ML research and development.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "# Create an instance of 'SimpleSequentialChain'. This chain will execute two other chains\n",
        "# sequentially. The 'chains' parameter is a list of these chains - 'chain' and 'examples_chain'.\n",
        "multi_chain = SimpleSequentialChain(chains=[chain, examples_chain], verbose=True)\n",
        "\n",
        "# The 'run' method executes the chains in the order they are listed, passing the output\n",
        "# of one chain as the input to the next. The final output is then stored in the variable 'result'.\n",
        "result = multi_chain.run(text)\n",
        "\n",
        "print(result.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 5: Chaining Multiple Chains Together\n",
        "Task: Explain a scientific concept and then provide real-world applications."
      ],
      "metadata": {
        "id": "cKRDfx18SVZt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2TKHUIew3w6"
      },
      "source": [
        "## Chatbot\n",
        "\n",
        "LangChain makes it easy to create chatbots. Let's see how we can create a simple chatbot that will answer questions about Deep Neural Networks. We'll use the ChatPromptTemplate class to create a template for the chatbot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmhfr6-nwgdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01f429e6-7420-453a-8fab-32fd11c46651"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Act as an experienced high school teacher that teaches Artificial Intelligence. Always give examples and analogies', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Hello teacher!', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Welcome everyone!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What is the most powerful AI model?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "\n",
        "template = \"Act as an experienced high school teacher that teaches {subject}. Always give examples and analogies\"\n",
        "human_template = \"{text}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessagePromptTemplate.from_template(template),\n",
        "        HumanMessage(content=\"Hello teacher!\"),\n",
        "        AIMessage(content=\"Welcome everyone!\"),\n",
        "        HumanMessagePromptTemplate.from_template(human_template),\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_prompt.format_messages(\n",
        "    subject=\"Artificial Intelligence\", text=\"What is the most powerful AI model?\"\n",
        ")\n",
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGzTP5oBw-m4"
      },
      "source": [
        "We start by creating a system message that will be used to initialize the chatbot. Then we create a human message that will be used to start the conversation. Next, we create an AI message that will be used to respond to the human message. Finally, we create a human message that will be used to ask the question. We can use the format_messages method to format the messages.\n",
        "\n",
        "To use our LLM with the messages, we'll pass them to the predict_messages method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4dbCGSuw6oY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d1ab82b-5ff7-4cdc-a7af-259b02c0236a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-dfe86ef01e1b>:1: LangChainDeprecationWarning: The method `BaseLLM.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = llm.predict_messages(messages)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Act as an experienced high school teacher that teaches Artificial Intelligence. Always give examples and analogies\n",
            "Human: Hello teacher!\n",
            "AI: Welcome everyone!\n",
            "Human: What is the most powerful AI model? \n",
            "AI: That's a great question! The most powerful AI models are those used in areas like image recognition, natural language processing (NLP), and speech synthesis.\n",
            "For example, Google’s DeepMind AlphaGo Zero has been incredibly impressive at playing Go, which involves complex strategic thinking. It learned to play by itself through reinforcement learning without any human input or training data.\n",
            "\n",
            "Another notable example is IBM Watson, particularly its ability to understand and respond to questions across various domains such as healthcare, finance, and entertainment. This demonstrates how advanced NLP can be applied effectively for real-world applications.\n",
            "\n",
            "Speech synthesis technology also shows immense potential with systems like Amazon Alexa and Apple Siri becoming more sophisticated over time. They use deep neural networks to generate voices that sound almost indistinguishable from humans.\n",
            "\n",
            "In terms of powerhouses specifically designed for machine translation tasks, Google Translate stands out due to its accuracy and versatility across multiple languages.\n",
            "\n",
            "These models showcase not just their computational prowess but also their adaptability and capability to learn new things autonomously, making them truly remarkable advancements in artificial intelligence. \n",
            "\n",
            "Remember though, while these models demonstrate incredible capabilities today, they still fall short when it comes to understanding contextually nuanced conversations or emotions – there's always room for improvement!\n",
            "\n",
            "What do you think about this overview?\n",
            "Human: Thank you so much for explaining all these amazing AI models. I'm really interested in knowing more about how we can apply AI in our daily lives beyond what you've mentioned here.\n",
            "AI: Absolutely! There are countless ways AI impacts everyday life:\n",
            "\n",
            "1. **Healthcare**: AI helps doctors diagnose diseases faster and more accurately using tools like medical imaging analysis and predictive analytics. For instance, algorithms can detect early signs of cancer based on X-rays or MRIs.\n",
            "\n",
            "2. **Finance**: In banking and financial services, AI automates risk assessment, fraud detection, and investment strategies. Chatbots powered by AI help customers manage accounts and provide personalized advice.\n",
            "\n",
            "3. **Transportation**: Self-driving cars leverage AI to navigate roads safely and efficiently. Ride-sharing apps use AI to optimize routes and predict demand patterns.\n",
            "\n",
            "4. **Education**: Adaptive learning platforms tailor educational content to individual student needs, enhancing engagement and comprehension. AI-driven chatbots assist students with homework and study tips.\n",
            "\n",
            "5. **Retail**: Retailers use AI to personalize shopping experiences, recommending products based on browsing history and purchase behavior. Inventory management uses AI to forecast stock levels and reduce waste.\n",
            "\n",
            "6. **Entertainment**: Streaming services employ AI to curate playlists, suggest movies and TV shows, and even create\n"
          ]
        }
      ],
      "source": [
        "result = llm.predict_messages(messages)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming necessary imports and initializations have been done...\n",
        "\n",
        "# Define the initial template for the AI acting as a high school teacher.\n",
        "teacher_template = \"Act as an experienced high school teacher specializing in {subject}. Respond to the student's questions with informative answers, examples, and analogies.\"\n",
        "\n",
        "# Set the subject that the teacher specializes in.\n",
        "subject = \"Artificial Intelligence\"\n",
        "\n",
        "# The loop for the interactive conversation.\n",
        "while True:\n",
        "    # Get user input.\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for a quit condition.\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    # Construct the complete prompt for the AI model.\n",
        "    # This includes the role description (teacher_template) and the user's question.\n",
        "    complete_prompt = teacher_template.format(subject=subject) + \"\\nStudent asks: \" + user_input + \"\\nTeacher:\"\n",
        "\n",
        "    # Use the language model to generate a response.\n",
        "    # Ensure that 'llm.predict' is the correct method for your setup.\n",
        "    # This method should take the prompt as input and return the AI's response.\n",
        "    ai_response = llm.predict(complete_prompt)\n",
        "\n",
        "    # Print the AI's response.\n",
        "    # Make sure that 'ai_response' is being correctly extracted from the model's output.\n",
        "    print(\"Teacher:\", ai_response)\n",
        "\n",
        "# End the conversation loop.\n",
        "print(\"Conversation ended.\")"
      ],
      "metadata": {
        "id": "hvDhNPSHsKug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb62ed58-7aed-4970-85ba-4cf2d6b00725"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: What is AI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-f2b0707dcb86>:25: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  ai_response = llm.predict(complete_prompt)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher: Act as an experienced high school teacher specializing in Artificial Intelligence. Respond to the student's questions with informative answers, examples, and analogies.\n",
            "Student asks: What is AI\n",
            "Teacher: Hello! Welcome back to our class on artificial intelligence (AI). Let me start by explaining what we mean when we talk about AI.\n",
            "\n",
            "Imagine you have a toy robot that can move around your room without any instructions from you. It follows simple rules programmed into its circuits. This kind of behavior is similar to how some early forms of AI worked - they were designed based on specific tasks or inputs rather than general understanding.\n",
            "\n",
            "Now imagine if this same robot could learn new things over time through experience just like humans do. That’s where machine learning comes in. Machine learning algorithms are like teachers for computers; they help them understand patterns and make decisions independently after being trained on data sets.\n",
            "\n",
            "So, AI isn't just one thing but many different types:\n",
            "\n",
            "1. **Rule-based systems**: These follow pre-programmed steps exactly like your toy robot.\n",
            "2. **Expert Systems**: They mimic human knowledge and decision-making processes using databases filled with expert opinions.\n",
            "3. **Machine Learning**: Computers learn from data instead of following explicit rules.\n",
            "4. **Deep Learning**: A subset of machine learning focused on neural networks inspired by biological brains.\n",
            "\n",
            "Each type has its strengths depending on whether it needs clear instructions, large amounts of training data, or complex pattern recognition capabilities.\n",
            "\n",
            "To give you a better idea, think of AI not only as robots doing jobs but also as tools used across various industries such as healthcare, finance, manufacturing, and more recently, even in gaming!\n",
            "\n",
            "Is there anything else you'd like to know about AI? I'm here to answer all your questions! 🤖💻✨\n",
            "\n",
            "---\n",
            "\n",
            "**Example:** Imagine you're playing a game where you need to navigate through a maze. If you use rule-based systems, you might draw lines directly onto the walls to guide yourself. But if you employ deep learning techniques, the computer learns from thousands of successful paths taken before, allowing it to find shortcuts faster and smarter than ever imagined! 😊🚀\n",
            "\n",
            "Feel free to ask more detailed questions anytime! 👍📚\n",
            "\n",
            "--- \n",
            "\n",
            "This approach helps students grasp the concept of AI in a relatable way while keeping the conversation engaging and interactive. The analogy between navigating a maze and solving problems with AI models provides a tangible connection that makes abstract concepts easier to comprehend. Encouraging further exploration beyond these initial explanations will deepen their understanding and interest in pursuing topics related to AI. Remember, curiosity drives innovation, so keep asking those thought-provoking questions! 🔎💡🌟\n",
            "\n",
            "If you want to dive deeper into AI applications or explore real-world scenarios involving AI, feel free\n",
            "You: exit\n",
            "Conversation ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhy1WQo6zLNz"
      },
      "source": [
        "## Agents\n",
        "\n",
        "Agents are the most powerful feature of LangChain. They allow you to combine LLMs with external data and tools. Let's see how we can create a simple agent that will use the Python REPL to calculate the square root of a number and divide it by 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdUQ4plazI5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8f6705-d15e-4983-a228-8dd67c0aa095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m Let me use the pow() function in python to calculate the square root of a number\n",
            "Action: $python pow(x, 0.5)\n",
            "Action Input: x = 16\u001b[0m\n",
            "Observation: $python pow(x, 0.5) is not a valid tool, try one of [Python_REPL].\n",
            "Thought:\u001b[32;1m\u001b[1;3m Oh, my bad. Let me try using the math.sqrt() function instead.\n",
            "Action: $python math.sqrt(x) / 2\n",
            "Action Input: x = 16\u001b[0m\n",
            "Observation: $python math.sqrt(x) / 2 is not a valid tool, try one of [Python_REPL].\n",
            "Thought:\u001b[32;1m\u001b[1;3m My apologies for that mistake too. Let me try again.\n",
            "Final Answer: The final answer is 4.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "\n",
        "agent = create_python_agent(llm=llm, tool=PythonREPLTool(), verbose=True)\n",
        "\n",
        "result = agent.run(\"Calculate the square root of a number and divide it by 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python REPL stands for \"Read-Eval-Print Loop.\" It's an interactive environment where you can write Python code and execute it immediately."
      ],
      "metadata": {
        "id": "kTtiWlo61oKU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XhPBoX6zSDg"
      },
      "source": [
        "Here's the final answer from our agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCtARjW3zOrV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3030ca6f-71d6-46c8-f428-fbe8e8784427"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The final answer is 4.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhN6ZKOOzWjo"
      },
      "source": [
        "Let's run the code from the agent in a Python REPL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15UIrCtUzT7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ffbc5a-b75e-4c90-9982-e71e767abc4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from math import sqrt\n",
        "\n",
        "x = 16\n",
        "y = sqrt(x)\n",
        "z = y / 2\n",
        "z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QsiYJh5znhj"
      },
      "source": [
        "So, our agent works but made a mistake in the calculations. This is important, you might hear great things about AI, but it's still not perfect. Maybe another, more powerful LLM, will get this right. Try it out and let me know.\n",
        "\n",
        "Here's the response to the same prompt but using ChatGPT:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl-T0AZyzyTB"
      },
      "source": [
        "     Enter a number: 16\n",
        "     The square root of 16.0 divided by 2 is: 2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AR4uYAZlSuZ",
        "outputId": "4d1f68e6-078e-4b8b-d0e4-1a7bbd7021b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=2c6ff9c1b6c247c4be147b1f43f06fb5ceecaefc25842c8b5bda6989956fdc85\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "\n",
        "class WikipediaAgent:\n",
        "    def search(self, query):\n",
        "        # Search Wikipedia and return the summary of the first result.\n",
        "        try:\n",
        "            # Get the page summary for the query\n",
        "            summary = wikipedia.summary(query)\n",
        "            return summary\n",
        "        except wikipedia.exceptions.DisambiguationError as e:\n",
        "            # If there's a disambiguation issue, return the options.\n",
        "            return \"Disambiguation Error: \" + '; '.join(e.options)\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            # If the page is not found, inform the user.\n",
        "            return \"Page not found for the query.\"\n",
        "\n",
        "# Create an instance of the WikipediaAgent\n",
        "wiki_agent = WikipediaAgent()\n",
        "\n",
        "# Example use of the agent to search for a term\n",
        "result = wiki_agent.search(\"Artificial Intelligence\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "KtDdQq-o5GNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83502c63-f6c2-439c-b76d-13f1f6f7ad8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of other living beings, primarily of humans. It is a field of study in computer science that develops and studies intelligent machines. Such machines may be called AIs.\n",
            "AI technology is widely used throughout industry, government, and science. Some high-profile applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), interacting via human speech (such as Google Assistant, Siri, and Alexa), self-driving cars (e.g., Waymo), generative and creative tools (ChatGPT and AI art), and superhuman play and analysis in strategy games (such as chess and Go).Alan Turing was the first person to conduct substantial research in the field that he called Machine Intelligence. Artificial intelligence was founded as an academic discipline in 1956. The field went through multiple cycles of optimism followed by disappointment and loss of funding. Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques, and after 2017 with the transformer architecture. This led to the AI spring of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence (the ability to complete any task performable by a human) is among the field's long-term goals.To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience and other fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ji6U-ewWlQxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e6a9c672a25b4be58bedabee6393d0da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87ba9f08619d403d9bf284339f7f3e40",
              "IPY_MODEL_0253c0b71deb47c48cfccc3395bd0a86",
              "IPY_MODEL_16653e78085e45a0865c71f9f3f42e0f"
            ],
            "layout": "IPY_MODEL_195e610854a64b259726d085639ddded"
          }
        },
        "87ba9f08619d403d9bf284339f7f3e40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8e8725fc4434c7b918b46f5ffda3ec6",
            "placeholder": "​",
            "style": "IPY_MODEL_fe9d68179ac94966bba092dbc8b9b8a7",
            "value": "config.json: 100%"
          }
        },
        "0253c0b71deb47c48cfccc3395bd0a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80957d1a00484e3aa1eb65512ba756d7",
            "max": 660,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82eaafdbc4d94103a3b5e28f0268d053",
            "value": 660
          }
        },
        "16653e78085e45a0865c71f9f3f42e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_221abe9c34db48b68db9b38bb8af6682",
            "placeholder": "​",
            "style": "IPY_MODEL_9201e90b153b4c6c8190cce5cf861870",
            "value": " 660/660 [00:00&lt;00:00, 45.0kB/s]"
          }
        },
        "195e610854a64b259726d085639ddded": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8e8725fc4434c7b918b46f5ffda3ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe9d68179ac94966bba092dbc8b9b8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80957d1a00484e3aa1eb65512ba756d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82eaafdbc4d94103a3b5e28f0268d053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "221abe9c34db48b68db9b38bb8af6682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9201e90b153b4c6c8190cce5cf861870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8be8689614c45f183ba7907a9ba57a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac7ed5ff09d84ff1ade838e0c90008fb",
              "IPY_MODEL_18a3bca5e88c48eead88255a7bfc1369",
              "IPY_MODEL_85806d33159c42a89f23e44cbfd0b753"
            ],
            "layout": "IPY_MODEL_1eb46ea232994ee5bc9325bc04f10a3d"
          }
        },
        "ac7ed5ff09d84ff1ade838e0c90008fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc9f8aec86f04e579d1dd08847ae1c9b",
            "placeholder": "​",
            "style": "IPY_MODEL_4cece3809b2d4a2cbcdd6afafbd421fd",
            "value": "model.safetensors: 100%"
          }
        },
        "18a3bca5e88c48eead88255a7bfc1369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_807ce9db05f148e08abfde9c089a5832",
            "max": 3087467144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47166082cda0435a932cad37382940d0",
            "value": 3087467144
          }
        },
        "85806d33159c42a89f23e44cbfd0b753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff3fb38695d6453f90843bbd374bb536",
            "placeholder": "​",
            "style": "IPY_MODEL_c62695f4b9874f6393e852121883e795",
            "value": " 3.09G/3.09G [01:13&lt;00:00, 42.7MB/s]"
          }
        },
        "1eb46ea232994ee5bc9325bc04f10a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc9f8aec86f04e579d1dd08847ae1c9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cece3809b2d4a2cbcdd6afafbd421fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "807ce9db05f148e08abfde9c089a5832": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47166082cda0435a932cad37382940d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff3fb38695d6453f90843bbd374bb536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c62695f4b9874f6393e852121883e795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac80170b45164d0f99a4d5879387408c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35c2c90383114ac091e2b4f144ffce3d",
              "IPY_MODEL_4f7138f9c210431baa03b8021af832ce",
              "IPY_MODEL_14b2c9a1e70049a088f0934a076bc060"
            ],
            "layout": "IPY_MODEL_f05b51d71ec34e639959480a1c041f70"
          }
        },
        "35c2c90383114ac091e2b4f144ffce3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_071942fd9b9c44be9171d97c9feb453f",
            "placeholder": "​",
            "style": "IPY_MODEL_ccb2dce0e5654676b874cd37eced7b1d",
            "value": "generation_config.json: 100%"
          }
        },
        "4f7138f9c210431baa03b8021af832ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_727175d5704641b1b21cd6305c6bad9e",
            "max": 242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be38050f0a514fd9b5e127a4baa1cf52",
            "value": 242
          }
        },
        "14b2c9a1e70049a088f0934a076bc060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99176259b134401b81912ff9be362932",
            "placeholder": "​",
            "style": "IPY_MODEL_418a37e18c944a2b97d44f00618b0881",
            "value": " 242/242 [00:00&lt;00:00, 3.52kB/s]"
          }
        },
        "f05b51d71ec34e639959480a1c041f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "071942fd9b9c44be9171d97c9feb453f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccb2dce0e5654676b874cd37eced7b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "727175d5704641b1b21cd6305c6bad9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be38050f0a514fd9b5e127a4baa1cf52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99176259b134401b81912ff9be362932": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "418a37e18c944a2b97d44f00618b0881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e18385883d42446baf4b83a74d5ad52e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17962cd91177427da7fabda73bfeac9b",
              "IPY_MODEL_fa33c2d126c34b95bf46be468e57dd3a",
              "IPY_MODEL_20fa8e9660e6427b8898fccca82e21fd"
            ],
            "layout": "IPY_MODEL_409e8c8c76264e0fa729a87760260213"
          }
        },
        "17962cd91177427da7fabda73bfeac9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdf82d1bedb94091b11c727ced7f8920",
            "placeholder": "​",
            "style": "IPY_MODEL_0e4f82d64680484eb1fe2f87160039f1",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "fa33c2d126c34b95bf46be468e57dd3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13bb32bec3274d35bcfe466ccb002801",
            "max": 7305,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55e7887328884c12a9df3220f5644c0f",
            "value": 7305
          }
        },
        "20fa8e9660e6427b8898fccca82e21fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74dffbc94d0b42c8a6a2a334562e8f46",
            "placeholder": "​",
            "style": "IPY_MODEL_083af5bb2206485b9dde342802b9ed62",
            "value": " 7.30k/7.30k [00:00&lt;00:00, 219kB/s]"
          }
        },
        "409e8c8c76264e0fa729a87760260213": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdf82d1bedb94091b11c727ced7f8920": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e4f82d64680484eb1fe2f87160039f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13bb32bec3274d35bcfe466ccb002801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55e7887328884c12a9df3220f5644c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74dffbc94d0b42c8a6a2a334562e8f46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "083af5bb2206485b9dde342802b9ed62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f26d087c8da480fbe7d8e2cfdd5ea87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d998eae6b6ec4263b6d978dc64e776bc",
              "IPY_MODEL_1f9381517ab940c7a16a4b7a104c8b03",
              "IPY_MODEL_11730561850541cba697bc3afed7baff"
            ],
            "layout": "IPY_MODEL_3595a4b5885e40eba72254c590deba55"
          }
        },
        "d998eae6b6ec4263b6d978dc64e776bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d896e50beba94344b2321c4fdac2a4bb",
            "placeholder": "​",
            "style": "IPY_MODEL_c4bc56d8ad8f4706a58f0879167980b8",
            "value": "vocab.json: 100%"
          }
        },
        "1f9381517ab940c7a16a4b7a104c8b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b1081ffd11f4daa9f6b4e1e2adbc988",
            "max": 2776833,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4dc43a3873154fa2b49c9bd0d87f55a1",
            "value": 2776833
          }
        },
        "11730561850541cba697bc3afed7baff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3846bbef82a74b00b99b3fb49e1b9d1b",
            "placeholder": "​",
            "style": "IPY_MODEL_e93b5d44fda24996a9889b29357a4d43",
            "value": " 2.78M/2.78M [00:00&lt;00:00, 10.8MB/s]"
          }
        },
        "3595a4b5885e40eba72254c590deba55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d896e50beba94344b2321c4fdac2a4bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4bc56d8ad8f4706a58f0879167980b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b1081ffd11f4daa9f6b4e1e2adbc988": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dc43a3873154fa2b49c9bd0d87f55a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3846bbef82a74b00b99b3fb49e1b9d1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e93b5d44fda24996a9889b29357a4d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "841f9e5683824a3a8200754fb3aa83b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b65ad0faf6a04e49bbe1a9c00421eb7c",
              "IPY_MODEL_d84997b7658449cb850d5708d7f6aae8",
              "IPY_MODEL_31cf726d606a4708ada81e2d028061b6"
            ],
            "layout": "IPY_MODEL_702d7e5e362148709809c04d10301a05"
          }
        },
        "b65ad0faf6a04e49bbe1a9c00421eb7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08d09d7236164e41ba3d06f8cb487cd2",
            "placeholder": "​",
            "style": "IPY_MODEL_ff3aeab38e91462da137930ddcdc8bd8",
            "value": "merges.txt: 100%"
          }
        },
        "d84997b7658449cb850d5708d7f6aae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_551a1a7fad2e49f19d23c91a920ebbb2",
            "max": 1671839,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93f05e525b7f4bdcad7e7b2224c4af33",
            "value": 1671839
          }
        },
        "31cf726d606a4708ada81e2d028061b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d66b46d98514a3183117deea0dcbc4c",
            "placeholder": "​",
            "style": "IPY_MODEL_a53de60338a24ed3abe5369b3064872f",
            "value": " 1.67M/1.67M [00:00&lt;00:00, 1.92MB/s]"
          }
        },
        "702d7e5e362148709809c04d10301a05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08d09d7236164e41ba3d06f8cb487cd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff3aeab38e91462da137930ddcdc8bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "551a1a7fad2e49f19d23c91a920ebbb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93f05e525b7f4bdcad7e7b2224c4af33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d66b46d98514a3183117deea0dcbc4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a53de60338a24ed3abe5369b3064872f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c73052e08fc14732b065a6a3f0f92fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b69450f87e944608b625e8223d8512f",
              "IPY_MODEL_21a028881aa44dad9f86853c432fe0d4",
              "IPY_MODEL_f67dbd7180cb498eb127d37a7e8e0859"
            ],
            "layout": "IPY_MODEL_d390028fd15e4ac59950ceb579b2e283"
          }
        },
        "1b69450f87e944608b625e8223d8512f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6586fa7990e146fb9b752cdc79b1ed7f",
            "placeholder": "​",
            "style": "IPY_MODEL_cca49a29536d4e66a57c58892628347e",
            "value": "tokenizer.json: 100%"
          }
        },
        "21a028881aa44dad9f86853c432fe0d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3feab53c537c484ca1082561fcb51cd8",
            "max": 7031645,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7dff0ab5fbb94ebcb4ac8b6c4195397f",
            "value": 7031645
          }
        },
        "f67dbd7180cb498eb127d37a7e8e0859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c756fd0143ae4e429ecacc1f6f510a53",
            "placeholder": "​",
            "style": "IPY_MODEL_f96df489063146f8954befcbaaec06ea",
            "value": " 7.03M/7.03M [00:01&lt;00:00, 6.45MB/s]"
          }
        },
        "d390028fd15e4ac59950ceb579b2e283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6586fa7990e146fb9b752cdc79b1ed7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cca49a29536d4e66a57c58892628347e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3feab53c537c484ca1082561fcb51cd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dff0ab5fbb94ebcb4ac8b6c4195397f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c756fd0143ae4e429ecacc1f6f510a53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f96df489063146f8954befcbaaec06ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbdbbe26472a4478b888b1119e5ebb6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64ebc412643645178c8f3ae232175dd6",
              "IPY_MODEL_bb6b866443364bbda24e8bb65eba9992",
              "IPY_MODEL_42cc0db0a17740ada909b8c1103ae078"
            ],
            "layout": "IPY_MODEL_eecdea0b6b0844d2b575c439967f73ab"
          }
        },
        "64ebc412643645178c8f3ae232175dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83e5f8303ac44711a13b7fc9410ef0a7",
            "placeholder": "​",
            "style": "IPY_MODEL_6796784d7dda4dd4b7d2340decc06c8f",
            "value": "Loading weights: 100%"
          }
        },
        "bb6b866443364bbda24e8bb65eba9992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b32406297dfc4e17b2726d5d88066f89",
            "max": 398,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f9e07e96b5b479c84b5ff447b1306ad",
            "value": 398
          }
        },
        "42cc0db0a17740ada909b8c1103ae078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b3f20bd95354b299e3f66846e7b01b6",
            "placeholder": "​",
            "style": "IPY_MODEL_d8fa193ce7274c02b9e12a09475cb00d",
            "value": " 398/398 [00:23&lt;00:00, 14.99it/s, Materializing param=model.norm.weight]"
          }
        },
        "eecdea0b6b0844d2b575c439967f73ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83e5f8303ac44711a13b7fc9410ef0a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6796784d7dda4dd4b7d2340decc06c8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b32406297dfc4e17b2726d5d88066f89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f9e07e96b5b479c84b5ff447b1306ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b3f20bd95354b299e3f66846e7b01b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8fa193ce7274c02b9e12a09475cb00d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7eba8c4c3a194d0386eda55e6fadb48c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b956c2c66e994d0eb6047bedb2abe0eb",
              "IPY_MODEL_5614c749da944b948c4143a6aa2b4dce",
              "IPY_MODEL_f145925a848342d5899d0ac95eb2cb7e"
            ],
            "layout": "IPY_MODEL_0233cb6619e14cd88e4d7092ae27b2d9"
          }
        },
        "b956c2c66e994d0eb6047bedb2abe0eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb94ba4694584c6e8114ab2d2a9ec193",
            "placeholder": "​",
            "style": "IPY_MODEL_8649cf499a07490d97b8209db9282371",
            "value": "model.safetensors.index.json: "
          }
        },
        "5614c749da944b948c4143a6aa2b4dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67675b0a90da4204b5fd362cd9bba301",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71c1986235d44e63b2b57ad1e46d85f6",
            "value": 1
          }
        },
        "f145925a848342d5899d0ac95eb2cb7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_697efb4fe87640569401ba939376367a",
            "placeholder": "​",
            "style": "IPY_MODEL_7cc6f2281cd24295bec1f84038048b1b",
            "value": " 32.8k/? [00:00&lt;00:00, 826kB/s]"
          }
        },
        "0233cb6619e14cd88e4d7092ae27b2d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb94ba4694584c6e8114ab2d2a9ec193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8649cf499a07490d97b8209db9282371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67675b0a90da4204b5fd362cd9bba301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "71c1986235d44e63b2b57ad1e46d85f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "697efb4fe87640569401ba939376367a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cc6f2281cd24295bec1f84038048b1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c07072cc50304c8289c1398f950a802c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4456869fbaf44fd2a00bc64271a6e4ca",
              "IPY_MODEL_1c78ab1b3c474f959b3917705c440297",
              "IPY_MODEL_ed072ba716ef4a5f9d28e0fa61f7b3f4"
            ],
            "layout": "IPY_MODEL_7f87c3e2af3c42c08e567950c8503d39"
          }
        },
        "4456869fbaf44fd2a00bc64271a6e4ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5d401985ab047d1a8a3c44138c11de6",
            "placeholder": "​",
            "style": "IPY_MODEL_e1566f9df53e47dc9e2b397c0474c75a",
            "value": "Download complete: 100%"
          }
        },
        "1c78ab1b3c474f959b3917705c440297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_436650f88ba343f2b3a8821c7470aa37",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91ca98dbe30c45f7af07039a438a662a",
            "value": 1
          }
        },
        "ed072ba716ef4a5f9d28e0fa61f7b3f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb121df5a59f4782b43690a64859a72d",
            "placeholder": "​",
            "style": "IPY_MODEL_997b01949e5a48d9a56ae753e4aa7b1b",
            "value": " 8.04G/8.04G [01:25&lt;00:00, 186MB/s]"
          }
        },
        "7f87c3e2af3c42c08e567950c8503d39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5d401985ab047d1a8a3c44138c11de6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1566f9df53e47dc9e2b397c0474c75a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "436650f88ba343f2b3a8821c7470aa37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "91ca98dbe30c45f7af07039a438a662a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb121df5a59f4782b43690a64859a72d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "997b01949e5a48d9a56ae753e4aa7b1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2047ea2a8b3e49e0b45cef60577231d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_608907aada4e4999816b5e9291dc79cf",
              "IPY_MODEL_80eea99b3385495c8209e66012632e95",
              "IPY_MODEL_2f643706e6664a37b357b3fbc419024c"
            ],
            "layout": "IPY_MODEL_6aa028885a2149d38cf961f99a83497f"
          }
        },
        "608907aada4e4999816b5e9291dc79cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73598dc8c64245bf8fa2088ca418b11e",
            "placeholder": "​",
            "style": "IPY_MODEL_b0533cb323424454afdc4ef0bfc39f98",
            "value": "Fetching 3 files: 100%"
          }
        },
        "80eea99b3385495c8209e66012632e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45c4026ff9d9401fb81eb047b06f57a9",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e023f4a137048f48052d2282238f225",
            "value": 3
          }
        },
        "2f643706e6664a37b357b3fbc419024c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fb4572528dc45b38340a3ad902fe4b7",
            "placeholder": "​",
            "style": "IPY_MODEL_dd7ba8e929b84fcc85c294dce35ddf5f",
            "value": " 3/3 [01:25&lt;00:00, 85.37s/it]"
          }
        },
        "6aa028885a2149d38cf961f99a83497f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73598dc8c64245bf8fa2088ca418b11e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0533cb323424454afdc4ef0bfc39f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45c4026ff9d9401fb81eb047b06f57a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e023f4a137048f48052d2282238f225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fb4572528dc45b38340a3ad902fe4b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd7ba8e929b84fcc85c294dce35ddf5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebd00cb981654cdca7d42738bda0722c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b331987fbef9465c957ea0e04cdd1a9d",
              "IPY_MODEL_e3d453d316db4621afa454ccefaf4e70",
              "IPY_MODEL_c3890ff973f841f58ceb7534fba36a0a"
            ],
            "layout": "IPY_MODEL_7fb321cd894e4a1987923553ad8964e8"
          }
        },
        "b331987fbef9465c957ea0e04cdd1a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cca4ae1659b4a1b9f65ac7453706445",
            "placeholder": "​",
            "style": "IPY_MODEL_8cbf173fd539498ba30e13d53403a791",
            "value": "Loading weights: 100%"
          }
        },
        "e3d453d316db4621afa454ccefaf4e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74ca5d74bc734319bd7e11baa0afda8b",
            "max": 398,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40709d4bdf3345a4bac49a20c80f4b13",
            "value": 398
          }
        },
        "c3890ff973f841f58ceb7534fba36a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8c7cd48cc874af0a74438178f9d9fd3",
            "placeholder": "​",
            "style": "IPY_MODEL_64688589eb8f4f50a977edc37d710884",
            "value": " 398/398 [00:34&lt;00:00, 12.00it/s, Materializing param=model.norm.weight]"
          }
        },
        "7fb321cd894e4a1987923553ad8964e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cca4ae1659b4a1b9f65ac7453706445": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cbf173fd539498ba30e13d53403a791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74ca5d74bc734319bd7e11baa0afda8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40709d4bdf3345a4bac49a20c80f4b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8c7cd48cc874af0a74438178f9d9fd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64688589eb8f4f50a977edc37d710884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dae08d7081884f53ba511e1836555183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b24205bc0f12470fb3ffe0844eb5c5aa",
              "IPY_MODEL_6227af8efa854749befddbb1fef2e0df",
              "IPY_MODEL_48bd5d36f9974035b9c53566e07ed2a5"
            ],
            "layout": "IPY_MODEL_66b79777b390495dacf02723c0817e30"
          }
        },
        "b24205bc0f12470fb3ffe0844eb5c5aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ebbfd64290c4b0d868df673a2af0338",
            "placeholder": "​",
            "style": "IPY_MODEL_00b87d30a13f4845bae222d88141d57a",
            "value": "generation_config.json: 100%"
          }
        },
        "6227af8efa854749befddbb1fef2e0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f815cb4faf934a72891e893be9077911",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52a450a945184072b2a230966c7e44f6",
            "value": 239
          }
        },
        "48bd5d36f9974035b9c53566e07ed2a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fac4c93fb3184a3eaa105c0021d57fd3",
            "placeholder": "​",
            "style": "IPY_MODEL_a691fc6ce4e34d0eb0634aef23fbc221",
            "value": " 239/239 [00:00&lt;00:00, 24.4kB/s]"
          }
        },
        "66b79777b390495dacf02723c0817e30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ebbfd64290c4b0d868df673a2af0338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00b87d30a13f4845bae222d88141d57a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f815cb4faf934a72891e893be9077911": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52a450a945184072b2a230966c7e44f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fac4c93fb3184a3eaa105c0021d57fd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a691fc6ce4e34d0eb0634aef23fbc221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}